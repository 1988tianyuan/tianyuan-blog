{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/1547709651604.png","path":"images/1547709651604.png","modified":0,"renderable":0},{"_id":"source/images/1547708979007.png","path":"images/1547708979007.png","modified":0,"renderable":0},{"_id":"source/images/1547712873168.png","path":"images/1547712873168.png","modified":0,"renderable":0},{"_id":"source/images/1547712156480.png","path":"images/1547712156480.png","modified":0,"renderable":0},{"_id":"source/images/1547709525963.png","path":"images/1547709525963.png","modified":0,"renderable":0},{"_id":"source/images/1547712312524.png","path":"images/1547712312524.png","modified":0,"renderable":0},{"_id":"source/images/1547713402558.png","path":"images/1547713402558.png","modified":0,"renderable":0},{"_id":"source/images/1547710015291.png","path":"images/1547710015291.png","modified":0,"renderable":0},{"_id":"source/images/1547713322805.png","path":"images/1547713322805.png","modified":0,"renderable":0},{"_id":"source/images/3pc-1.png","path":"images/3pc-1.png","modified":0,"renderable":0},{"_id":"source/images/1547792452619.png","path":"images/1547792452619.png","modified":0,"renderable":0},{"_id":"source/images/1559482380158.png","path":"images/1559482380158.png","modified":0,"renderable":0},{"_id":"source/images/image.png","path":"images/image.png","modified":0,"renderable":0},{"_id":"source/images/1559481447073.png","path":"images/1559481447073.png","modified":0,"renderable":0},{"_id":"source/images/3pc-2.png","path":"images/3pc-2.png","modified":0,"renderable":0},{"_id":"source/images/pasted-1.png","path":"images/pasted-1.png","modified":0,"renderable":0},{"_id":"source/images/3pc-3.png","path":"images/3pc-3.png","modified":0,"renderable":0},{"_id":"source/images/pasted-20.png","path":"images/pasted-20.png","modified":0,"renderable":0},{"_id":"source/images/pasted-19.png","path":"images/pasted-19.png","modified":0,"renderable":0},{"_id":"source/images/pasted-24.png","path":"images/pasted-24.png","modified":0,"renderable":0},{"_id":"source/images/pasted-2.png","path":"images/pasted-2.png","modified":0,"renderable":0},{"_id":"source/images/pasted-25.png","path":"images/pasted-25.png","modified":0,"renderable":0},{"_id":"source/images/pasted-5.png","path":"images/pasted-5.png","modified":0,"renderable":0},{"_id":"source/images/pasted-3.png","path":"images/pasted-3.png","modified":0,"renderable":0},{"_id":"source/images/pasted-6.png","path":"images/pasted-6.png","modified":0,"renderable":0},{"_id":"source/images/pasted-7.png","path":"images/pasted-7.png","modified":0,"renderable":0},{"_id":"source/images/pasted-29.png","path":"images/pasted-29.png","modified":0,"renderable":0},{"_id":"source/images/pasted-4.png","path":"images/pasted-4.png","modified":0,"renderable":0},{"_id":"source/images/pasted-8.png","path":"images/pasted-8.png","modified":0,"renderable":0},{"_id":"source/images/pasted-9.png","path":"images/pasted-9.png","modified":0,"renderable":0},{"_id":"source/images/nagle.jpg","path":"images/nagle.jpg","modified":0,"renderable":0},{"_id":"source/images/1559568139040.png","path":"images/1559568139040.png","modified":0,"renderable":0},{"_id":"source/images/1548985987329.png","path":"images/1548985987329.png","modified":0,"renderable":0},{"_id":"source/images/pasted-16.png","path":"images/pasted-16.png","modified":0,"renderable":0},{"_id":"source/images/kafka.png","path":"images/kafka.png","modified":0,"renderable":0},{"_id":"source/images/pasted-18.png","path":"images/pasted-18.png","modified":0,"renderable":0},{"_id":"source/images/pasted-22.png","path":"images/pasted-22.png","modified":0,"renderable":0},{"_id":"source/images/pasted-21.png","path":"images/pasted-21.png","modified":0,"renderable":0},{"_id":"source/images/pasted-23.png","path":"images/pasted-23.png","modified":0,"renderable":0},{"_id":"source/images/pasted-26.png","path":"images/pasted-26.png","modified":0,"renderable":0},{"_id":"source/images/pasted-28.png","path":"images/pasted-28.png","modified":0,"renderable":0},{"_id":"source/images/pasted-11.png","path":"images/pasted-11.png","modified":0,"renderable":0},{"_id":"source/images/pasted-13.png","path":"images/pasted-13.png","modified":0,"renderable":0},{"_id":"source/images/pasted-15.png","path":"images/pasted-15.png","modified":0,"renderable":0},{"_id":"source/images/pasted-14.png","path":"images/pasted-14.png","modified":0,"renderable":0},{"_id":"source/images/pasted-10.png","path":"images/pasted-10.png","modified":0,"renderable":0},{"_id":"source/images/pasted-17.png","path":"images/pasted-17.png","modified":0,"renderable":0},{"_id":"source/images/pasted-27.png","path":"images/pasted-27.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/images/pasted-12.png","path":"images/pasted-12.png","modified":0,"renderable":0},{"_id":"source/images/pasted-0.png","path":"images/pasted-0.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest-nomobile.min.js","path":"lib/canvas-nest/canvas-nest-nomobile.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","path":"lib/canvas-nest/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/README.md","path":"lib/canvas-nest/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/next-boot.js","path":"js/src/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/muse.js","path":"js/src/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1553009877047},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1553009877043},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1553009877044},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1553009877048},{"_id":"themes/next/.gitignore","hash":"69e702b833c6aa9646ad24c45dd9cf00ab5ce6b9","modified":1553009877091},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1553009877091},{"_id":"themes/next/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1553009877094},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1553009877116},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1553009877098},{"_id":"themes/next/bower.json","hash":"9869a845e7a8bf51558350d7620ec9e17be89265","modified":1553009877114},{"_id":"themes/next/README.md","hash":"f17e06d0daade304f66fe885f32078656079eed0","modified":1553009877105},{"_id":"themes/next/.all-contributorsrc","hash":"a31b459a30b609f2b045728566ba50566fdcdf79","modified":1553009877042},{"_id":"themes/next/gulpfile.coffee","hash":"67eaf2515100971f6195b60eeebbfe5e8de895ab","modified":1553009877218},{"_id":"themes/next/package.json","hash":"5912233efcabf8c6d7dcd2c2036c77b6631b5677","modified":1553009877810},{"_id":"themes/next/_config.yml","hash":"4f367b8a93634fb1a3957f8872be395f492ebab1","modified":1553089548520},{"_id":"source/categories/index.md","hash":"4ef6788360d9bbf70eb71ca5e6600ddc4a46db16","modified":1553002258498},{"_id":"source/_discarded/TCP连接-Nagle-和-Cork.md","hash":"bac695d007f3f4e173c930185e8922235aad3570","modified":1553002258372},{"_id":"source/_discarded/kafka学习笔记（1）——-kafka基本特点以及与其他mq的对比-1.md","hash":"0728611aa2d71cc48cf1a56a534a7b9aef41b89b","modified":1553002258402},{"_id":"source/_discarded/hello-world.md","hash":"cfd604a540adb19296ef3741b4f6008d71bd0359","modified":1553002258398},{"_id":"source/_discarded/如何理解3PC协议-1.md","hash":"708bbec1dd46994e9c4f518d1787a59d5e2b9923","modified":1553783688544},{"_id":"source/_discarded/kafka学习笔记（1）——-kafka基本特点以及与其他mq的对比.md","hash":"1dba44f54500b5f9745cede77ab9cb04e29da31b","modified":1553002258406},{"_id":"source/tags/index.md","hash":"98528c6273a20e3918d18ffa3cb0a5c5c1013141","modified":1553002258858},{"_id":"source/_discarded/new.md","hash":"993a1e66216c445a399562cc2cd2fcbe825cdd51","modified":1553002258409},{"_id":"source/_discarded/如何理解3PC协议.md","hash":"72c5b28ed06ba9257a4b8170bcd693edf12044b2","modified":1553783684959},{"_id":"source/images/1547709651604.png","hash":"c67398544bde7838d68d71dac601c928c951d2ce","modified":1553002258505},{"_id":"source/images/1547708979007.png","hash":"b1f7b5b6eae6bb711a2b3c5ee408dd349aa55b0d","modified":1553002258501},{"_id":"source/images/1547712873168.png","hash":"33b16eb035ee2ead7cd58b8ac4096e60b110782d","modified":1553002258510},{"_id":"source/images/1547712156480.png","hash":"89344a6a67e8861d9836f07dcf908c77af0cf84e","modified":1553002258507},{"_id":"source/images/1547709525963.png","hash":"72399a2f28c89b498f86a7f3f203fe4a43986ad3","modified":1553002258503},{"_id":"source/images/1547712312524.png","hash":"aa8dfb6a7b289029dd52d0987ee1568fce82d2a9","modified":1553002258509},{"_id":"source/images/1547713402558.png","hash":"7960d279b99e8de04f1e25e0f2277bf97946fa86","modified":1553002258511},{"_id":"source/images/1547710015291.png","hash":"4ba0af0e179bce338f32ffba3f74afdf2a005854","modified":1553002258506},{"_id":"source/images/1547713322805.png","hash":"fbe70104ed5347dde7c3d8ad72507b861899a2e5","modified":1553002258510},{"_id":"source/images/3pc-1.png","hash":"2a669a25d9a86d1fecc415fc7d205970859224c6","modified":1553784562039},{"_id":"source/images/1547792452619.png","hash":"e7f78be7f846f465a499b8e7ff18411e2131bed3","modified":1553002258513},{"_id":"source/images/1559482380158.png","hash":"e2fd45ae2ce12c0aa776d80dae26bd1fe537d51f","modified":1559482380170},{"_id":"source/images/image.png","hash":"a0b536df2acad1d7501fc10b38041c572ee47467","modified":1562937403604},{"_id":"source/images/1559481447073.png","hash":"0176c9c1fb16b3b8e24276bb3723145d04210b50","modified":1559481447112},{"_id":"source/images/3pc-2.png","hash":"b915bd357a110aeb91a0de036dfea4f58702aebb","modified":1553784592209},{"_id":"source/images/pasted-1.png","hash":"064386e353e32ae73f96d3eb521a6c4b6f544d7e","modified":1553002258770},{"_id":"source/images/3pc-3.png","hash":"88209d7105c0fcf9423c190bbbd46aceb31c7a8b","modified":1553784611370},{"_id":"source/images/pasted-20.png","hash":"26999d9aa4326ff104df1358ddddaa6ebad37a0c","modified":1553002258834},{"_id":"source/images/pasted-19.png","hash":"40a4a3f5d3640959c626bf6ec784f70d1f6ca664","modified":1553002258831},{"_id":"source/images/pasted-24.png","hash":"12a8dfa52e3e5d8b5136cddba2a375d6f62603f8","modified":1553002258840},{"_id":"source/images/pasted-2.png","hash":"3cbd3c0ba47836fc06b6106571d61fc330b7b2c5","modified":1553002258832},{"_id":"source/images/pasted-25.png","hash":"c83e737c7eb02596ccbcd8ea86fa7c7239a3d895","modified":1553002258841},{"_id":"source/images/pasted-5.png","hash":"07b43af19dec0c3d0265b8abd5bbc6f5000f1241","modified":1553002258851},{"_id":"source/images/pasted-3.png","hash":"9e6c97f86ff66b84de59238342d8706dbafb298f","modified":1553002258849},{"_id":"source/images/pasted-6.png","hash":"91ff11cc5e36ea28f55cb7e59a10d8cbc1adbdd8","modified":1553002258852},{"_id":"source/images/pasted-7.png","hash":"711e78431f2b6051041e84b847475ab06a19046e","modified":1553002258853},{"_id":"source/images/pasted-29.png","hash":"6c223870fbc0bccfebd680bda7947f42d407ec8d","modified":1553002258848},{"_id":"source/images/pasted-4.png","hash":"62d15792f2440d8d299adc67f7a6b784f28e6be3","modified":1553002258850},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1553009877021},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1553009219721},{"_id":"themes/next/.git/config","hash":"7f8b507977a695e25b27d4c2e8aaed71410536bf","modified":1553009877032},{"_id":"source/images/pasted-8.png","hash":"ce4ba3d666934f7eed61512e6b24dc5db58fb672","modified":1553002258854},{"_id":"source/images/pasted-9.png","hash":"be2a4c8f18d02d1fba519d45b0ea6929478db084","modified":1553002258855},{"_id":"themes/next/.git/index","hash":"9f7256ddade5c289404dc40ce6d64dd2424b03e9","modified":1553009878670},{"_id":"source/images/nagle.jpg","hash":"b09937be89b4c34a9181559560bd6611a7b3c13c","modified":1553002258767},{"_id":"source/_posts/Kubernetes学习-——-如何将自己的应用部署为k8s-service.md","hash":"c9cb4a3417a184c8f080109f73f83c3bd0eb153b","modified":1562937407820},{"_id":"source/_posts/Vim编辑器.md","hash":"9a6d3d1c070bdeb67ebe8ca7e83efadda398f86e","modified":1553002258430},{"_id":"source/_posts/TCP连接-Nagle-和-Cork-1.md","hash":"db8045b7784d5983a4d228993fbf795f21012f9e","modified":1553002258426},{"_id":"source/_posts/Nginx-root和alias路径映射.md","hash":"b959a8bbfb9ea0c398a362690251ba628d5033fe","modified":1553002258413},{"_id":"source/_posts/Yarn初探.md","hash":"deebf3a5f07362c81c956b8c7e54eefcd58437fd","modified":1557069391806},{"_id":"source/_posts/Reactor-Kafka（1）.md","hash":"df6026834294924df14efd8cf7dc1f811a919df8","modified":1553002258420},{"_id":"source/_posts/java线程池源码分析--submit-的过程.md","hash":"7fbcab3b0e46894345f501f12685466743914041","modified":1553002258444},{"_id":"source/_posts/java线程池源码分析--shutdown, shutdownNow, awaitTermination.md","hash":"dd87f22127fce577cd6bc871cfc503bd3c13c940","modified":1553002258437},{"_id":"source/_posts/kafka学习笔记（3）——-消费者-consumer.md","hash":"85af80b1b82098de4125f6844bdc44c9f1a6d144","modified":1553002258461},{"_id":"source/_posts/kafka学习笔记（4）——-深入集群.md","hash":"e564cba0d5b4f3272558df3e07e8ec8431ccc3b5","modified":1553002258465},{"_id":"source/_posts/mongodb索引类型.md","hash":"cd1f9b6a43ee5cb61c956f97ae6338ac0c3c304c","modified":1553002258471},{"_id":"source/_posts/netty-ByteBuf浅析.md","hash":"52350bebec5c9d1df8bdff0c7a8c9c80232adc1d","modified":1553002258475},{"_id":"source/_posts/vue学习笔记-——-用vue-cli搭建spa工程.md","hash":"60bd1af451387a01b203d6e2ed6df329ca9fb437","modified":1553002258483},{"_id":"source/_posts/如何优雅地遍历并删除一个map中的元素.md","hash":"4324bbe1188e4a593ee31d15024a1db0c5244b08","modified":1553099522935},{"_id":"source/_posts/何为RestTemplate.md","hash":"7ed222e5ba8d22ccaa34e61e5647cec1968a1f6b","modified":1553002258493},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"f7ddb7faed8031a9f40eae4ee7bb48c1bc50fd14","modified":1553009877053},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"18535141a2f7d1e274175e4aa7e8cf164641e760","modified":1553009877061},{"_id":"source/_posts/kafka学习笔记（1）——-kafka基本特点以及与其他mq的对比-2.md","hash":"7c35534c59c6ba2436564689e179e7216f9a7542","modified":1553002258448},{"_id":"source/_posts/kafka学习笔记（2）——-生产者-producer.md","hash":"f32771e4cccfff9080086986ff7de8837ef57b1a","modified":1553002258454},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"1e212fe229bd659726b4a3bcf4b5b14e0310ba3a","modified":1553009877065},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"66e529edef048f3fab87d9670fa8999f483292bf","modified":1553009877085},{"_id":"source/_posts/如何理解3PC协议-1.md","hash":"c01ec74b3950f9da75e91fa340a67bd4bc7e7e32","modified":1553784622624},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1dada3c3404445a00367882b8f97cdf092b7943d","modified":1553009877125},{"_id":"source/_posts/自定义类加载器实践.md","hash":"45a476227d3d8cc514b5567da59a0d08fe62ffb4","modified":1556028748231},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1553009877133},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1553009877138},{"_id":"themes/next/.git/packed-refs","hash":"ab9ed36d3bf17001786a4c57899bbc6d225135ef","modified":1553009877014},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1553009877129},{"_id":"themes/next/docs/MATH.md","hash":"0540cd9c961b07931af9f38a83bc9a0f90cd5291","modified":1553009877155},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1553009877148},{"_id":"themes/next/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1553009877830},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1553009877841},{"_id":"themes/next/.github/stale.yml","hash":"ff822ab5556453ee3242ba524790b877a32540ce","modified":1553009877088},{"_id":"themes/next/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1553009877227},{"_id":"themes/next/languages/fr.yml","hash":"0393558717065293bdf732866471cebb0c884f6a","modified":1553009877230},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1553009877121},{"_id":"themes/next/languages/it.yml","hash":"31eb878b53d60ff47e3e534cdd7a839c8801ac6e","modified":1553009877238},{"_id":"themes/next/languages/id.yml","hash":"f3302a4dfdc9be38a52d6e081411574b1ea01671","modified":1553009877234},{"_id":"themes/next/languages/ja.yml","hash":"3f25eca504ee5a519987b4402731f1bb7f5191c9","modified":1553009877241},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1553009877224},{"_id":"themes/next/languages/nl.yml","hash":"08f16ce395dacc88847fc30dc6b985ce22fb8948","modified":1553009877247},{"_id":"themes/next/languages/de.yml","hash":"79b3221344da335743b5ef5a82efa9338d64feb0","modified":1553009877221},{"_id":"themes/next/languages/ko.yml","hash":"75f2fe142f76bf623e34ed3570598226f55f2b8b","modified":1553009877244},{"_id":"themes/next/languages/pt.yml","hash":"ca5072c967e5eb1178ffed91827459eda6e4e6e2","modified":1553009877253},{"_id":"themes/next/languages/pt-BR.yml","hash":"c7de8b77f44e75be4f04423088a1c891537aa601","modified":1553009877250},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1553009877159},{"_id":"themes/next/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1553009877256},{"_id":"themes/next/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1553009877260},{"_id":"themes/next/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1553009877269},{"_id":"themes/next/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1553009877272},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1553009878650},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"721a1aa9feed1b580ab99af8e69ed22699121e88","modified":1553009877144},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1553009878665},{"_id":"themes/next/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1553009877275},{"_id":"themes/next/languages/vi.yml","hash":"e2f0dd7f020a36aa6b73ed4d00dcc4259a7e5e9d","modified":1553009877266},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1553009878658},{"_id":"source/_posts/Yarn任务调度机制探析.md","hash":"da33b76b8fa1050cd3d30e5d0d3c6fda7b4ba151","modified":1559653415249},{"_id":"themes/next/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1553009877789},{"_id":"themes/next/layout/_layout.swig","hash":"ba786b1baba49021928e2e508da53f2fd1369b3f","modified":1553009877280},{"_id":"themes/next/layout/page.swig","hash":"5d06ee8f477ffc39932d0251aa792ffcaf8faf14","modified":1553009877792},{"_id":"themes/next/layout/post.swig","hash":"af74e97d57cf00cde6f8dbd4364f27910915454e","modified":1553009877795},{"_id":"themes/next/layout/schedule.swig","hash":"e79f43df0e9a6cf48bbf00882de48c5a58080247","modified":1553009877798},{"_id":"themes/next/layout/archive.swig","hash":"61bc56e77e653684fc834f63dcbdadf18687c748","modified":1553009877783},{"_id":"themes/next/layout/tag.swig","hash":"283519d4d5b67814412863a3e0212bac18bcc5a0","modified":1553009877806},{"_id":"themes/next/layout/category.swig","hash":"ad0ac6a1ff341f8eab9570e7fb443962948c5f9d","modified":1553009877786},{"_id":"source/images/1559568139040.png","hash":"559932dc49023ac51b1efc7cf29cb20ac0bbf451","modified":1559568139076},{"_id":"source/images/1548985987329.png","hash":"c38ca2c2a8b601bc97e50702177c830cf66b4b24","modified":1553002258515},{"_id":"source/images/pasted-16.png","hash":"62bc028ca4a4c593a049a964036aa9394b7527c6","modified":1553002258825},{"_id":"source/images/kafka.png","hash":"1f31a5be42a398d4d49d881668193cccdbb5eec8","modified":1553002258617},{"_id":"source/images/pasted-18.png","hash":"37677ac1df362c6fe7e28c2da2f95e833cdddf73","modified":1553002258829},{"_id":"source/images/pasted-22.png","hash":"42129287146e43832115b4d65530c00c76ba88b6","modified":1553002258837},{"_id":"source/images/pasted-21.png","hash":"a4eff7602c459321e6316c96f91bc07fe9840e7b","modified":1553002258835},{"_id":"source/images/pasted-23.png","hash":"dffac47cc509f485291141408f77d15a52c6316c","modified":1553002258839},{"_id":"source/images/pasted-26.png","hash":"b6c01a308be3d6ea5a0242355a21f281cf02bde7","modified":1553002258843},{"_id":"source/images/pasted-28.png","hash":"65eca2f65c07405761a471c45cbe06c6abbda72c","modified":1553002258847},{"_id":"themes/next/languages/uk.yml","hash":"6320439c6e9ff81e5b8f8129ca16e9a744b37032","modified":1553009877263},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878370},{"_id":"source/images/pasted-11.png","hash":"f2565ba35cae83dbd0be52b499b11ee854da6364","modified":1553002258780},{"_id":"source/images/pasted-13.png","hash":"77cc92fd2ca699617a96f000233a94e2808196c0","modified":1553002258784},{"_id":"source/images/pasted-15.png","hash":"1618139831d7a4314e6b04b48e346b1b41e90e48","modified":1553002258824},{"_id":"source/images/pasted-14.png","hash":"ec9bbb26029475e2553647553b05831ef92e2376","modified":1553002258821},{"_id":"source/images/pasted-10.png","hash":"0e52a8f50388a77bab8493832614d8c2c624365f","modified":1553002258772},{"_id":"source/images/pasted-17.png","hash":"07ca26cc665a98d3eddddfe07d6e88137412bf27","modified":1553002258827},{"_id":"source/images/pasted-27.png","hash":"266b8a7f0501f78f05a97a47d24cb63e9e786b94","modified":1553002258845},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1553009219722},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1553009219735},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1553009219729},{"_id":"themes/next/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1553009219728},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1553009219735},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1553009219723},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1553009219739},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1553009219730},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1553009219743},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1553009219740},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"795b8ddb251da8e2327299d5f7dbf446fb9867c6","modified":1553009877069},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"245917ffaa296bc2d9a85444acf639077ca25944","modified":1553009877073},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"ae22e700b7c63c60746321719a20d34022ad78d9","modified":1553009877081},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1553009219736},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cdb6152582313268d970ffeef99b4a8a7850f034","modified":1553009877820},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"a40ce6bc852bb4bff8b9f984fa064741dd151e96","modified":1553009877826},{"_id":"themes/next/scripts/tags/exturl.js","hash":"f43683042b16dc32877c48b8f7a82f06dff33813","modified":1553009877859},{"_id":"themes/next/scripts/tags/button.js","hash":"6ef342a0c4b58000ba11148b4c9b2b599edd86b5","modified":1553009877846},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"59b2b45e151972bbe08582cde22f398e58832765","modified":1553009877077},{"_id":"themes/next/scripts/tags/full-image.js","hash":"755b0d518352ec27354124105b48e302ac84d66f","modified":1553009877864},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1553009877877},{"_id":"themes/next/scripts/tags/label.js","hash":"4cee94f53fdecc7f9c2d91c06ab9e7a433b5ec5a","modified":1553009877881},{"_id":"themes/next/.git/logs/HEAD","hash":"b9d7a71da5210f9cbec7f55a27786efe22b50d49","modified":1553009877025},{"_id":"themes/next/scripts/tags/note.js","hash":"f1b560d6e63d1b06fd80e12bbac32660125c223c","modified":1553009877890},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"0114c26303cb157a4c0db5e958892bc35351a5c6","modified":1553009877885},{"_id":"themes/next/scripts/tags/tabs.js","hash":"ca885c8fa46a76a7b8977730575551622497410b","modified":1553009877899},{"_id":"themes/next/scripts/tags/video.js","hash":"ac548868723183a07ec8ed98e6fd1192030f6069","modified":1553009877903},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"18cadbc8ff360367f28520e5d449c331f91e79b8","modified":1553009877872},{"_id":"themes/next/scripts/filters/exturl.js","hash":"b19c7c1021e57367b3b3bbf5678381017ed5667d","modified":1553009877815},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"e771c5b745608c6fb5ae2fa1c06c61b3699627ec","modified":1553009877180},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"018a259694f4a8c7c384e1f323531442cba5fbf3","modified":1553009877184},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"fe87e94f8ce8e9e89c70dd760954d153834eee52","modified":1553009877188},{"_id":"themes/next/scripts/tags/pdf.js","hash":"0cac870913ae7929719f7ca48e8757fbd50aaf2a","modified":1553009877893},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"b17fc344ff61603f83387c0f9b2b2189aae81d50","modified":1553009877201},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1553009877195},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"734b371a0dd910eb9fe087f50c95ce35340bb832","modified":1553009877205},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1553009877163},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1553009877166},{"_id":"themes/next/docs/zh-CN/README.md","hash":"fe4bfa69bcb16a777d5c5ab5d2b617bff548d6a0","modified":1553009877211},{"_id":"themes/next/docs/ru/README.md","hash":"3e191bd6b09dee33e6b8931c3bc338db3d04e479","modified":1553009877172},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1553009877850},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1553009877176},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1553009877192},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1553009877214},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1553009219734},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1553009877276},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1553009877276},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1553009877276},{"_id":"themes/next/source/css/main.styl","hash":"5e7d28bc539e84f8b03e68df82292f7fc0f2d023","modified":1553009878369},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1553009878372},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1553009878373},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1553009878371},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1553009878377},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1553009878385},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1553009878381},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1553009878389},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1553009878399},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1553009878401},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1553009878397},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1553009878393},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1553009878402},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1553009878398},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1553009878400},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1553009878403},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1553009878401},{"_id":"themes/next/layout/_partials/comments.swig","hash":"d0b9e841d55c974d02f43823a06a2627f8e46431","modified":1553009877306},{"_id":"themes/next/layout/_partials/footer.swig","hash":"6d56acdcdc12ebca9c1d90f8a2b52ad17aafca6e","modified":1553009877309},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1553009878406},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"94c5d55df7121d73ee0340beac85b9c7c103a3e2","modified":1553009877313},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1553009877355},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1553009877352},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1553009878407},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"9e00cb9b3fdfe2e2c4877a874d0d3ecb7fd0f3ee","modified":1553009877410},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"28b1cd4c065fcd214a1d6dd06f54bb62c3519aad","modified":1553009877413},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"c31d54154eed347f603009d2d65f7bf8d9a6885a","modified":1553009877407},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"efb3404a3303622f3be60944d9d1926972c5c248","modified":1553009877416},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4130f995f0c4f81a44266194ecae9df96fad174c","modified":1553009877440},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"f58463133bf8cfef5ff07f686b834ff8cbbe492f","modified":1553009877434},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"87bcb495f7ddd81cc3fe2c2a886e51c08053019b","modified":1553009877559},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"891ab67815969dd8736cb22fbbb3f791b8fff4e4","modified":1553009877289},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1553009877565},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"91017f58f83d9505ce99109fffdc51c032bf017e","modified":1553009877303},{"_id":"themes/next/layout/_macro/post.swig","hash":"31ba947998f0c962b04ae7f42f9d3db934209a79","modified":1553009877298},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"07fc0ae1a30c5aa9269d6efdaec598164b1d191c","modified":1553009877675},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"7db4ad4a8dd5420dad2f6890f5299945df0af970","modified":1553009877701},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"7cc1294a5fbedf3502688248a433c358339e5ae0","modified":1553009877736},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1553009877704},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"882cd0b68c493af1b6d945660f9c21085e006ffc","modified":1553009877754},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"4ae61c7efb16e962385bfe522a38c4d29cdcccbe","modified":1553009877710},{"_id":"source/images/pasted-12.png","hash":"b36412b4c5a0d1d18da2265976a19dce05b7f009","modified":1553002258782},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"b44010cd577e4d063c3406772938c4b117ec7b7b","modified":1553009877780},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"c476dc3693a9dd0be2d136a45b0d7fdef55d4d92","modified":1553009877743},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"80dfc0879866e6512cb67590a3b2d8741a66f980","modified":1553009877696},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878272},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878272},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878281},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"87182367d7954457cb2498bbfa9445c03c2d619e","modified":1553009877573},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878364},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553009878353},{"_id":"source/images/pasted-0.png","hash":"7a0e5c05ba596029f62d0abdb73a8fb38f0ba075","modified":1553002258769},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1553009878270},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1553009878271},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"318886aafad11cc2a598807c7d5c0b0ad42bd0ac","modified":1553009878281},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e9b0752f08398709e787546a246baca12b4c557f","modified":1553009878349},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"1aabac9e37a8f4451c86d09037b3a1f8b30eaf5e","modified":1553009878275},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1553009878353},{"_id":"themes/next/source/css/_variables/base.styl","hash":"e37aab667be94576f6145b61a78cfe87836c68b6","modified":1553009878364},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1553009878514},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1553009878516},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1553009878519},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1553009878511},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1553009878646},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1553009878522},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"da7049f3d9a157abe0ecc62611edcf43605ba84d","modified":1553009878358},{"_id":"themes/next/.git/refs/heads/master","hash":"37d3660a9a0b097ab3a52355f34f2827f134af14","modified":1553009877024},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest-nomobile.min.js","hash":"6b4437a9cd8aa04329cc6220a595acfe1fb9b598","modified":1553088800672},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1553009878625},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1553009878635},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","hash":"336611e76f0638d3d8aeca6b1b97138d2a07523f","modified":1553088800645},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"39c4ad0e36b7c1260da98ba345f7bd72a2ac0f2e","modified":1553009877339},{"_id":"themes/next/source/lib/canvas-nest/README.md","hash":"28bc2250a16e22c705cba7b3c17fcc15081e50f2","modified":1553088800652},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"e015c7d9b84062b60b15b36be3ef11929dd10943","modified":1553009877343},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"b57bf9c865bed0f22157176a8085de168a1aef77","modified":1553009877316},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"4b53a0659a7e800871d8e9a4bd20f7b892a8e29b","modified":1553009877322},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"e47f2cdeb79770353fff5a1da01e5255bce5e9c9","modified":1553009877329},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1553088800682},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1553009877283},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"03f669356bbaa70144b743f3312178e1981ac3a8","modified":1553009877333},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c609097b95eb6127c2784f47f2230e6e6efc0be2","modified":1553009877386},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"daa6e5b7dbc409d6bf8a031d5413d8229e9c0995","modified":1553009877346},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"f46699a9daa5fef599733cbab35cb75cf7a05444","modified":1553009877349},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1553009877336},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f331ad02beea8990066d32ad6ec9f859672c3615","modified":1553009877361},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"be6683db6a269d83bb0441d7cf74db63a240fa8a","modified":1553009877358},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"f62b801c7999da67b4bdca9c5e373b9b5ed039dc","modified":1553009877364},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1553009877286},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"fb7727e8ec63a58238a7206bf70eb273c8879993","modified":1553009877368},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f14e9e8c27af82f1bfe794e252dec0d7e521f503","modified":1553009877374},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1553009877371},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1553009877377},{"_id":"themes/next/source/js/src/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1553009878434},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"31245e09ce0465b994cebd94223a531585c4eab4","modified":1553009877383},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"f11e84def0352b7dd6393f1b83e55a40ab468686","modified":1553009878446},{"_id":"themes/next/source/js/src/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1553009878470},{"_id":"themes/next/source/js/src/motion.js","hash":"d0a6d9dbcc57159e54bbb1f683b86632ae0b78f0","modified":1553009878456},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1553009878486},{"_id":"themes/next/source/js/src/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1553009878417},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"e0f0a753d4920ffb37ddbc8270515654a0b9b92a","modified":1553009877419},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"647e8677d1ccfb3f7918dd3ea2ff7078504a845d","modified":1553009877403},{"_id":"themes/next/source/js/src/next-boot.js","hash":"696a0c2cf158001576d56b48195ec8e39e835b47","modified":1553009878461},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"3c548934b97cc426544947f7a2ae35c270b5e33f","modified":1553009877425},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"84018384d00e4a584d613589adae6674a3060a36","modified":1553009877428},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a62c93f19429f159bcf0c2e533ffc619aa399755","modified":1553009877422},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"23c6d15aa2a305f9d29caee1b60cfae84d32fa09","modified":1553009877431},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"54b43d406cf37932e7b60f46814e864d31b1842c","modified":1553009877400},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"66d562b3778dbc839f7c00103bd0099c5d61602a","modified":1553009877443},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"2e1de38f44af00209129d4051b7ae307cb11ad68","modified":1553009877481},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"335005a9f8b36349f0ad0a7beeba6969c55fc7f7","modified":1553009877484},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"83dd7df11b100bae38c9faab9a478f92149a0315","modified":1553009877451},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"623e73bedef067ac24a398ef27c8197295da872d","modified":1553009877514},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"73576c9683d9ad9b124916dc6c660607fe7cc1fa","modified":1553009877478},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"39928f358dd13d9fc1a4641800e57be157ecd815","modified":1553009877517},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1553009877501},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"71fb01bcad43bc9410ab19190373b9f7e59215b5","modified":1553009877530},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"53202062267391353d49f269e7eb74eb87d30921","modified":1553009877490},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"4cff8bf5c42c62f7f0ac1f0d70f839dae39ba77a","modified":1553009877553},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"54d7993ae773573ee103c22802b7e98b193e1a3a","modified":1553009877507},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a22d1ea29a5ffe46199ab7d108a291a05af8d5b6","modified":1553009877546},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"d18c87d7839e7407e39acd2998bcc9e0b34611b0","modified":1553009877539},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"8b4a94dd80b3bac7c5390c8a7fd377b88c2cb78e","modified":1553009877637},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"d685df1516cb138d7a83bac5d7878a1e0fa8bc04","modified":1553009877630},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"43a20fa0e9ae2f4254f04813f9c619dd36b49ae5","modified":1553009877678},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"4e86e1ace90a70bb8862f5e6de9dbe7bfc046bee","modified":1553009877644},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"bc3fc9d053b3d1fc0cd3918bf9a629a6f38f6414","modified":1553009877590},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"767ba29f258db5d2e5baf875a6f36ac1d44df6a3","modified":1553009877691},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"ea1c136f960667a0a13b334db497b9b19c41f629","modified":1553009877681},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"b3818fd0b3028dadf341b6d0b180e1243683de6a","modified":1553009877654},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"078bd2d5815eb23e8c5f74467dc0042babea00ae","modified":1553009877760},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"9a4923d2aa5182531ea7a7fb9abe824450026208","modified":1553009877651},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1a5d94f5779a2ce13abc886dd78e0617f89c34b9","modified":1553009877660},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"89e41d4c298d8d70b4d1c833c7e599d089f2b3d4","modified":1553009877758},{"_id":"themes/next/source/js/src/utils.js","hash":"6a07990fe4374f8485b7dfa5797d029d8c8a024d","modified":1553009878507},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1553009878426},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1553009878496},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"1b72c755101c9dfb85da13df9a0abccf37cd1dd2","modified":1553009877666},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"d45ca53af17d1d83fd27f8ed0917a72f0060e1a9","modified":1553009877777},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1553009878585},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1553009877020},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"b9d7a71da5210f9cbec7f55a27786efe22b50d49","modified":1553009877026},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"1a4ac0d119f2126ef8951897338706edce112235","modified":1553009878242},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"961dd63ff256e0dadb58fa221afe03292441b684","modified":1553009878247},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1553009878264},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"97d39280d8f48ae250bb7d0982b37b066e0461ff","modified":1553009878256},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"cb2c0beb69bfc56c0ed86e609bc1c35edb799b99","modified":1553009878251},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"c4bfce1fca9ea5d0fd991d98e08b8e771d33d731","modified":1553009878267},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"9521ca21c6ac3e9ad017eeca499cc5eade932e92","modified":1553009878286},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"fec36a14080104b5862e9f021eab117d87c5f7c5","modified":1553009878260},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"4712c420411427f3565aec1cbc03a6036b72b2e7","modified":1553009878299},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1553009878295},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1553009878290},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"6c748a55f09efef2059b4b570452465b027c971d","modified":1553009878294},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"30d61fa31e405fcfe3d2ff6174ccad60be1745f9","modified":1553009878328},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"06d9d00257abd28414ec0b746f866bf9911cf5ec","modified":1553009878303},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"25f05ed8da68d034dce7f06e0f20f6cd55841070","modified":1553009878311},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"1795c041e774e00478023590fbb49c2d65cfad39","modified":1553009878334},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0d6f0df798449b710e1e5dbd43d470089b2a3c95","modified":1553009878342},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"6bdef35b9cda9340915d067e727df6c65b65d68f","modified":1553009878339},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"adb7379f3b9001840eb38b260434e89365771a81","modified":1553009878346},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"28f0444ccdc85a34ada651d8ee52479e16311167","modified":1553009878331},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1553009878314},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"157e6915dcf5990566e463acffa71043b2651c07","modified":1553009878317},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"9f35b95beb344f4eeca5ca584fbe7206f791372e","modified":1553009878323},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"b55c4c759f7586a0b83ea1135d76ee3f94a59d79","modified":1553009878321},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"13dfba1fc57ef39e7f2bbe15fe73bca1e47880a9","modified":1553009877907},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1553009878324},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1553009877912},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"698e4d6d894dd3db14fca5695b84bafcc4b1e4aa","modified":1553009877910},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"ef7b9ccdffaa577aa874bc2174aeacad84226007","modified":1553009877934},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1553009877911},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"747da792964518752b1a292dc4ce1ae8dcf5fb1c","modified":1553009878052},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"afdd21533db18d846e1a2663b1199761b1bd2c1e","modified":1553009878072},{"_id":"themes/next/source/lib/canvas-nest/.github/stale.yml","hash":"dbd5e6bf89b76ad1f2b081578b239c7ae32755af","modified":1553088800639},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"9f35b95beb344f4eeca5ca584fbe7206f791372e","modified":1553009878306},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1553009877977},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1553009878530},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1553009878526},{"_id":"themes/next/.git/objects/pack/pack-73c78afd3dd283f7c76630067010c85594ae8fbf.idx","hash":"ef8f394da8be9b5cf0291bcc907ebbd3cefbe7b0","modified":1553009876791},{"_id":"themes/next/source/lib/canvas-nest/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1553088800604},{"_id":"themes/next/source/lib/canvas-nest/.git/config","hash":"78c4459d066ad795856608d603d780b53488073d","modified":1553088800619},{"_id":"themes/next/source/lib/canvas-nest/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1553088793919},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1553009878534},{"_id":"themes/next/source/lib/canvas-nest/.git/index","hash":"7991889cccdfe83e1637a558567bdaba8b66ff63","modified":1553088800685},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"c97c819a65f6967485184399397601e5133deda6","modified":1553009878113},{"_id":"themes/next/source/lib/canvas-nest/.git/packed-refs","hash":"949c61b7ce3b6e582b7d47f985a3d13ddfbb82ca","modified":1553088800595},{"_id":"themes/next/source/js/src/schemes/muse.js","hash":"ccc0c5cd4ec6f8159c98990ad83f11a5c0b0234c","modified":1553009878475},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"3eea56cc9ce47bb4760930c4c69cebf847a7fbb2","modified":1553009878481},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1553009878544},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1553009878548},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1553009878606},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"b9d7a71da5210f9cbec7f55a27786efe22b50d49","modified":1553009877019},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1553009878313},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1553009878324},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1553009878312},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"48bb741f6bda73b322a25a8fbe37fd3d5e0ff601","modified":1553009877940},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1553009877944},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"c9cfb4b99e1ec8ec9cf075cb761b8f7fa5fe63fd","modified":1553009877943},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1553009877952},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1ec3102ee8f5b8cc0877da1fd109d37470401e7b","modified":1553009877947},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"e5a5f8747fdf2ca960e4e73c081b8952afd62224","modified":1553009877963},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"b8647d6140141b0a160607f6353e4d4594cca92e","modified":1553009877951},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"024e8ff40ca881c6fbf45712897e22f58a3811ab","modified":1553009877955},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fa1cea6fcc3f552d57cc7d28380a304859139bf6","modified":1553009877966},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"9c1a082e6c1f96187a099c3f4cb5424c0c9fd06e","modified":1553009877958},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1553009877973},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"ad4cae23c8e383f4fabc9a2a95bca6055020d22e","modified":1553009877972},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"db1df0186a4572844d69d0d7bb974bd120cb64d5","modified":1553009877976},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1553009877969},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"610e6be1881fa2609113e131ceecb3d0afcc026b","modified":1553009878075},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"24e12f67d8cf60ef3747abfa06f12a1b6b2798d2","modified":1553009878079},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"7e2ba73073daaea0a18c3d67ff137dd683af7011","modified":1553009878095},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-chat.styl","hash":"2661e10534d3b81414e1d3d00374eb8eaba45bcb","modified":1553009878085},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"5fa4940f5c7f7b86a20463d4c8cac3f8c3937ab7","modified":1553009878103},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"2df409df04fdb52d7234876a9f6e502edd4e3929","modified":1553009877937},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"08d1ccf94ec60f548fd0ee27b3bbfd7f039bfe18","modified":1553009878110},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"dc9f66eea48d8a61e68cfe0e089a4bdf74d696d9","modified":1553009878106},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1553009878134},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1553009878114},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"7af14846bb8623ef1379575dd6f36c65589e69b4","modified":1553009878091},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77dbac12a5785ed0950a98dce9ecafa8b645b207","modified":1553009878099},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"176aa69919ef4ea1eee960926fd815cd4e344997","modified":1553009878171},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1553009878137},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"16d0356702eaaba6bd1b32524427fc2bb15bcfd1","modified":1553009878182},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"2994c15172bcc916c64f484ca34c0c45951e95ca","modified":1553009878088},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"d705cbd78503d24f20efbb79c896c5cb440c07e6","modified":1553009878174},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1553009878136},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"d9c244b1c3a09a7fccd3c3f732e6fb112a8cd565","modified":1553009878212},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"30ccc107061dc23943198f087759079161ee24e9","modified":1553009878179},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"33108733ab3e38e459eaddce3ce646e1289713f4","modified":1553009878218},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"f26c32a0c3045e5ae826b983abc3a3c139456663","modified":1553009878186},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"ed8a12982c0497eeb9d7642781abeb801428f83d","modified":1553009878224},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ace002c23c9a0c121c3dc6ce44d9a9a30b92fa42","modified":1553009878227},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"2a1008f1044b450b806adc166754ba9513e68375","modified":1553009878221},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"74412b0bf4ec0d28aedd2e60b27affd4d5cd1452","modified":1553009878235},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1553009877978},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1553009878231},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1553009878215},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"fc94dd09b4245143b452d6cf2fc4c12134d99d6d","modified":1553009877985},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"3241c9ae85ca11b6c4e125ac471aa4342ba1ce9c","modified":1553009877981},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"77da38898bdd99cf8fd3e0ae8cc4d2ac943bcb60","modified":1553009877988},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"9a001c3b19022df67f7a2bb75b1c46e6773c5de7","modified":1553009877991},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"d99e0f259aafd30877b2019264b3cca0966bd606","modified":1553009878153},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"cba1eb089e0d58b7afb074b927f1968c4c7514ed","modified":1553009878238},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"b6a241626783d2ac115d683fd59ec283af68e5bb","modified":1553009877994},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"03a4e75e963e3e7cc393d588b1495a88d52e0e40","modified":1553009878082},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ccd0b1309acff0c676fdcc848a8ae2d05f0369ab","modified":1553009878006},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1553009878007},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"a0e84b21ecc1f69d8d42c83630c1004d3419e3fd","modified":1553009877998},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1553009878007},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"2ea91d7b75966d471bf857a9f3fbf87fd01aea90","modified":1553009878003},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1553009877960},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1553009878011},{"_id":"themes/next/source/lib/canvas-nest/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1553088794032},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"7b69c1ad392f8a386854e318d4c8ddeb9ba8d793","modified":1553009878000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"539fc0880b2e035e8316d5d4b423703195c1b7ba","modified":1553009878011},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/HEAD","hash":"d5a6a18da3af966e8f439d434c779a71c297273c","modified":1553088800610},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"100a48b56ae32a6abb11907a6d4ae2a86ef4ff25","modified":1553009878033},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1553088793962},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"981795aad232c8bd3f52a0ed8720db696d18a234","modified":1553009878014},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1553088793961},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1553088793952},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1553088793998},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1553088793994},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1553088793964},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1553088793957},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1553088794001},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1553088793948},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1553088793968},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1553009878540},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1553088794000},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/2a/f622a4d7df40a2708946e91d6d7a0df1dc468c","hash":"3da7207fb18d361b83c56f4e35f67e9e945abd82","modified":1553088800417},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/heads/master","hash":"42b96d49f5eae1a58b8413a60a0c2699e94df28d","modified":1553088800609},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/2f/9eba51ec174b1e0c719d12cafa7c3c07140471","hash":"fc994d9d8b3b21ec7c941eea7e3862970e297e9b","modified":1553088800141},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/0c/dada082d621dbfdd00f7020c33dc751129167f","hash":"b490c11cdefde6b331a7d4ddb055e34ad08459d8","modified":1553088800116},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/45/9262fe92f0115707bf8d8764f1886bc5e7c9e0","hash":"36040483f8af76775b7e4b6d87cec53729625399","modified":1553088800466},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/5a/69ce9c2e4a1a34f6063ae9a121af1555669c69","hash":"dad25cc0f450e2827b5676975f4a70636e3fd2c8","modified":1553088800189},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/46/cad4f872aa93e813aed99547c4705322ca483f","hash":"b0465d3186e2d58a8a99c56c6e68aa2965a396d4","modified":1553088799855},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/50/dd2a6539498a70226c81a587db486b47e839ff","hash":"3844b0c815d0b4b32c6312c751a826bf9dc2c945","modified":1553088799882},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/5e/8ae972c99b04af7dd56dabfc485e8fdae5094d","hash":"791b3349c5696ccacae00bffbdbb8d88a03e61a9","modified":1553088799908},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/75/de2b8fa62d52690de32c351c63ab6446104ed5","hash":"52d10122d633ce4895a0690c5955e1b356f5a391","modified":1553088800441},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1553088800166},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/7b/c7e3186212b6f2e06d3370502565e2c6326890","hash":"379f3c6486f589fc9c1ab07d0382adacf4f655a2","modified":1553088799932},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/86/1c9f4241fe0eb6af02ad770d5ce04c1f68972b","hash":"7005c3e36015a4af30d4b91bd5a849a7861a073e","modified":1553088800383},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/90/f6477118d05f5f96ce0a63c6f18b7b2baea200","hash":"385f58e92981f27fa54eb52bf60424e87c70a9d8","modified":1553088800022},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/51/7c5eb7dcc2cb9769efea2e7375ff6e04123150","hash":"ec53157077d47430f4729bf164999d18d370aeab","modified":1553088800279},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/91/f99a0c53b26dd54f56b9e452c68f56b06f8f7e","hash":"3dca8a5629e66599b6e0f146aa32f1b7ce023d89","modified":1553088800359},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/99/be66a33ab4ebc34f62f2880a0e0cc6d334d0f2","hash":"f2346fe8ddd7d7abf38f2946f3083d8150f502d2","modified":1553088800091},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/98/67d1132e0e50bbb7df754a63358d70741df6d5","hash":"3cb710a1faee73c08036f5e2df7df3a7ce29e9dd","modified":1553088799958},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/b1/bb278ca2e50dff1b343f9d5ca025272859432f","hash":"74f0afa72a30268d84613fb0d1d893bba866f01d","modified":1553088800333},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/39233ece53c9bdb9a1faf3271ed5768b034aad","hash":"5a770d418c1bb7b0f031f4d5416530002032fcf3","modified":1553088800232},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/aa/da83ad9aa55faa2b34ede31b1d41e16966f80b","hash":"b304541ab95b7969a63ba2ec4f60f5391bd8bb44","modified":1553088799983},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/bb/5755c22b6c1b7461319624f0f000bc947882ee","hash":"2b87a2a354a0fa77cbddf461b03b0b8e43c16a4f","modified":1553088800214},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/d4/95d28a8fab74d23908f6ccef9e4db2625fbacb","hash":"59e6067b0a806deee7bda6460b36c0f63e2e1db5","modified":1553088800308},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/ca/3466a8cbf05c2982c58199d6ee71ec6d0271ca","hash":"a9b80b5d827b5e84229b1afd7920d9218dce610f","modified":1553088800062},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/44/6ddf9b6c0e5ade17ca5cb99f9b3a5300919c57","hash":"fb72799ff98445f72fda041337da4cf105d9dcba","modified":1553088800258},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/heads/master","hash":"d5a6a18da3af966e8f439d434c779a71c297273c","modified":1553088800612},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1553088800603},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/remotes/origin/HEAD","hash":"d5a6a18da3af966e8f439d434c779a71c297273c","modified":1553088800601},{"_id":"source/images/avatar.jpg","hash":"b66e0fe3b14ee5efd3e6355e737f54e6c3285075","modified":1479488719329},{"_id":"themes/next/.git/objects/pack/pack-73c78afd3dd283f7c76630067010c85594ae8fbf.pack","hash":"20b1cb2257da1bd7799ac0ba194e80f557f21029","modified":1553009876851},{"_id":"source/_drafts/对于Map-Reduce并行度的理解.md","hash":"6e30363836ada67ee6894061505bf35478cad281","modified":1563973755639},{"_id":"source/_posts/对于Map-Reduce并行度的理解.md","hash":"6e30363836ada67ee6894061505bf35478cad281","modified":1563973755717}],"Category":[{"name":"工具使用","_id":"cjyh9js3f0006g0qrew5okjj4"},{"name":"基础知识","_id":"cjyh9js3i000ag0qrohjo5i4j"},{"name":"大数据","_id":"cjyh9js3l000fg0qr0d8rr8yq"}],"Data":[],"Page":[{"title":"categories","date":"2019-03-18T01:54:08.000Z","type":"categories","_content":"\n","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-03-18 09:54:08\ntype: categories\n---\n\n","updated":"2019-03-19T13:30:58.498Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjyh9js0k0000g0qr0q5fglvr","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2019-01-21T06:17:42.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-01-21 14:17:42\ntype: tags\n---\n","updated":"2019-03-19T13:30:58.858Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjyh9js1t0001g0qra4qdf8h6","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Kubernetes学习 —— 如何将自己的应用部署为k8s service","author":"天渊","date":"2019-07-12T13:14:00.000Z","_content":"`Service`是kubernetes对用户应用服务的一层抽象封装，一个`Service`对应多个具有相同功能的应用实例（`Pod`），为外界访问服务提供统一的入口，将请求负载均衡分发到多个`Pod`上\n<!--more-->\n用户在k8s上将自己的应用发布为`Deployment`后，只能通过`kubernetes Proxy`间接访问`Pod`的形式来调用服务，由于`Pod`生命周期的不确定性，这种方法可行性不高，因此需要将应用程序以`Service`的形式进行暴露，将应用程序实例和服务抽象进行充分解耦，集群中其他服务对该服务的调用就不会受到集群down机和动态缩/扩容的影响，用户在调试时也可以通过Node Port的方式直接在外界访问这个服务\n\n#### 发布一个Nginx服务\n\n将应用程序发布为`Service`有以下几个基本步骤：\n\n1. 创建docker image\n2. 基于应用程序的Docker Image发布k8s deployment，并设置需要暴露的端口和副本数\n3. 查看`replica set`和`pod`的状态，并指定`Labels`和`Selector`\n4. 将`deployment`暴露为`service`\n\n##### 创建docker image\n\n（略）\n\n##### 发布deployment\n\n`deployment`是k8s提供的用于发布无状态服务的资源形式，对应由`Deployment Controller`对用户发布的无状态应用程序进行统一管理，基于`deployment`可以随时启动，删除和动态缩/扩容`pod`，并暴露为外界可调用的`service`\n\n一旦应用发布为`deployment`，`Deployment Controller`便创建相应的`ReplicaSet`和`Pod`，并交给k8s scheduler调度到有空闲资源的服务器节点上启动运行\n\n用`kubectl run`命令直接创建一个Nginx的`deployment`（类似于docker run命令创建容器）：\n\n```shell\nkubectl run nginx --image=nginx:latest --port=80 --replicas=1\n```\n\n1. run后面指定该`deployment`的名称，这个名称是该应用在集群中的唯一标识\n2. `--image`是必带参数，指定Docker镜像，k8s会自动从远程registry拉取所需要的docker镜像\n3. `--port`是可选参数，指定该`deployment`的`pod`需要暴露的端口号，比如nginx服务就需要暴露它的80端口\n4. `--replicas`是可选参数，指定`pod`副本数量\n\n启动完成后，`Deployment Controller`会自动创建`Replica Set`（管理pod的副本集）和多个`pod`（由`--replicas`参数指定）\n\n用kubectl get deployments命令查看创建的nginx deployment (如果不指定名称nginx，则显示所有的deployment)：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get deployments nginx\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           17h\n```\n\n使用describe命令查看这个deployment的配置细节：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe deployments nginx\nName:                   nginx\nNamespace:              default\nCreationTimestamp:      Mon, 08 Jul 2019 17:46:20 +0900\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-79b9dfdd46 (1/1 replicas created)\nEvents:          <none>\n```\n\n在这里面可以查看当前deployment的状态，比如名称，namespace，创建时间，当前副本整体状态，还有就是`Labels`和`Selector`，用于后续`replica set`和`pod`还有`service`和`pod`之间的配对关系\n\n##### 查看`replica set`和`pod`\n\n使用`kubectl run`命令创建`deployment`的话，会自动创建默认的`replica set`和`pod` ，这也是k8s官方推荐的方式 （如果不采用这种方式，则需要自己指定template然后使用`kubectl create`分别创建deployment, replica set和pod）\n\n使用以下命令查看刚创建的`replica set`\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get replicasets\nNAME               DESIRED   CURRENT   READY   AGE\ncurl-6bf6db5c4f    1         1         1       25h\nnginx-79b9dfdd46   1         1         1       18h\n```\n\n默认创建的`replica set`是nginx-79b9dfdd46，使用`kubectl describe`命令查看详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe replicaset nginx-79b9dfdd46\nName:           nginx-79b9dfdd46\nNamespace:      default\nSelector:       pod-template-hash=79b9dfdd46,run=nginx\nLabels:         pod-template-hash=79b9dfdd46\n                run=nginx\nAnnotations:    deployment.kubernetes.io/desired-replicas: 1\n                deployment.kubernetes.io/max-replicas: 2\n                deployment.kubernetes.io/revision: 1\nControlled By:  Deployment/nginx\nReplicas:       1 current / 1 desired\nPods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  pod-template-hash=79b9dfdd46\n           run=nginx\n  Containers:\n   nginx:\n    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:           <none>\n```\n\n参数与`deployment`差别不大，可重点关注以下参数：\n\n1. `deployment.kubernetes.io/desired-replicas`和`deployment.kubernetes.io/max-replicas`参数，配置了期望副本数和最大副本数\n2. Controlled By参数，说明是由nginx的这个`deployment`来进行管理\n3. Pods Status：当前管理的pod状态\n4. Replicas：当前副本集状态，`1 current / 1 desired`说明当前已经成功启动一个pod，并且期望的pod副本数也为1个\n\n使用以下命令查看`pod`：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\ncurl-6bf6db5c4f-4mkxk    1/1     Running   0          18h\nnginx-79b9dfdd46-qc94z   1/1     Running   0          18h\n```\n\n可以看到，刚创建的默认pod只有一个，status为Running，说明当前运行健康（若为Pending或者Unknown等状态说明pod调度失败）\n\n查看`pod`详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe pods nginx-79b9dfdd46-qc94z\nName:           nginx-79b9dfdd46-qc94z\nNamespace:      default\nPriority:       0\nNode:           dev-ncc-slave-1-ncl/10.106.147.158\nStart Time:     Mon, 08 Jul 2019 18:58:40 +0900\nLabels:         pod-template-hash=79b9dfdd46\n                run=nginx\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.2.2\nControlled By:  ReplicaSet/nginx-79b9dfdd46\nContainers:\n  nginx:\n    Container ID:   docker://4c06715be9d3fc575285621f595c5c2d9f67ef5fbd6d792618f0fb3449f85892\n    Image:          registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Image ID:       docker-pullable://registry.navercorp.com/ncp-image/ncp-nginx@sha256:650cfc6f4e39b5bd5ec6bc57063886ba6e8808d691ac99200ac39fac2252c6ea\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 08 Jul 2019 18:59:01 +0900\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zbdxq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-zbdxq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-zbdxq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:          <none>\n```\n\n重点关注以下参数：\n\n1. Node：当前pod被调度至服务器节点的host name\n2. Labels：标签，只有当labels与之前`replica set`的`Selector`保持一致，才会被相应的`Replica Controller`纳入管理进行动态缩/扩容；后续的`service`也是通过标签来discover当前有效的pod；标签可以在运行时动态修改\n3. IP：pod在k8s集群内部的ip\n4. Controlled By：标明当前pod是由哪个`replica set`进行管理\n5. Containers：pod封装的容器信息，一个pod可以有多个容器\n6. Tolerations：指定该pod多长时间未达到Ready状态或者k8s多长时间未检测到pod心跳后，允许k8s重新调度pod\n7. Events：pod经历的事件，deployment的滚动升级和缩/扩容等都会产生事件\n\n在确认`replica set`和`pod`状态确认无误后，即可将该应用暴露为服务\n\n##### 暴露服务\n\n使用`kubectl expose`命令将nginx deployment暴露为服务：\n\n```shell\nkubectl expose deployment/nginx --type=\"NodePort\" --port 80\n```\n\n1. --type：当前暴露形式，指定`NodePort`的话，k8s会给当前服务随机分派一个30000-32767之间的端口号，外界可以直接通过`服务器node ip + node port `的方式访问这个服务\n2. --port：指定应用所需要暴露的端口，nginx服务则需要暴露他的80端口\n\n查看暴露的服务：\n\n```shell\n$ kubectl get services\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nkubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          12m\nnginx                 NodePort    10.109.107.109   <none>        80:31482/TCP   5m1s\n```\n\n查看详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe services nginx\nName:                     nginx\nNamespace:                default\nLabels:                   run=nginx\nAnnotations:              <none>\nSelector:                 run=nginx\nType:                     NodePort\nIP:                       10.109.107.109\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  31482/TCP\nEndpoints:                10.244.2.2:80\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n```\n\n默认情况下，`service`名称和`deployment`名称保持一致，其他参数：\n\n- Selector：`service`通过`Selector`选择来匹配对应的`pod`，k8s会通过`Endpoints Controller`来定期更新健康的符合`Selector`匹配规则的`pod`路由表，在这里nginx service将寻找所有labels为run=nginx的`pod`作为路由对象\n- IP：`service`在集群中的唯一ip地址\n\n`service`的`NodePort`是31482，因此我们可以直接在本机使用localhost访问这个nginx服务了：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ curl localhost:31482\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n...\n```\n\n至此已经成功发布nginx service\n\n`Service`和`Deployment`,`Replica Set`还有`Pod`之间的关系：\n\n![upload successful](\\blog\\images\\image.png)\n\n一个`deployment`可以创建多个`replica set`和`pod`以及`service`，`replica set`和`service`通过`Selector`指定的值来匹配带有相关`Labels`的`pod`\n\n#### k8s内部的服务发现\n\nk8s内部通过何种方式发现我们发布的`nginx service` ?\n\n目前有三种方式：\n\n1. `NodePort方式`：即上面通过`node ip + node port`将访问路径固定，这种方式不够灵活，通常只能用于外界调试\n\n2. `环境变量方式`：k8s默认会在每个 pod 启动时候会把所有服务的 IP 和 port 信息配置到当前pod的环境变量中，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。\n\n3. `kube-dns`方式：k8s官方推荐通过`kubeDNS + dnsmasq`的方式配置kube-dns插件，kube-dns可以缓存所有已经存在的`service`信息供服务调用方发现并调用服务，其他服务可以直接使用以下方式调用nginx服务：\n\n   ```shell\n   http://<service_name>.<namespace>.svc.<domain>:80/\n   ```\n\n   `service_name`：即服务名nginx\n\n   `namespace`：k8s命名空间，创建deployment时不特别指定的话，`namespace`均为\"default\"\n\n   `domain`：域名后缀，默认为`cluster.local`\n\n   在 `pod` 中访问也可以使用缩写 `service_name.namespace`，如果 pod 和 service 在同一个 `namespace`，可以直接使用 `service_name`，因此如果同一个`namespace`有其他服务要访问nginx，则直接使用`nginx`作为域名即可：\n\n   ```shell\n   http://nginx:80/\n   ```\n\n##### 测试服务发现\n\n手动测试`service`的服务发现需要进入到`pod`内部，执行以下命令进入`pod`内部环境，进入到`pod`后可以通过curl命令访问`http://nginx:80/` (镜像没有安装curl，可用yum进行安装)：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\ncurl-6bf6db5c4f-4mkxk    1/1     Running   0          22h\nnginx-5ff9d6cc77-5nxpn   1/1     Running   0          59m\n[irteam@dev-ncc-client-ncl ~]$ kubectl exec nginx-5ff9d6cc77-5nxpn -it -- /bin/bash\nroot@nginx-5ff9d6cc77-5nxpn:/# curl nginx:80/\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n......\n```\n\n使用`Ctrl P + Ctrl Q`命令退出`pod`环境\n\n#### 删除service和deployment\n\n`kubectl`工具提供一键式删除`service`和`deployment`，当`deployment`被删除后，对应的`pod`同时被回收\n\n使用以下命令删除nginx service:\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl delete services nginx\nservice \"nginx\" deleted\n```\n\n使用以下命令删除nginx deployment:\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl delete deployments nginx\ndeployment.extensions \"nginx\" deleted\n```\n\n随后再查看`pod`可以发现之前创建的`pod`均被删除","source":"_posts/Kubernetes学习-——-如何将自己的应用部署为k8s-service.md","raw":"title: Kubernetes学习 —— 如何将自己的应用部署为k8s service\nauthor: 天渊\ntags:\n  - k8s\n  - devops\n  - 云原生\ncategories: []\ndate: 2019-07-12 21:14:00\n---\n`Service`是kubernetes对用户应用服务的一层抽象封装，一个`Service`对应多个具有相同功能的应用实例（`Pod`），为外界访问服务提供统一的入口，将请求负载均衡分发到多个`Pod`上\n<!--more-->\n用户在k8s上将自己的应用发布为`Deployment`后，只能通过`kubernetes Proxy`间接访问`Pod`的形式来调用服务，由于`Pod`生命周期的不确定性，这种方法可行性不高，因此需要将应用程序以`Service`的形式进行暴露，将应用程序实例和服务抽象进行充分解耦，集群中其他服务对该服务的调用就不会受到集群down机和动态缩/扩容的影响，用户在调试时也可以通过Node Port的方式直接在外界访问这个服务\n\n#### 发布一个Nginx服务\n\n将应用程序发布为`Service`有以下几个基本步骤：\n\n1. 创建docker image\n2. 基于应用程序的Docker Image发布k8s deployment，并设置需要暴露的端口和副本数\n3. 查看`replica set`和`pod`的状态，并指定`Labels`和`Selector`\n4. 将`deployment`暴露为`service`\n\n##### 创建docker image\n\n（略）\n\n##### 发布deployment\n\n`deployment`是k8s提供的用于发布无状态服务的资源形式，对应由`Deployment Controller`对用户发布的无状态应用程序进行统一管理，基于`deployment`可以随时启动，删除和动态缩/扩容`pod`，并暴露为外界可调用的`service`\n\n一旦应用发布为`deployment`，`Deployment Controller`便创建相应的`ReplicaSet`和`Pod`，并交给k8s scheduler调度到有空闲资源的服务器节点上启动运行\n\n用`kubectl run`命令直接创建一个Nginx的`deployment`（类似于docker run命令创建容器）：\n\n```shell\nkubectl run nginx --image=nginx:latest --port=80 --replicas=1\n```\n\n1. run后面指定该`deployment`的名称，这个名称是该应用在集群中的唯一标识\n2. `--image`是必带参数，指定Docker镜像，k8s会自动从远程registry拉取所需要的docker镜像\n3. `--port`是可选参数，指定该`deployment`的`pod`需要暴露的端口号，比如nginx服务就需要暴露它的80端口\n4. `--replicas`是可选参数，指定`pod`副本数量\n\n启动完成后，`Deployment Controller`会自动创建`Replica Set`（管理pod的副本集）和多个`pod`（由`--replicas`参数指定）\n\n用kubectl get deployments命令查看创建的nginx deployment (如果不指定名称nginx，则显示所有的deployment)：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get deployments nginx\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           17h\n```\n\n使用describe命令查看这个deployment的配置细节：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe deployments nginx\nName:                   nginx\nNamespace:              default\nCreationTimestamp:      Mon, 08 Jul 2019 17:46:20 +0900\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-79b9dfdd46 (1/1 replicas created)\nEvents:          <none>\n```\n\n在这里面可以查看当前deployment的状态，比如名称，namespace，创建时间，当前副本整体状态，还有就是`Labels`和`Selector`，用于后续`replica set`和`pod`还有`service`和`pod`之间的配对关系\n\n##### 查看`replica set`和`pod`\n\n使用`kubectl run`命令创建`deployment`的话，会自动创建默认的`replica set`和`pod` ，这也是k8s官方推荐的方式 （如果不采用这种方式，则需要自己指定template然后使用`kubectl create`分别创建deployment, replica set和pod）\n\n使用以下命令查看刚创建的`replica set`\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get replicasets\nNAME               DESIRED   CURRENT   READY   AGE\ncurl-6bf6db5c4f    1         1         1       25h\nnginx-79b9dfdd46   1         1         1       18h\n```\n\n默认创建的`replica set`是nginx-79b9dfdd46，使用`kubectl describe`命令查看详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe replicaset nginx-79b9dfdd46\nName:           nginx-79b9dfdd46\nNamespace:      default\nSelector:       pod-template-hash=79b9dfdd46,run=nginx\nLabels:         pod-template-hash=79b9dfdd46\n                run=nginx\nAnnotations:    deployment.kubernetes.io/desired-replicas: 1\n                deployment.kubernetes.io/max-replicas: 2\n                deployment.kubernetes.io/revision: 1\nControlled By:  Deployment/nginx\nReplicas:       1 current / 1 desired\nPods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  pod-template-hash=79b9dfdd46\n           run=nginx\n  Containers:\n   nginx:\n    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:           <none>\n```\n\n参数与`deployment`差别不大，可重点关注以下参数：\n\n1. `deployment.kubernetes.io/desired-replicas`和`deployment.kubernetes.io/max-replicas`参数，配置了期望副本数和最大副本数\n2. Controlled By参数，说明是由nginx的这个`deployment`来进行管理\n3. Pods Status：当前管理的pod状态\n4. Replicas：当前副本集状态，`1 current / 1 desired`说明当前已经成功启动一个pod，并且期望的pod副本数也为1个\n\n使用以下命令查看`pod`：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\ncurl-6bf6db5c4f-4mkxk    1/1     Running   0          18h\nnginx-79b9dfdd46-qc94z   1/1     Running   0          18h\n```\n\n可以看到，刚创建的默认pod只有一个，status为Running，说明当前运行健康（若为Pending或者Unknown等状态说明pod调度失败）\n\n查看`pod`详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe pods nginx-79b9dfdd46-qc94z\nName:           nginx-79b9dfdd46-qc94z\nNamespace:      default\nPriority:       0\nNode:           dev-ncc-slave-1-ncl/10.106.147.158\nStart Time:     Mon, 08 Jul 2019 18:58:40 +0900\nLabels:         pod-template-hash=79b9dfdd46\n                run=nginx\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.2.2\nControlled By:  ReplicaSet/nginx-79b9dfdd46\nContainers:\n  nginx:\n    Container ID:   docker://4c06715be9d3fc575285621f595c5c2d9f67ef5fbd6d792618f0fb3449f85892\n    Image:          registry.navercorp.com/ncp-image/ncp-nginx:latest\n    Image ID:       docker-pullable://registry.navercorp.com/ncp-image/ncp-nginx@sha256:650cfc6f4e39b5bd5ec6bc57063886ba6e8808d691ac99200ac39fac2252c6ea\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 08 Jul 2019 18:59:01 +0900\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zbdxq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-zbdxq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-zbdxq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:          <none>\n```\n\n重点关注以下参数：\n\n1. Node：当前pod被调度至服务器节点的host name\n2. Labels：标签，只有当labels与之前`replica set`的`Selector`保持一致，才会被相应的`Replica Controller`纳入管理进行动态缩/扩容；后续的`service`也是通过标签来discover当前有效的pod；标签可以在运行时动态修改\n3. IP：pod在k8s集群内部的ip\n4. Controlled By：标明当前pod是由哪个`replica set`进行管理\n5. Containers：pod封装的容器信息，一个pod可以有多个容器\n6. Tolerations：指定该pod多长时间未达到Ready状态或者k8s多长时间未检测到pod心跳后，允许k8s重新调度pod\n7. Events：pod经历的事件，deployment的滚动升级和缩/扩容等都会产生事件\n\n在确认`replica set`和`pod`状态确认无误后，即可将该应用暴露为服务\n\n##### 暴露服务\n\n使用`kubectl expose`命令将nginx deployment暴露为服务：\n\n```shell\nkubectl expose deployment/nginx --type=\"NodePort\" --port 80\n```\n\n1. --type：当前暴露形式，指定`NodePort`的话，k8s会给当前服务随机分派一个30000-32767之间的端口号，外界可以直接通过`服务器node ip + node port `的方式访问这个服务\n2. --port：指定应用所需要暴露的端口，nginx服务则需要暴露他的80端口\n\n查看暴露的服务：\n\n```shell\n$ kubectl get services\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nkubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          12m\nnginx                 NodePort    10.109.107.109   <none>        80:31482/TCP   5m1s\n```\n\n查看详情：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl describe services nginx\nName:                     nginx\nNamespace:                default\nLabels:                   run=nginx\nAnnotations:              <none>\nSelector:                 run=nginx\nType:                     NodePort\nIP:                       10.109.107.109\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  31482/TCP\nEndpoints:                10.244.2.2:80\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n```\n\n默认情况下，`service`名称和`deployment`名称保持一致，其他参数：\n\n- Selector：`service`通过`Selector`选择来匹配对应的`pod`，k8s会通过`Endpoints Controller`来定期更新健康的符合`Selector`匹配规则的`pod`路由表，在这里nginx service将寻找所有labels为run=nginx的`pod`作为路由对象\n- IP：`service`在集群中的唯一ip地址\n\n`service`的`NodePort`是31482，因此我们可以直接在本机使用localhost访问这个nginx服务了：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ curl localhost:31482\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n...\n```\n\n至此已经成功发布nginx service\n\n`Service`和`Deployment`,`Replica Set`还有`Pod`之间的关系：\n\n![upload successful](\\blog\\images\\image.png)\n\n一个`deployment`可以创建多个`replica set`和`pod`以及`service`，`replica set`和`service`通过`Selector`指定的值来匹配带有相关`Labels`的`pod`\n\n#### k8s内部的服务发现\n\nk8s内部通过何种方式发现我们发布的`nginx service` ?\n\n目前有三种方式：\n\n1. `NodePort方式`：即上面通过`node ip + node port`将访问路径固定，这种方式不够灵活，通常只能用于外界调试\n\n2. `环境变量方式`：k8s默认会在每个 pod 启动时候会把所有服务的 IP 和 port 信息配置到当前pod的环境变量中，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。\n\n3. `kube-dns`方式：k8s官方推荐通过`kubeDNS + dnsmasq`的方式配置kube-dns插件，kube-dns可以缓存所有已经存在的`service`信息供服务调用方发现并调用服务，其他服务可以直接使用以下方式调用nginx服务：\n\n   ```shell\n   http://<service_name>.<namespace>.svc.<domain>:80/\n   ```\n\n   `service_name`：即服务名nginx\n\n   `namespace`：k8s命名空间，创建deployment时不特别指定的话，`namespace`均为\"default\"\n\n   `domain`：域名后缀，默认为`cluster.local`\n\n   在 `pod` 中访问也可以使用缩写 `service_name.namespace`，如果 pod 和 service 在同一个 `namespace`，可以直接使用 `service_name`，因此如果同一个`namespace`有其他服务要访问nginx，则直接使用`nginx`作为域名即可：\n\n   ```shell\n   http://nginx:80/\n   ```\n\n##### 测试服务发现\n\n手动测试`service`的服务发现需要进入到`pod`内部，执行以下命令进入`pod`内部环境，进入到`pod`后可以通过curl命令访问`http://nginx:80/` (镜像没有安装curl，可用yum进行安装)：\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\ncurl-6bf6db5c4f-4mkxk    1/1     Running   0          22h\nnginx-5ff9d6cc77-5nxpn   1/1     Running   0          59m\n[irteam@dev-ncc-client-ncl ~]$ kubectl exec nginx-5ff9d6cc77-5nxpn -it -- /bin/bash\nroot@nginx-5ff9d6cc77-5nxpn:/# curl nginx:80/\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n......\n```\n\n使用`Ctrl P + Ctrl Q`命令退出`pod`环境\n\n#### 删除service和deployment\n\n`kubectl`工具提供一键式删除`service`和`deployment`，当`deployment`被删除后，对应的`pod`同时被回收\n\n使用以下命令删除nginx service:\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl delete services nginx\nservice \"nginx\" deleted\n```\n\n使用以下命令删除nginx deployment:\n\n```shell\n[irteam@dev-ncc-client-ncl ~]$ kubectl delete deployments nginx\ndeployment.extensions \"nginx\" deleted\n```\n\n随后再查看`pod`可以发现之前创建的`pod`均被删除","slug":"Kubernetes学习-——-如何将自己的应用部署为k8s-service","published":1,"updated":"2019-07-12T13:16:47.820Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js2d0002g0qruk1srr4j","content":"<p><code>Service</code>是kubernetes对用户应用服务的一层抽象封装，一个<code>Service</code>对应多个具有相同功能的应用实例（<code>Pod</code>），为外界访问服务提供统一的入口，将请求负载均衡分发到多个<code>Pod</code>上<br><a id=\"more\"></a><br>用户在k8s上将自己的应用发布为<code>Deployment</code>后，只能通过<code>kubernetes Proxy</code>间接访问<code>Pod</code>的形式来调用服务，由于<code>Pod</code>生命周期的不确定性，这种方法可行性不高，因此需要将应用程序以<code>Service</code>的形式进行暴露，将应用程序实例和服务抽象进行充分解耦，集群中其他服务对该服务的调用就不会受到集群down机和动态缩/扩容的影响，用户在调试时也可以通过Node Port的方式直接在外界访问这个服务</p>\n<h4 id=\"发布一个Nginx服务\"><a href=\"#发布一个Nginx服务\" class=\"headerlink\" title=\"发布一个Nginx服务\"></a>发布一个Nginx服务</h4><p>将应用程序发布为<code>Service</code>有以下几个基本步骤：</p>\n<ol>\n<li>创建docker image</li>\n<li>基于应用程序的Docker Image发布k8s deployment，并设置需要暴露的端口和副本数</li>\n<li>查看<code>replica set</code>和<code>pod</code>的状态，并指定<code>Labels</code>和<code>Selector</code></li>\n<li>将<code>deployment</code>暴露为<code>service</code></li>\n</ol>\n<h5 id=\"创建docker-image\"><a href=\"#创建docker-image\" class=\"headerlink\" title=\"创建docker image\"></a>创建docker image</h5><p>（略）</p>\n<h5 id=\"发布deployment\"><a href=\"#发布deployment\" class=\"headerlink\" title=\"发布deployment\"></a>发布deployment</h5><p><code>deployment</code>是k8s提供的用于发布无状态服务的资源形式，对应由<code>Deployment Controller</code>对用户发布的无状态应用程序进行统一管理，基于<code>deployment</code>可以随时启动，删除和动态缩/扩容<code>pod</code>，并暴露为外界可调用的<code>service</code></p>\n<p>一旦应用发布为<code>deployment</code>，<code>Deployment Controller</code>便创建相应的<code>ReplicaSet</code>和<code>Pod</code>，并交给k8s scheduler调度到有空闲资源的服务器节点上启动运行</p>\n<p>用<code>kubectl run</code>命令直接创建一个Nginx的<code>deployment</code>（类似于docker run命令创建容器）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl run nginx --image=nginx:latest --port=80 --replicas=1</span><br></pre></td></tr></table></figure>\n<ol>\n<li>run后面指定该<code>deployment</code>的名称，这个名称是该应用在集群中的唯一标识</li>\n<li><code>--image</code>是必带参数，指定Docker镜像，k8s会自动从远程registry拉取所需要的docker镜像</li>\n<li><code>--port</code>是可选参数，指定该<code>deployment</code>的<code>pod</code>需要暴露的端口号，比如nginx服务就需要暴露它的80端口</li>\n<li><code>--replicas</code>是可选参数，指定<code>pod</code>副本数量</li>\n</ol>\n<p>启动完成后，<code>Deployment Controller</code>会自动创建<code>Replica Set</code>（管理pod的副本集）和多个<code>pod</code>（由<code>--replicas</code>参数指定）</p>\n<p>用kubectl get deployments命令查看创建的nginx deployment (如果不指定名称nginx，则显示所有的deployment)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get deployments nginx</span><br><span class=\"line\">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class=\"line\">nginx   1/1     1            1           17h</span><br></pre></td></tr></table></figure>\n<p>使用describe命令查看这个deployment的配置细节：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe deployments nginx</span><br><span class=\"line\">Name:                   nginx</span><br><span class=\"line\">Namespace:              default</span><br><span class=\"line\">CreationTimestamp:      Mon, 08 Jul 2019 17:46:20 +0900</span><br><span class=\"line\">Labels:                 run=nginx</span><br><span class=\"line\">Annotations:            deployment.kubernetes.io/revision: 1</span><br><span class=\"line\">Selector:               run=nginx</span><br><span class=\"line\">Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable</span><br><span class=\"line\">StrategyType:           RollingUpdate</span><br><span class=\"line\">MinReadySeconds:        0</span><br><span class=\"line\">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class=\"line\">Pod Template:</span><br><span class=\"line\">  Labels:  run=nginx</span><br><span class=\"line\">  Containers:</span><br><span class=\"line\">   nginx:</span><br><span class=\"line\">    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Port:         80/TCP</span><br><span class=\"line\">    Host Port:    0/TCP</span><br><span class=\"line\">    Environment:  &lt;none&gt;</span><br><span class=\"line\">    Mounts:       &lt;none&gt;</span><br><span class=\"line\">  Volumes:        &lt;none&gt;</span><br><span class=\"line\">Conditions:</span><br><span class=\"line\">  Type           Status  Reason</span><br><span class=\"line\">  ----           ------  ------</span><br><span class=\"line\">  Available      True    MinimumReplicasAvailable</span><br><span class=\"line\">  Progressing    True    NewReplicaSetAvailable</span><br><span class=\"line\">OldReplicaSets:  &lt;none&gt;</span><br><span class=\"line\">NewReplicaSet:   nginx-79b9dfdd46 (1/1 replicas created)</span><br><span class=\"line\">Events:          &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>在这里面可以查看当前deployment的状态，比如名称，namespace，创建时间，当前副本整体状态，还有就是<code>Labels</code>和<code>Selector</code>，用于后续<code>replica set</code>和<code>pod</code>还有<code>service</code>和<code>pod</code>之间的配对关系</p>\n<h5 id=\"查看replica-set和pod\"><a href=\"#查看replica-set和pod\" class=\"headerlink\" title=\"查看replica set和pod\"></a>查看<code>replica set</code>和<code>pod</code></h5><p>使用<code>kubectl run</code>命令创建<code>deployment</code>的话，会自动创建默认的<code>replica set</code>和<code>pod</code> ，这也是k8s官方推荐的方式 （如果不采用这种方式，则需要自己指定template然后使用<code>kubectl create</code>分别创建deployment, replica set和pod）</p>\n<p>使用以下命令查看刚创建的<code>replica set</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get replicasets</span><br><span class=\"line\">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class=\"line\">curl-6bf6db5c4f    1         1         1       25h</span><br><span class=\"line\">nginx-79b9dfdd46   1         1         1       18h</span><br></pre></td></tr></table></figure>\n<p>默认创建的<code>replica set</code>是nginx-79b9dfdd46，使用<code>kubectl describe</code>命令查看详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe replicaset nginx-79b9dfdd46</span><br><span class=\"line\">Name:           nginx-79b9dfdd46</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Selector:       pod-template-hash=79b9dfdd46,run=nginx</span><br><span class=\"line\">Labels:         pod-template-hash=79b9dfdd46</span><br><span class=\"line\">                run=nginx</span><br><span class=\"line\">Annotations:    deployment.kubernetes.io/desired-replicas: 1</span><br><span class=\"line\">                deployment.kubernetes.io/max-replicas: 2</span><br><span class=\"line\">                deployment.kubernetes.io/revision: 1</span><br><span class=\"line\">Controlled By:  Deployment/nginx</span><br><span class=\"line\">Replicas:       1 current / 1 desired</span><br><span class=\"line\">Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class=\"line\">Pod Template:</span><br><span class=\"line\">  Labels:  pod-template-hash=79b9dfdd46</span><br><span class=\"line\">           run=nginx</span><br><span class=\"line\">  Containers:</span><br><span class=\"line\">   nginx:</span><br><span class=\"line\">    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Port:         80/TCP</span><br><span class=\"line\">    Host Port:    0/TCP</span><br><span class=\"line\">    Environment:  &lt;none&gt;</span><br><span class=\"line\">    Mounts:       &lt;none&gt;</span><br><span class=\"line\">  Volumes:        &lt;none&gt;</span><br><span class=\"line\">Events:           &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>参数与<code>deployment</code>差别不大，可重点关注以下参数：</p>\n<ol>\n<li><code>deployment.kubernetes.io/desired-replicas</code>和<code>deployment.kubernetes.io/max-replicas</code>参数，配置了期望副本数和最大副本数</li>\n<li>Controlled By参数，说明是由nginx的这个<code>deployment</code>来进行管理</li>\n<li>Pods Status：当前管理的pod状态</li>\n<li>Replicas：当前副本集状态，<code>1 current / 1 desired</code>说明当前已经成功启动一个pod，并且期望的pod副本数也为1个</li>\n</ol>\n<p>使用以下命令查看<code>pod</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get pods</span><br><span class=\"line\">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">curl-6bf6db5c4f-4mkxk    1/1     Running   0          18h</span><br><span class=\"line\">nginx-79b9dfdd46-qc94z   1/1     Running   0          18h</span><br></pre></td></tr></table></figure>\n<p>可以看到，刚创建的默认pod只有一个，status为Running，说明当前运行健康（若为Pending或者Unknown等状态说明pod调度失败）</p>\n<p>查看<code>pod</code>详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe pods nginx-79b9dfdd46-qc94z</span><br><span class=\"line\">Name:           nginx-79b9dfdd46-qc94z</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Priority:       0</span><br><span class=\"line\">Node:           dev-ncc-slave-1-ncl/10.106.147.158</span><br><span class=\"line\">Start Time:     Mon, 08 Jul 2019 18:58:40 +0900</span><br><span class=\"line\">Labels:         pod-template-hash=79b9dfdd46</span><br><span class=\"line\">                run=nginx</span><br><span class=\"line\">Annotations:    &lt;none&gt;</span><br><span class=\"line\">Status:         Running</span><br><span class=\"line\">IP:             10.244.2.2</span><br><span class=\"line\">Controlled By:  ReplicaSet/nginx-79b9dfdd46</span><br><span class=\"line\">Containers:</span><br><span class=\"line\">  nginx:</span><br><span class=\"line\">    Container ID:   docker://4c06715be9d3fc575285621f595c5c2d9f67ef5fbd6d792618f0fb3449f85892</span><br><span class=\"line\">    Image:          registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Image ID:       docker-pullable://registry.navercorp.com/ncp-image/ncp-nginx@sha256:650cfc6f4e39b5bd5ec6bc57063886ba6e8808d691ac99200ac39fac2252c6ea</span><br><span class=\"line\">    Port:           80/TCP</span><br><span class=\"line\">    Host Port:      0/TCP</span><br><span class=\"line\">    State:          Running</span><br><span class=\"line\">      Started:      Mon, 08 Jul 2019 18:59:01 +0900</span><br><span class=\"line\">    Ready:          True</span><br><span class=\"line\">    Restart Count:  0</span><br><span class=\"line\">    Environment:    &lt;none&gt;</span><br><span class=\"line\">    Mounts:</span><br><span class=\"line\">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zbdxq (ro)</span><br><span class=\"line\">Conditions:</span><br><span class=\"line\">  Type              Status</span><br><span class=\"line\">  Initialized       True </span><br><span class=\"line\">  Ready             True </span><br><span class=\"line\">  ContainersReady   True </span><br><span class=\"line\">  PodScheduled      True </span><br><span class=\"line\">Volumes:</span><br><span class=\"line\">  default-token-zbdxq:</span><br><span class=\"line\">    Type:        Secret (a volume populated by a Secret)</span><br><span class=\"line\">    SecretName:  default-token-zbdxq</span><br><span class=\"line\">    Optional:    false</span><br><span class=\"line\">QoS Class:       BestEffort</span><br><span class=\"line\">Node-Selectors:  &lt;none&gt;</span><br><span class=\"line\">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class=\"line\">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class=\"line\">Events:          &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>重点关注以下参数：</p>\n<ol>\n<li>Node：当前pod被调度至服务器节点的host name</li>\n<li>Labels：标签，只有当labels与之前<code>replica set</code>的<code>Selector</code>保持一致，才会被相应的<code>Replica Controller</code>纳入管理进行动态缩/扩容；后续的<code>service</code>也是通过标签来discover当前有效的pod；标签可以在运行时动态修改</li>\n<li>IP：pod在k8s集群内部的ip</li>\n<li>Controlled By：标明当前pod是由哪个<code>replica set</code>进行管理</li>\n<li>Containers：pod封装的容器信息，一个pod可以有多个容器</li>\n<li>Tolerations：指定该pod多长时间未达到Ready状态或者k8s多长时间未检测到pod心跳后，允许k8s重新调度pod</li>\n<li>Events：pod经历的事件，deployment的滚动升级和缩/扩容等都会产生事件</li>\n</ol>\n<p>在确认<code>replica set</code>和<code>pod</code>状态确认无误后，即可将该应用暴露为服务</p>\n<h5 id=\"暴露服务\"><a href=\"#暴露服务\" class=\"headerlink\" title=\"暴露服务\"></a>暴露服务</h5><p>使用<code>kubectl expose</code>命令将nginx deployment暴露为服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl expose deployment/nginx --type=\"NodePort\" --port 80</span><br></pre></td></tr></table></figure>\n<ol>\n<li>–type：当前暴露形式，指定<code>NodePort</code>的话，k8s会给当前服务随机分派一个30000-32767之间的端口号，外界可以直接通过<code>服务器node ip + node port</code>的方式访问这个服务</li>\n<li>–port：指定应用所需要暴露的端口，nginx服务则需要暴露他的80端口</li>\n</ol>\n<p>查看暴露的服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> kubectl get services</span><br><span class=\"line\">NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class=\"line\">kubernetes            ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          12m</span><br><span class=\"line\">nginx                 NodePort    10.109.107.109   &lt;none&gt;        80:31482/TCP   5m1s</span><br></pre></td></tr></table></figure>\n<p>查看详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe services nginx</span><br><span class=\"line\">Name:                     nginx</span><br><span class=\"line\">Namespace:                default</span><br><span class=\"line\">Labels:                   run=nginx</span><br><span class=\"line\">Annotations:              &lt;none&gt;</span><br><span class=\"line\">Selector:                 run=nginx</span><br><span class=\"line\">Type:                     NodePort</span><br><span class=\"line\">IP:                       10.109.107.109</span><br><span class=\"line\">Port:                     &lt;unset&gt;  80/TCP</span><br><span class=\"line\">TargetPort:               80/TCP</span><br><span class=\"line\">NodePort:                 &lt;unset&gt;  31482/TCP</span><br><span class=\"line\">Endpoints:                10.244.2.2:80</span><br><span class=\"line\">Session Affinity:         None</span><br><span class=\"line\">External Traffic Policy:  Cluster</span><br><span class=\"line\">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>默认情况下，<code>service</code>名称和<code>deployment</code>名称保持一致，其他参数：</p>\n<ul>\n<li>Selector：<code>service</code>通过<code>Selector</code>选择来匹配对应的<code>pod</code>，k8s会通过<code>Endpoints Controller</code>来定期更新健康的符合<code>Selector</code>匹配规则的<code>pod</code>路由表，在这里nginx service将寻找所有labels为run=nginx的<code>pod</code>作为路由对象</li>\n<li>IP：<code>service</code>在集群中的唯一ip地址</li>\n</ul>\n<p><code>service</code>的<code>NodePort</code>是31482，因此我们可以直接在本机使用localhost访问这个nginx服务了：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ curl localhost:31482</span><br><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class=\"line\">&lt;style&gt;</span><br><span class=\"line\">    body &#123;</span><br><span class=\"line\">        width: 35em;</span><br><span class=\"line\">        margin: 0 auto;</span><br><span class=\"line\">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>至此已经成功发布nginx service</p>\n<p><code>Service</code>和<code>Deployment</code>,<code>Replica Set</code>还有<code>Pod</code>之间的关系：</p>\n<p><img src=\"\\blog\\images\\image.png\" alt=\"upload successful\"></p>\n<p>一个<code>deployment</code>可以创建多个<code>replica set</code>和<code>pod</code>以及<code>service</code>，<code>replica set</code>和<code>service</code>通过<code>Selector</code>指定的值来匹配带有相关<code>Labels</code>的<code>pod</code></p>\n<h4 id=\"k8s内部的服务发现\"><a href=\"#k8s内部的服务发现\" class=\"headerlink\" title=\"k8s内部的服务发现\"></a>k8s内部的服务发现</h4><p>k8s内部通过何种方式发现我们发布的<code>nginx service</code> ?</p>\n<p>目前有三种方式：</p>\n<ol>\n<li><p><code>NodePort方式</code>：即上面通过<code>node ip + node port</code>将访问路径固定，这种方式不够灵活，通常只能用于外界调试</p>\n</li>\n<li><p><code>环境变量方式</code>：k8s默认会在每个 pod 启动时候会把所有服务的 IP 和 port 信息配置到当前pod的环境变量中，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。</p>\n</li>\n<li><p><code>kube-dns</code>方式：k8s官方推荐通过<code>kubeDNS + dnsmasq</code>的方式配置kube-dns插件，kube-dns可以缓存所有已经存在的<code>service</code>信息供服务调用方发现并调用服务，其他服务可以直接使用以下方式调用nginx服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://&lt;service_name&gt;.&lt;namespace&gt;.svc.&lt;domain&gt;:80/</span><br></pre></td></tr></table></figure>\n<p><code>service_name</code>：即服务名nginx</p>\n<p><code>namespace</code>：k8s命名空间，创建deployment时不特别指定的话，<code>namespace</code>均为”default”</p>\n<p><code>domain</code>：域名后缀，默认为<code>cluster.local</code></p>\n<p>在 <code>pod</code> 中访问也可以使用缩写 <code>service_name.namespace</code>，如果 pod 和 service 在同一个 <code>namespace</code>，可以直接使用 <code>service_name</code>，因此如果同一个<code>namespace</code>有其他服务要访问nginx，则直接使用<code>nginx</code>作为域名即可：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://nginx:80/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h5 id=\"测试服务发现\"><a href=\"#测试服务发现\" class=\"headerlink\" title=\"测试服务发现\"></a>测试服务发现</h5><p>手动测试<code>service</code>的服务发现需要进入到<code>pod</code>内部，执行以下命令进入<code>pod</code>内部环境，进入到<code>pod</code>后可以通过curl命令访问<code>http://nginx:80/</code> (镜像没有安装curl，可用yum进行安装)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get pods</span><br><span class=\"line\">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">curl-6bf6db5c4f-4mkxk    1/1     Running   0          22h</span><br><span class=\"line\">nginx-5ff9d6cc77-5nxpn   1/1     Running   0          59m</span><br><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl exec nginx-5ff9d6cc77-5nxpn -it -- /bin/bash</span><br><span class=\"line\">root@nginx-5ff9d6cc77-5nxpn:/# curl nginx:80/</span><br><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class=\"line\">&lt;style&gt;</span><br><span class=\"line\">    body &#123;</span><br><span class=\"line\">        width: 35em;</span><br><span class=\"line\">        margin: 0 auto;</span><br><span class=\"line\">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br><span class=\"line\">......</span><br></pre></td></tr></table></figure>\n<p>使用<code>Ctrl P + Ctrl Q</code>命令退出<code>pod</code>环境</p>\n<h4 id=\"删除service和deployment\"><a href=\"#删除service和deployment\" class=\"headerlink\" title=\"删除service和deployment\"></a>删除service和deployment</h4><p><code>kubectl</code>工具提供一键式删除<code>service</code>和<code>deployment</code>，当<code>deployment</code>被删除后，对应的<code>pod</code>同时被回收</p>\n<p>使用以下命令删除nginx service:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl delete services nginx</span><br><span class=\"line\">service \"nginx\" deleted</span><br></pre></td></tr></table></figure>\n<p>使用以下命令删除nginx deployment:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl delete deployments nginx</span><br><span class=\"line\">deployment.extensions \"nginx\" deleted</span><br></pre></td></tr></table></figure>\n<p>随后再查看<code>pod</code>可以发现之前创建的<code>pod</code>均被删除</p>\n","site":{"data":{}},"excerpt":"<p><code>Service</code>是kubernetes对用户应用服务的一层抽象封装，一个<code>Service</code>对应多个具有相同功能的应用实例（<code>Pod</code>），为外界访问服务提供统一的入口，将请求负载均衡分发到多个<code>Pod</code>上<br>","more":"<br>用户在k8s上将自己的应用发布为<code>Deployment</code>后，只能通过<code>kubernetes Proxy</code>间接访问<code>Pod</code>的形式来调用服务，由于<code>Pod</code>生命周期的不确定性，这种方法可行性不高，因此需要将应用程序以<code>Service</code>的形式进行暴露，将应用程序实例和服务抽象进行充分解耦，集群中其他服务对该服务的调用就不会受到集群down机和动态缩/扩容的影响，用户在调试时也可以通过Node Port的方式直接在外界访问这个服务</p>\n<h4 id=\"发布一个Nginx服务\"><a href=\"#发布一个Nginx服务\" class=\"headerlink\" title=\"发布一个Nginx服务\"></a>发布一个Nginx服务</h4><p>将应用程序发布为<code>Service</code>有以下几个基本步骤：</p>\n<ol>\n<li>创建docker image</li>\n<li>基于应用程序的Docker Image发布k8s deployment，并设置需要暴露的端口和副本数</li>\n<li>查看<code>replica set</code>和<code>pod</code>的状态，并指定<code>Labels</code>和<code>Selector</code></li>\n<li>将<code>deployment</code>暴露为<code>service</code></li>\n</ol>\n<h5 id=\"创建docker-image\"><a href=\"#创建docker-image\" class=\"headerlink\" title=\"创建docker image\"></a>创建docker image</h5><p>（略）</p>\n<h5 id=\"发布deployment\"><a href=\"#发布deployment\" class=\"headerlink\" title=\"发布deployment\"></a>发布deployment</h5><p><code>deployment</code>是k8s提供的用于发布无状态服务的资源形式，对应由<code>Deployment Controller</code>对用户发布的无状态应用程序进行统一管理，基于<code>deployment</code>可以随时启动，删除和动态缩/扩容<code>pod</code>，并暴露为外界可调用的<code>service</code></p>\n<p>一旦应用发布为<code>deployment</code>，<code>Deployment Controller</code>便创建相应的<code>ReplicaSet</code>和<code>Pod</code>，并交给k8s scheduler调度到有空闲资源的服务器节点上启动运行</p>\n<p>用<code>kubectl run</code>命令直接创建一个Nginx的<code>deployment</code>（类似于docker run命令创建容器）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl run nginx --image=nginx:latest --port=80 --replicas=1</span><br></pre></td></tr></table></figure>\n<ol>\n<li>run后面指定该<code>deployment</code>的名称，这个名称是该应用在集群中的唯一标识</li>\n<li><code>--image</code>是必带参数，指定Docker镜像，k8s会自动从远程registry拉取所需要的docker镜像</li>\n<li><code>--port</code>是可选参数，指定该<code>deployment</code>的<code>pod</code>需要暴露的端口号，比如nginx服务就需要暴露它的80端口</li>\n<li><code>--replicas</code>是可选参数，指定<code>pod</code>副本数量</li>\n</ol>\n<p>启动完成后，<code>Deployment Controller</code>会自动创建<code>Replica Set</code>（管理pod的副本集）和多个<code>pod</code>（由<code>--replicas</code>参数指定）</p>\n<p>用kubectl get deployments命令查看创建的nginx deployment (如果不指定名称nginx，则显示所有的deployment)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get deployments nginx</span><br><span class=\"line\">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class=\"line\">nginx   1/1     1            1           17h</span><br></pre></td></tr></table></figure>\n<p>使用describe命令查看这个deployment的配置细节：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe deployments nginx</span><br><span class=\"line\">Name:                   nginx</span><br><span class=\"line\">Namespace:              default</span><br><span class=\"line\">CreationTimestamp:      Mon, 08 Jul 2019 17:46:20 +0900</span><br><span class=\"line\">Labels:                 run=nginx</span><br><span class=\"line\">Annotations:            deployment.kubernetes.io/revision: 1</span><br><span class=\"line\">Selector:               run=nginx</span><br><span class=\"line\">Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable</span><br><span class=\"line\">StrategyType:           RollingUpdate</span><br><span class=\"line\">MinReadySeconds:        0</span><br><span class=\"line\">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class=\"line\">Pod Template:</span><br><span class=\"line\">  Labels:  run=nginx</span><br><span class=\"line\">  Containers:</span><br><span class=\"line\">   nginx:</span><br><span class=\"line\">    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Port:         80/TCP</span><br><span class=\"line\">    Host Port:    0/TCP</span><br><span class=\"line\">    Environment:  &lt;none&gt;</span><br><span class=\"line\">    Mounts:       &lt;none&gt;</span><br><span class=\"line\">  Volumes:        &lt;none&gt;</span><br><span class=\"line\">Conditions:</span><br><span class=\"line\">  Type           Status  Reason</span><br><span class=\"line\">  ----           ------  ------</span><br><span class=\"line\">  Available      True    MinimumReplicasAvailable</span><br><span class=\"line\">  Progressing    True    NewReplicaSetAvailable</span><br><span class=\"line\">OldReplicaSets:  &lt;none&gt;</span><br><span class=\"line\">NewReplicaSet:   nginx-79b9dfdd46 (1/1 replicas created)</span><br><span class=\"line\">Events:          &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>在这里面可以查看当前deployment的状态，比如名称，namespace，创建时间，当前副本整体状态，还有就是<code>Labels</code>和<code>Selector</code>，用于后续<code>replica set</code>和<code>pod</code>还有<code>service</code>和<code>pod</code>之间的配对关系</p>\n<h5 id=\"查看replica-set和pod\"><a href=\"#查看replica-set和pod\" class=\"headerlink\" title=\"查看replica set和pod\"></a>查看<code>replica set</code>和<code>pod</code></h5><p>使用<code>kubectl run</code>命令创建<code>deployment</code>的话，会自动创建默认的<code>replica set</code>和<code>pod</code> ，这也是k8s官方推荐的方式 （如果不采用这种方式，则需要自己指定template然后使用<code>kubectl create</code>分别创建deployment, replica set和pod）</p>\n<p>使用以下命令查看刚创建的<code>replica set</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get replicasets</span><br><span class=\"line\">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class=\"line\">curl-6bf6db5c4f    1         1         1       25h</span><br><span class=\"line\">nginx-79b9dfdd46   1         1         1       18h</span><br></pre></td></tr></table></figure>\n<p>默认创建的<code>replica set</code>是nginx-79b9dfdd46，使用<code>kubectl describe</code>命令查看详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe replicaset nginx-79b9dfdd46</span><br><span class=\"line\">Name:           nginx-79b9dfdd46</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Selector:       pod-template-hash=79b9dfdd46,run=nginx</span><br><span class=\"line\">Labels:         pod-template-hash=79b9dfdd46</span><br><span class=\"line\">                run=nginx</span><br><span class=\"line\">Annotations:    deployment.kubernetes.io/desired-replicas: 1</span><br><span class=\"line\">                deployment.kubernetes.io/max-replicas: 2</span><br><span class=\"line\">                deployment.kubernetes.io/revision: 1</span><br><span class=\"line\">Controlled By:  Deployment/nginx</span><br><span class=\"line\">Replicas:       1 current / 1 desired</span><br><span class=\"line\">Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class=\"line\">Pod Template:</span><br><span class=\"line\">  Labels:  pod-template-hash=79b9dfdd46</span><br><span class=\"line\">           run=nginx</span><br><span class=\"line\">  Containers:</span><br><span class=\"line\">   nginx:</span><br><span class=\"line\">    Image:        registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Port:         80/TCP</span><br><span class=\"line\">    Host Port:    0/TCP</span><br><span class=\"line\">    Environment:  &lt;none&gt;</span><br><span class=\"line\">    Mounts:       &lt;none&gt;</span><br><span class=\"line\">  Volumes:        &lt;none&gt;</span><br><span class=\"line\">Events:           &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>参数与<code>deployment</code>差别不大，可重点关注以下参数：</p>\n<ol>\n<li><code>deployment.kubernetes.io/desired-replicas</code>和<code>deployment.kubernetes.io/max-replicas</code>参数，配置了期望副本数和最大副本数</li>\n<li>Controlled By参数，说明是由nginx的这个<code>deployment</code>来进行管理</li>\n<li>Pods Status：当前管理的pod状态</li>\n<li>Replicas：当前副本集状态，<code>1 current / 1 desired</code>说明当前已经成功启动一个pod，并且期望的pod副本数也为1个</li>\n</ol>\n<p>使用以下命令查看<code>pod</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get pods</span><br><span class=\"line\">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">curl-6bf6db5c4f-4mkxk    1/1     Running   0          18h</span><br><span class=\"line\">nginx-79b9dfdd46-qc94z   1/1     Running   0          18h</span><br></pre></td></tr></table></figure>\n<p>可以看到，刚创建的默认pod只有一个，status为Running，说明当前运行健康（若为Pending或者Unknown等状态说明pod调度失败）</p>\n<p>查看<code>pod</code>详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe pods nginx-79b9dfdd46-qc94z</span><br><span class=\"line\">Name:           nginx-79b9dfdd46-qc94z</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Priority:       0</span><br><span class=\"line\">Node:           dev-ncc-slave-1-ncl/10.106.147.158</span><br><span class=\"line\">Start Time:     Mon, 08 Jul 2019 18:58:40 +0900</span><br><span class=\"line\">Labels:         pod-template-hash=79b9dfdd46</span><br><span class=\"line\">                run=nginx</span><br><span class=\"line\">Annotations:    &lt;none&gt;</span><br><span class=\"line\">Status:         Running</span><br><span class=\"line\">IP:             10.244.2.2</span><br><span class=\"line\">Controlled By:  ReplicaSet/nginx-79b9dfdd46</span><br><span class=\"line\">Containers:</span><br><span class=\"line\">  nginx:</span><br><span class=\"line\">    Container ID:   docker://4c06715be9d3fc575285621f595c5c2d9f67ef5fbd6d792618f0fb3449f85892</span><br><span class=\"line\">    Image:          registry.navercorp.com/ncp-image/ncp-nginx:latest</span><br><span class=\"line\">    Image ID:       docker-pullable://registry.navercorp.com/ncp-image/ncp-nginx@sha256:650cfc6f4e39b5bd5ec6bc57063886ba6e8808d691ac99200ac39fac2252c6ea</span><br><span class=\"line\">    Port:           80/TCP</span><br><span class=\"line\">    Host Port:      0/TCP</span><br><span class=\"line\">    State:          Running</span><br><span class=\"line\">      Started:      Mon, 08 Jul 2019 18:59:01 +0900</span><br><span class=\"line\">    Ready:          True</span><br><span class=\"line\">    Restart Count:  0</span><br><span class=\"line\">    Environment:    &lt;none&gt;</span><br><span class=\"line\">    Mounts:</span><br><span class=\"line\">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zbdxq (ro)</span><br><span class=\"line\">Conditions:</span><br><span class=\"line\">  Type              Status</span><br><span class=\"line\">  Initialized       True </span><br><span class=\"line\">  Ready             True </span><br><span class=\"line\">  ContainersReady   True </span><br><span class=\"line\">  PodScheduled      True </span><br><span class=\"line\">Volumes:</span><br><span class=\"line\">  default-token-zbdxq:</span><br><span class=\"line\">    Type:        Secret (a volume populated by a Secret)</span><br><span class=\"line\">    SecretName:  default-token-zbdxq</span><br><span class=\"line\">    Optional:    false</span><br><span class=\"line\">QoS Class:       BestEffort</span><br><span class=\"line\">Node-Selectors:  &lt;none&gt;</span><br><span class=\"line\">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class=\"line\">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class=\"line\">Events:          &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>重点关注以下参数：</p>\n<ol>\n<li>Node：当前pod被调度至服务器节点的host name</li>\n<li>Labels：标签，只有当labels与之前<code>replica set</code>的<code>Selector</code>保持一致，才会被相应的<code>Replica Controller</code>纳入管理进行动态缩/扩容；后续的<code>service</code>也是通过标签来discover当前有效的pod；标签可以在运行时动态修改</li>\n<li>IP：pod在k8s集群内部的ip</li>\n<li>Controlled By：标明当前pod是由哪个<code>replica set</code>进行管理</li>\n<li>Containers：pod封装的容器信息，一个pod可以有多个容器</li>\n<li>Tolerations：指定该pod多长时间未达到Ready状态或者k8s多长时间未检测到pod心跳后，允许k8s重新调度pod</li>\n<li>Events：pod经历的事件，deployment的滚动升级和缩/扩容等都会产生事件</li>\n</ol>\n<p>在确认<code>replica set</code>和<code>pod</code>状态确认无误后，即可将该应用暴露为服务</p>\n<h5 id=\"暴露服务\"><a href=\"#暴露服务\" class=\"headerlink\" title=\"暴露服务\"></a>暴露服务</h5><p>使用<code>kubectl expose</code>命令将nginx deployment暴露为服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl expose deployment/nginx --type=\"NodePort\" --port 80</span><br></pre></td></tr></table></figure>\n<ol>\n<li>–type：当前暴露形式，指定<code>NodePort</code>的话，k8s会给当前服务随机分派一个30000-32767之间的端口号，外界可以直接通过<code>服务器node ip + node port</code>的方式访问这个服务</li>\n<li>–port：指定应用所需要暴露的端口，nginx服务则需要暴露他的80端口</li>\n</ol>\n<p>查看暴露的服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> kubectl get services</span><br><span class=\"line\">NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class=\"line\">kubernetes            ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          12m</span><br><span class=\"line\">nginx                 NodePort    10.109.107.109   &lt;none&gt;        80:31482/TCP   5m1s</span><br></pre></td></tr></table></figure>\n<p>查看详情：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl describe services nginx</span><br><span class=\"line\">Name:                     nginx</span><br><span class=\"line\">Namespace:                default</span><br><span class=\"line\">Labels:                   run=nginx</span><br><span class=\"line\">Annotations:              &lt;none&gt;</span><br><span class=\"line\">Selector:                 run=nginx</span><br><span class=\"line\">Type:                     NodePort</span><br><span class=\"line\">IP:                       10.109.107.109</span><br><span class=\"line\">Port:                     &lt;unset&gt;  80/TCP</span><br><span class=\"line\">TargetPort:               80/TCP</span><br><span class=\"line\">NodePort:                 &lt;unset&gt;  31482/TCP</span><br><span class=\"line\">Endpoints:                10.244.2.2:80</span><br><span class=\"line\">Session Affinity:         None</span><br><span class=\"line\">External Traffic Policy:  Cluster</span><br><span class=\"line\">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure>\n<p>默认情况下，<code>service</code>名称和<code>deployment</code>名称保持一致，其他参数：</p>\n<ul>\n<li>Selector：<code>service</code>通过<code>Selector</code>选择来匹配对应的<code>pod</code>，k8s会通过<code>Endpoints Controller</code>来定期更新健康的符合<code>Selector</code>匹配规则的<code>pod</code>路由表，在这里nginx service将寻找所有labels为run=nginx的<code>pod</code>作为路由对象</li>\n<li>IP：<code>service</code>在集群中的唯一ip地址</li>\n</ul>\n<p><code>service</code>的<code>NodePort</code>是31482，因此我们可以直接在本机使用localhost访问这个nginx服务了：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ curl localhost:31482</span><br><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class=\"line\">&lt;style&gt;</span><br><span class=\"line\">    body &#123;</span><br><span class=\"line\">        width: 35em;</span><br><span class=\"line\">        margin: 0 auto;</span><br><span class=\"line\">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>至此已经成功发布nginx service</p>\n<p><code>Service</code>和<code>Deployment</code>,<code>Replica Set</code>还有<code>Pod</code>之间的关系：</p>\n<p><img src=\"\\blog\\images\\image.png\" alt=\"upload successful\"></p>\n<p>一个<code>deployment</code>可以创建多个<code>replica set</code>和<code>pod</code>以及<code>service</code>，<code>replica set</code>和<code>service</code>通过<code>Selector</code>指定的值来匹配带有相关<code>Labels</code>的<code>pod</code></p>\n<h4 id=\"k8s内部的服务发现\"><a href=\"#k8s内部的服务发现\" class=\"headerlink\" title=\"k8s内部的服务发现\"></a>k8s内部的服务发现</h4><p>k8s内部通过何种方式发现我们发布的<code>nginx service</code> ?</p>\n<p>目前有三种方式：</p>\n<ol>\n<li><p><code>NodePort方式</code>：即上面通过<code>node ip + node port</code>将访问路径固定，这种方式不够灵活，通常只能用于外界调试</p>\n</li>\n<li><p><code>环境变量方式</code>：k8s默认会在每个 pod 启动时候会把所有服务的 IP 和 port 信息配置到当前pod的环境变量中，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。</p>\n</li>\n<li><p><code>kube-dns</code>方式：k8s官方推荐通过<code>kubeDNS + dnsmasq</code>的方式配置kube-dns插件，kube-dns可以缓存所有已经存在的<code>service</code>信息供服务调用方发现并调用服务，其他服务可以直接使用以下方式调用nginx服务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://&lt;service_name&gt;.&lt;namespace&gt;.svc.&lt;domain&gt;:80/</span><br></pre></td></tr></table></figure>\n<p><code>service_name</code>：即服务名nginx</p>\n<p><code>namespace</code>：k8s命名空间，创建deployment时不特别指定的话，<code>namespace</code>均为”default”</p>\n<p><code>domain</code>：域名后缀，默认为<code>cluster.local</code></p>\n<p>在 <code>pod</code> 中访问也可以使用缩写 <code>service_name.namespace</code>，如果 pod 和 service 在同一个 <code>namespace</code>，可以直接使用 <code>service_name</code>，因此如果同一个<code>namespace</code>有其他服务要访问nginx，则直接使用<code>nginx</code>作为域名即可：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://nginx:80/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h5 id=\"测试服务发现\"><a href=\"#测试服务发现\" class=\"headerlink\" title=\"测试服务发现\"></a>测试服务发现</h5><p>手动测试<code>service</code>的服务发现需要进入到<code>pod</code>内部，执行以下命令进入<code>pod</code>内部环境，进入到<code>pod</code>后可以通过curl命令访问<code>http://nginx:80/</code> (镜像没有安装curl，可用yum进行安装)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl get pods</span><br><span class=\"line\">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">curl-6bf6db5c4f-4mkxk    1/1     Running   0          22h</span><br><span class=\"line\">nginx-5ff9d6cc77-5nxpn   1/1     Running   0          59m</span><br><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl exec nginx-5ff9d6cc77-5nxpn -it -- /bin/bash</span><br><span class=\"line\">root@nginx-5ff9d6cc77-5nxpn:/# curl nginx:80/</span><br><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class=\"line\">&lt;style&gt;</span><br><span class=\"line\">    body &#123;</span><br><span class=\"line\">        width: 35em;</span><br><span class=\"line\">        margin: 0 auto;</span><br><span class=\"line\">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br><span class=\"line\">......</span><br></pre></td></tr></table></figure>\n<p>使用<code>Ctrl P + Ctrl Q</code>命令退出<code>pod</code>环境</p>\n<h4 id=\"删除service和deployment\"><a href=\"#删除service和deployment\" class=\"headerlink\" title=\"删除service和deployment\"></a>删除service和deployment</h4><p><code>kubectl</code>工具提供一键式删除<code>service</code>和<code>deployment</code>，当<code>deployment</code>被删除后，对应的<code>pod</code>同时被回收</p>\n<p>使用以下命令删除nginx service:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl delete services nginx</span><br><span class=\"line\">service \"nginx\" deleted</span><br></pre></td></tr></table></figure>\n<p>使用以下命令删除nginx deployment:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[irteam@dev-ncc-client-ncl ~]$ kubectl delete deployments nginx</span><br><span class=\"line\">deployment.extensions \"nginx\" deleted</span><br></pre></td></tr></table></figure>\n<p>随后再查看<code>pod</code>可以发现之前创建的<code>pod</code>均被删除</p>"},{"title":"vim快捷键（1）","author":"天渊","date":"2019-01-21T03:18:00.000Z","_content":"vim编辑器有三个模式：一般模式，编辑模式，命令模式：\n<!--more-->\n\n- 一般模式：默认模式，可以新增删除复制粘贴，按Esc从当前模式转换到普通模式\n- 编辑模式：按i,o,a等字符进入编辑模式，可以编辑文本内容\n- 命令模式：按:,/,?三个字符中的一个进入命令模式，可以读取、查找数据、大量替换字符等操作\n\n### 基本操作\nvi+文件名 进入文档，按命令键进入编辑或者命令模式，Esc回到一般模式（命令模式和编辑模式不能相互转换），输入:w保存文档，输入:wq保存并离开文档，使用:wq!在没有权限的情况下强制写入，输入:q不保存并退出\n\n#### 文本浏览\n\n- 重新载入文件：\n\n  ```shell\n  :e\n  :e! #放弃当前修改，强制重新载入\n  :bufdo e 或者 :bufdo :e! #重新载入所有打开的文件\n  ```\n\n- 光标移动\n\n  ```shell\n  0  #数字0）移动光标至本行开头\n  $  #移动光标至本行末尾\n  ^  #移动光标至本行第一个非空字符\n  w  #向前移动一个词\n  W  #向前移动一个词 （以空格分隔的词）\n  5w  #向前移动5个词\n  b  #向后移动一个词\n  B  #向后移动一个词 （以空格分隔的词）\n  5b  #向后移动5个词\n  G  #移动至文件末尾\n  gg #移动至文件开头\n  ```\n\n- 搜索和替换\n\n  ```shell\n  :/search_text  #在文档后面部分检索search_text这个内容\n  :?search_text  #在文档前面部分检索search_text这个内容\n  n  #移动到后一个检索结果\n  N  #移动到前一个检索结果\n  :%s/original/replacement  #将第一个检索到的original替换为replacement\n  :%s/original/replacement/g\t#将所有original替换为replacement\n  :%s/original/replacement/gc\t #将所有original替换为replacement，但会先询问\n  ```","source":"_posts/Vim编辑器.md","raw":"title: vim快捷键（1）\ntags:\n  - vim\ncategories:\n  - 工具使用\nauthor: 天渊\ndate: 2019-01-21 11:18:00\n---\nvim编辑器有三个模式：一般模式，编辑模式，命令模式：\n<!--more-->\n\n- 一般模式：默认模式，可以新增删除复制粘贴，按Esc从当前模式转换到普通模式\n- 编辑模式：按i,o,a等字符进入编辑模式，可以编辑文本内容\n- 命令模式：按:,/,?三个字符中的一个进入命令模式，可以读取、查找数据、大量替换字符等操作\n\n### 基本操作\nvi+文件名 进入文档，按命令键进入编辑或者命令模式，Esc回到一般模式（命令模式和编辑模式不能相互转换），输入:w保存文档，输入:wq保存并离开文档，使用:wq!在没有权限的情况下强制写入，输入:q不保存并退出\n\n#### 文本浏览\n\n- 重新载入文件：\n\n  ```shell\n  :e\n  :e! #放弃当前修改，强制重新载入\n  :bufdo e 或者 :bufdo :e! #重新载入所有打开的文件\n  ```\n\n- 光标移动\n\n  ```shell\n  0  #数字0）移动光标至本行开头\n  $  #移动光标至本行末尾\n  ^  #移动光标至本行第一个非空字符\n  w  #向前移动一个词\n  W  #向前移动一个词 （以空格分隔的词）\n  5w  #向前移动5个词\n  b  #向后移动一个词\n  B  #向后移动一个词 （以空格分隔的词）\n  5b  #向后移动5个词\n  G  #移动至文件末尾\n  gg #移动至文件开头\n  ```\n\n- 搜索和替换\n\n  ```shell\n  :/search_text  #在文档后面部分检索search_text这个内容\n  :?search_text  #在文档前面部分检索search_text这个内容\n  n  #移动到后一个检索结果\n  N  #移动到前一个检索结果\n  :%s/original/replacement  #将第一个检索到的original替换为replacement\n  :%s/original/replacement/g\t#将所有original替换为replacement\n  :%s/original/replacement/gc\t #将所有original替换为replacement，但会先询问\n  ```","slug":"Vim编辑器","published":1,"updated":"2019-03-19T13:30:58.430Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js380003g0qr7xe4fiss","content":"<p>vim编辑器有三个模式：一般模式，编辑模式，命令模式：<br><a id=\"more\"></a></p>\n<ul>\n<li>一般模式：默认模式，可以新增删除复制粘贴，按Esc从当前模式转换到普通模式</li>\n<li>编辑模式：按i,o,a等字符进入编辑模式，可以编辑文本内容</li>\n<li>命令模式：按:,/,?三个字符中的一个进入命令模式，可以读取、查找数据、大量替换字符等操作</li>\n</ul>\n<h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><p>vi+文件名 进入文档，按命令键进入编辑或者命令模式，Esc回到一般模式（命令模式和编辑模式不能相互转换），输入:w保存文档，输入:wq保存并离开文档，使用:wq!在没有权限的情况下强制写入，输入:q不保存并退出</p>\n<h4 id=\"文本浏览\"><a href=\"#文本浏览\" class=\"headerlink\" title=\"文本浏览\"></a>文本浏览</h4><ul>\n<li><p>重新载入文件：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:e</span><br><span class=\"line\">:e! #放弃当前修改，强制重新载入</span><br><span class=\"line\">:bufdo e 或者 :bufdo :e! #重新载入所有打开的文件</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>光标移动</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">0  #数字0）移动光标至本行开头</span><br><span class=\"line\"><span class=\"meta\">$</span>  #移动光标至本行末尾</span><br><span class=\"line\">^  #移动光标至本行第一个非空字符</span><br><span class=\"line\">w  #向前移动一个词</span><br><span class=\"line\">W  #向前移动一个词 （以空格分隔的词）</span><br><span class=\"line\">5w  #向前移动5个词</span><br><span class=\"line\">b  #向后移动一个词</span><br><span class=\"line\">B  #向后移动一个词 （以空格分隔的词）</span><br><span class=\"line\">5b  #向后移动5个词</span><br><span class=\"line\">G  #移动至文件末尾</span><br><span class=\"line\">gg #移动至文件开头</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>搜索和替换</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:/search_text  #在文档后面部分检索search_text这个内容</span><br><span class=\"line\">:?search_text  #在文档前面部分检索search_text这个内容</span><br><span class=\"line\">n  #移动到后一个检索结果</span><br><span class=\"line\">N  #移动到前一个检索结果</span><br><span class=\"line\">:%s/original/replacement  #将第一个检索到的original替换为replacement</span><br><span class=\"line\">:%s/original/replacement/g\t#将所有original替换为replacement</span><br><span class=\"line\">:%s/original/replacement/gc\t #将所有original替换为replacement，但会先询问</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>vim编辑器有三个模式：一般模式，编辑模式，命令模式：<br>","more":"</p>\n<ul>\n<li>一般模式：默认模式，可以新增删除复制粘贴，按Esc从当前模式转换到普通模式</li>\n<li>编辑模式：按i,o,a等字符进入编辑模式，可以编辑文本内容</li>\n<li>命令模式：按:,/,?三个字符中的一个进入命令模式，可以读取、查找数据、大量替换字符等操作</li>\n</ul>\n<h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><p>vi+文件名 进入文档，按命令键进入编辑或者命令模式，Esc回到一般模式（命令模式和编辑模式不能相互转换），输入:w保存文档，输入:wq保存并离开文档，使用:wq!在没有权限的情况下强制写入，输入:q不保存并退出</p>\n<h4 id=\"文本浏览\"><a href=\"#文本浏览\" class=\"headerlink\" title=\"文本浏览\"></a>文本浏览</h4><ul>\n<li><p>重新载入文件：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:e</span><br><span class=\"line\">:e! #放弃当前修改，强制重新载入</span><br><span class=\"line\">:bufdo e 或者 :bufdo :e! #重新载入所有打开的文件</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>光标移动</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">0  #数字0）移动光标至本行开头</span><br><span class=\"line\"><span class=\"meta\">$</span>  #移动光标至本行末尾</span><br><span class=\"line\">^  #移动光标至本行第一个非空字符</span><br><span class=\"line\">w  #向前移动一个词</span><br><span class=\"line\">W  #向前移动一个词 （以空格分隔的词）</span><br><span class=\"line\">5w  #向前移动5个词</span><br><span class=\"line\">b  #向后移动一个词</span><br><span class=\"line\">B  #向后移动一个词 （以空格分隔的词）</span><br><span class=\"line\">5b  #向后移动5个词</span><br><span class=\"line\">G  #移动至文件末尾</span><br><span class=\"line\">gg #移动至文件开头</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>搜索和替换</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:/search_text  #在文档后面部分检索search_text这个内容</span><br><span class=\"line\">:?search_text  #在文档前面部分检索search_text这个内容</span><br><span class=\"line\">n  #移动到后一个检索结果</span><br><span class=\"line\">N  #移动到前一个检索结果</span><br><span class=\"line\">:%s/original/replacement  #将第一个检索到的original替换为replacement</span><br><span class=\"line\">:%s/original/replacement/g\t#将所有original替换为replacement</span><br><span class=\"line\">:%s/original/replacement/gc\t #将所有original替换为replacement，但会先询问</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"TCP连接 - Nagle 和 Cork","author":"天渊","date":"2019-02-13T07:02:00.000Z","_content":"TCP连接涉及到的参数设置及调优策略纷繁多样，其中跟到数据包发送策略有关的有`Nagle算法`和`Cork算法`，这两种算法的都涉及到化TCP通讯过程中的小数据包传输优化，初学时感觉很类似，不容易区分，需要一探究竟\n<!-- more -->\n\n### Nagle有什么用\n\nTCP/IP网络传输的发送端，在某些场景下可能会在短时间发送大量小数据包，导致网络拥塞（例如`糊涂窗口综合症`），`Nagle算法`初衷就是想解决这样的问题。TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。如果在数据包中，除开协议头后实际的数据尺寸太小，此类小数据包大量堆积会造成极大的网络拥塞，因此`Nagle`算法决定降低此类小包的发送频率，希望每次都能够以`MSS尺寸`的数据块来发送数据，或者在当前窗口还存在未Ack的数据包时，延迟发送后续的数据包，避免网络中充斥着许多小数据块。\n\n`Nagle算法`的逻辑流程：\n\n![](/blog/images/nagle.jpg)\n\n### Nagle的劣势\n\n`Nagle算法`并没有阻止小包发送，只是阻止了短时间内大量小包的发送，而且在某种程度上降低了数据实时性\n\n`Nagle算法`与TCP接收端的`延迟ACK`策略在某些情况下会造成冲突，极大的降低数据实时性：\n\n> tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送\n\n如果`Nagle算法`和`延迟ACK`同时在发送端和接收端存在，则会造成以下现象：\n\n1. 写-写-读的场景，发送端首先发送了小数据包A\n2. 接收端接收到数据包A，延迟本次ACK待下一次发送数据时再一并将ACK发送回去\n3. 发送端未收到数据包A的ACK，因此进行等待，暂不发送后续的数据包B\n4. 双方僵持，直到发送端或接收端等待超时\n\n如果对数据实时性要求高而且网络资源充足的情况下可以将其关闭，例如在Netty中可以对Channel设置`TCP_NODELAY`属性来关闭Nagle功能：\n\n```java\nbootstrap.childOption(ChannelOption.TCP_NODELAY, true)\n```\n\n\n\n### Cork算法有什么用\n\n`Nagle算法`并没有完全解决小数据包问题，仅仅是解决了发送大量小包带来的网络拥塞问题，但`Cork算法`的出现就能很好地降低小数据包带来的影响\n\n`Cork算法`和`Nagle算法`非常类似，但是它们的着眼点不一样，CORK算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小，方法是如果当前数据包小于MSS大小，则在缓冲区等待，待后续数据包到来时合并未同一个数据包，将小包合并为大包共享一个协议头，这样就达到了消灭小包的目的\n\n通过设置`TCP_CORK`来开启`Cork算法`，并如果开启了`Cork算法`的话，`Nagle`算法也是默认开启的","source":"_posts/TCP连接-Nagle-和-Cork-1.md","raw":"title: TCP连接 - Nagle 和 Cork\nauthor: 天渊\ntags:\n  - TCP\n  - 计算机网络\ncategories:\n  - 基础知识\ndate: 2019-02-13 15:02:00\n---\nTCP连接涉及到的参数设置及调优策略纷繁多样，其中跟到数据包发送策略有关的有`Nagle算法`和`Cork算法`，这两种算法的都涉及到化TCP通讯过程中的小数据包传输优化，初学时感觉很类似，不容易区分，需要一探究竟\n<!-- more -->\n\n### Nagle有什么用\n\nTCP/IP网络传输的发送端，在某些场景下可能会在短时间发送大量小数据包，导致网络拥塞（例如`糊涂窗口综合症`），`Nagle算法`初衷就是想解决这样的问题。TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。如果在数据包中，除开协议头后实际的数据尺寸太小，此类小数据包大量堆积会造成极大的网络拥塞，因此`Nagle`算法决定降低此类小包的发送频率，希望每次都能够以`MSS尺寸`的数据块来发送数据，或者在当前窗口还存在未Ack的数据包时，延迟发送后续的数据包，避免网络中充斥着许多小数据块。\n\n`Nagle算法`的逻辑流程：\n\n![](/blog/images/nagle.jpg)\n\n### Nagle的劣势\n\n`Nagle算法`并没有阻止小包发送，只是阻止了短时间内大量小包的发送，而且在某种程度上降低了数据实时性\n\n`Nagle算法`与TCP接收端的`延迟ACK`策略在某些情况下会造成冲突，极大的降低数据实时性：\n\n> tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送\n\n如果`Nagle算法`和`延迟ACK`同时在发送端和接收端存在，则会造成以下现象：\n\n1. 写-写-读的场景，发送端首先发送了小数据包A\n2. 接收端接收到数据包A，延迟本次ACK待下一次发送数据时再一并将ACK发送回去\n3. 发送端未收到数据包A的ACK，因此进行等待，暂不发送后续的数据包B\n4. 双方僵持，直到发送端或接收端等待超时\n\n如果对数据实时性要求高而且网络资源充足的情况下可以将其关闭，例如在Netty中可以对Channel设置`TCP_NODELAY`属性来关闭Nagle功能：\n\n```java\nbootstrap.childOption(ChannelOption.TCP_NODELAY, true)\n```\n\n\n\n### Cork算法有什么用\n\n`Nagle算法`并没有完全解决小数据包问题，仅仅是解决了发送大量小包带来的网络拥塞问题，但`Cork算法`的出现就能很好地降低小数据包带来的影响\n\n`Cork算法`和`Nagle算法`非常类似，但是它们的着眼点不一样，CORK算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小，方法是如果当前数据包小于MSS大小，则在缓冲区等待，待后续数据包到来时合并未同一个数据包，将小包合并为大包共享一个协议头，这样就达到了消灭小包的目的\n\n通过设置`TCP_CORK`来开启`Cork算法`，并如果开启了`Cork算法`的话，`Nagle`算法也是默认开启的","slug":"TCP连接-Nagle-和-Cork-1","published":1,"updated":"2019-03-19T13:30:58.426Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3e0005g0qr8k8bjic6","content":"<p>TCP连接涉及到的参数设置及调优策略纷繁多样，其中跟到数据包发送策略有关的有<code>Nagle算法</code>和<code>Cork算法</code>，这两种算法的都涉及到化TCP通讯过程中的小数据包传输优化，初学时感觉很类似，不容易区分，需要一探究竟<br><a id=\"more\"></a></p>\n<h3 id=\"Nagle有什么用\"><a href=\"#Nagle有什么用\" class=\"headerlink\" title=\"Nagle有什么用\"></a>Nagle有什么用</h3><p>TCP/IP网络传输的发送端，在某些场景下可能会在短时间发送大量小数据包，导致网络拥塞（例如<code>糊涂窗口综合症</code>），<code>Nagle算法</code>初衷就是想解决这样的问题。TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。如果在数据包中，除开协议头后实际的数据尺寸太小，此类小数据包大量堆积会造成极大的网络拥塞，因此<code>Nagle</code>算法决定降低此类小包的发送频率，希望每次都能够以<code>MSS尺寸</code>的数据块来发送数据，或者在当前窗口还存在未Ack的数据包时，延迟发送后续的数据包，避免网络中充斥着许多小数据块。</p>\n<p><code>Nagle算法</code>的逻辑流程：</p>\n<p><img src=\"/blog/images/nagle.jpg\" alt></p>\n<h3 id=\"Nagle的劣势\"><a href=\"#Nagle的劣势\" class=\"headerlink\" title=\"Nagle的劣势\"></a>Nagle的劣势</h3><p><code>Nagle算法</code>并没有阻止小包发送，只是阻止了短时间内大量小包的发送，而且在某种程度上降低了数据实时性</p>\n<p><code>Nagle算法</code>与TCP接收端的<code>延迟ACK</code>策略在某些情况下会造成冲突，极大的降低数据实时性：</p>\n<blockquote>\n<p>tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送</p>\n</blockquote>\n<p>如果<code>Nagle算法</code>和<code>延迟ACK</code>同时在发送端和接收端存在，则会造成以下现象：</p>\n<ol>\n<li>写-写-读的场景，发送端首先发送了小数据包A</li>\n<li>接收端接收到数据包A，延迟本次ACK待下一次发送数据时再一并将ACK发送回去</li>\n<li>发送端未收到数据包A的ACK，因此进行等待，暂不发送后续的数据包B</li>\n<li>双方僵持，直到发送端或接收端等待超时</li>\n</ol>\n<p>如果对数据实时性要求高而且网络资源充足的情况下可以将其关闭，例如在Netty中可以对Channel设置<code>TCP_NODELAY</code>属性来关闭Nagle功能：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bootstrap.childOption(ChannelOption.TCP_NODELAY, <span class=\"keyword\">true</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Cork算法有什么用\"><a href=\"#Cork算法有什么用\" class=\"headerlink\" title=\"Cork算法有什么用\"></a>Cork算法有什么用</h3><p><code>Nagle算法</code>并没有完全解决小数据包问题，仅仅是解决了发送大量小包带来的网络拥塞问题，但<code>Cork算法</code>的出现就能很好地降低小数据包带来的影响</p>\n<p><code>Cork算法</code>和<code>Nagle算法</code>非常类似，但是它们的着眼点不一样，CORK算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小，方法是如果当前数据包小于MSS大小，则在缓冲区等待，待后续数据包到来时合并未同一个数据包，将小包合并为大包共享一个协议头，这样就达到了消灭小包的目的</p>\n<p>通过设置<code>TCP_CORK</code>来开启<code>Cork算法</code>，并如果开启了<code>Cork算法</code>的话，<code>Nagle</code>算法也是默认开启的</p>\n","site":{"data":{}},"excerpt":"<p>TCP连接涉及到的参数设置及调优策略纷繁多样，其中跟到数据包发送策略有关的有<code>Nagle算法</code>和<code>Cork算法</code>，这两种算法的都涉及到化TCP通讯过程中的小数据包传输优化，初学时感觉很类似，不容易区分，需要一探究竟<br>","more":"</p>\n<h3 id=\"Nagle有什么用\"><a href=\"#Nagle有什么用\" class=\"headerlink\" title=\"Nagle有什么用\"></a>Nagle有什么用</h3><p>TCP/IP网络传输的发送端，在某些场景下可能会在短时间发送大量小数据包，导致网络拥塞（例如<code>糊涂窗口综合症</code>），<code>Nagle算法</code>初衷就是想解决这样的问题。TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。如果在数据包中，除开协议头后实际的数据尺寸太小，此类小数据包大量堆积会造成极大的网络拥塞，因此<code>Nagle</code>算法决定降低此类小包的发送频率，希望每次都能够以<code>MSS尺寸</code>的数据块来发送数据，或者在当前窗口还存在未Ack的数据包时，延迟发送后续的数据包，避免网络中充斥着许多小数据块。</p>\n<p><code>Nagle算法</code>的逻辑流程：</p>\n<p><img src=\"/blog/images/nagle.jpg\" alt></p>\n<h3 id=\"Nagle的劣势\"><a href=\"#Nagle的劣势\" class=\"headerlink\" title=\"Nagle的劣势\"></a>Nagle的劣势</h3><p><code>Nagle算法</code>并没有阻止小包发送，只是阻止了短时间内大量小包的发送，而且在某种程度上降低了数据实时性</p>\n<p><code>Nagle算法</code>与TCP接收端的<code>延迟ACK</code>策略在某些情况下会造成冲突，极大的降低数据实时性：</p>\n<blockquote>\n<p>tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送</p>\n</blockquote>\n<p>如果<code>Nagle算法</code>和<code>延迟ACK</code>同时在发送端和接收端存在，则会造成以下现象：</p>\n<ol>\n<li>写-写-读的场景，发送端首先发送了小数据包A</li>\n<li>接收端接收到数据包A，延迟本次ACK待下一次发送数据时再一并将ACK发送回去</li>\n<li>发送端未收到数据包A的ACK，因此进行等待，暂不发送后续的数据包B</li>\n<li>双方僵持，直到发送端或接收端等待超时</li>\n</ol>\n<p>如果对数据实时性要求高而且网络资源充足的情况下可以将其关闭，例如在Netty中可以对Channel设置<code>TCP_NODELAY</code>属性来关闭Nagle功能：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bootstrap.childOption(ChannelOption.TCP_NODELAY, <span class=\"keyword\">true</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Cork算法有什么用\"><a href=\"#Cork算法有什么用\" class=\"headerlink\" title=\"Cork算法有什么用\"></a>Cork算法有什么用</h3><p><code>Nagle算法</code>并没有完全解决小数据包问题，仅仅是解决了发送大量小包带来的网络拥塞问题，但<code>Cork算法</code>的出现就能很好地降低小数据包带来的影响</p>\n<p><code>Cork算法</code>和<code>Nagle算法</code>非常类似，但是它们的着眼点不一样，CORK算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小，方法是如果当前数据包小于MSS大小，则在缓冲区等待，待后续数据包到来时合并未同一个数据包，将小包合并为大包共享一个协议头，这样就达到了消灭小包的目的</p>\n<p>通过设置<code>TCP_CORK</code>来开启<code>Cork算法</code>，并如果开启了<code>Cork算法</code>的话，<code>Nagle</code>算法也是默认开启的</p>"},{"title":"Nginx -- root和alias路径映射","author":"天渊","date":"2019-03-18T04:40:00.000Z","_content":"在nginx中，`root`和`alias`命令都用于将http请求和服务器上的资源地址进行映射，不过使用方式不太相同\n<!--more-->\n### 使用范围\n两个命令在nginx.conf中的作用范围如下：\n- root: http、server、location和if中均可配置，用于指定当前范围的根路径\n- alias: 尽在location中有效\n\n### 使用方法\nroot与alias主要区别在于nginx如何解释location后面的uri，这会使两者分别以不同的方式将请求映射到服务器文件上，两者使用方法均为 `root/alisa path`\n#### root映射方式：\n\nroot路径 + location路径\n如下配置中，nginx将会把`/blog/***`这样的请求路径映射到` /root/deploy/static-file/blog`目录下的资源中，因此`/blog/index.html`请求得到的资源即为`/root/deploy/static-file/blog/index.html`\n\n```lua\nlocation /blog {\n    root /root/deploy/static-file;\n}\n```\n\n#### alias映射方式：\n\n  alias路径直接替换原请求路径\n  对于alias，如下配置，`/blog/index.html`请求得到的资源仍然为`/root/deploy/static-file/blog/index.html`，\n\n  ```lua\nlocation /blog {\n    alias /root/deploy/static-file/blog/;\n}\n  ```\n  因此root和alias主要区别在于，当映射文件路径的时候，前者用于指定根路径，将原请求在根路径的基础上组合成新的路径，而后者用于指定url别名，将该别名作为新路径替换掉原请求路径\n\n**注意**：alias后面的路径必须加`/`正斜杠\n\n### proxy_pass\n相应的，`proxy_pass`作为反响代理时，对`/`正斜杠也有一定的讲究\n##### path加正斜杠\n`proxy_pass`的path加正斜杠时，用法与`alias`一样，都是用新路径替换掉原路径：\n\n```lua\nlocation /tomcat {\n    proxy_pass http://localhost:8080/;\n}\n```\n\n如上配置，nginx监听80端口，当请求`localhost/tomcat`时，请求转发到`http://localhost:8080/`\n\n##### path不加正斜杠\n\n```lua\nlocation /tomcat {\n    proxy_pass http://localhost:8080;\n}\n```\n\n如果去掉正斜杠，当请求`localhost/tomcat`时，请求转发到`http://localhost:8080/tomcat`\n\n这个地方容易踩坑，需要注意","source":"_posts/Nginx-root和alias路径映射.md","raw":"title: Nginx -- root和alias路径映射\nauthor: 天渊\ntags:\n  - Nginx\ncategories:\n  - 基础知识\ndate: 2019-03-18 12:40:00\n---\n在nginx中，`root`和`alias`命令都用于将http请求和服务器上的资源地址进行映射，不过使用方式不太相同\n<!--more-->\n### 使用范围\n两个命令在nginx.conf中的作用范围如下：\n- root: http、server、location和if中均可配置，用于指定当前范围的根路径\n- alias: 尽在location中有效\n\n### 使用方法\nroot与alias主要区别在于nginx如何解释location后面的uri，这会使两者分别以不同的方式将请求映射到服务器文件上，两者使用方法均为 `root/alisa path`\n#### root映射方式：\n\nroot路径 + location路径\n如下配置中，nginx将会把`/blog/***`这样的请求路径映射到` /root/deploy/static-file/blog`目录下的资源中，因此`/blog/index.html`请求得到的资源即为`/root/deploy/static-file/blog/index.html`\n\n```lua\nlocation /blog {\n    root /root/deploy/static-file;\n}\n```\n\n#### alias映射方式：\n\n  alias路径直接替换原请求路径\n  对于alias，如下配置，`/blog/index.html`请求得到的资源仍然为`/root/deploy/static-file/blog/index.html`，\n\n  ```lua\nlocation /blog {\n    alias /root/deploy/static-file/blog/;\n}\n  ```\n  因此root和alias主要区别在于，当映射文件路径的时候，前者用于指定根路径，将原请求在根路径的基础上组合成新的路径，而后者用于指定url别名，将该别名作为新路径替换掉原请求路径\n\n**注意**：alias后面的路径必须加`/`正斜杠\n\n### proxy_pass\n相应的，`proxy_pass`作为反响代理时，对`/`正斜杠也有一定的讲究\n##### path加正斜杠\n`proxy_pass`的path加正斜杠时，用法与`alias`一样，都是用新路径替换掉原路径：\n\n```lua\nlocation /tomcat {\n    proxy_pass http://localhost:8080/;\n}\n```\n\n如上配置，nginx监听80端口，当请求`localhost/tomcat`时，请求转发到`http://localhost:8080/`\n\n##### path不加正斜杠\n\n```lua\nlocation /tomcat {\n    proxy_pass http://localhost:8080;\n}\n```\n\n如果去掉正斜杠，当请求`localhost/tomcat`时，请求转发到`http://localhost:8080/tomcat`\n\n这个地方容易踩坑，需要注意","slug":"Nginx-root和alias路径映射","published":1,"updated":"2019-03-19T13:30:58.413Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3g0007g0qr0upjhy26","content":"<p>在nginx中，<code>root</code>和<code>alias</code>命令都用于将http请求和服务器上的资源地址进行映射，不过使用方式不太相同<br><a id=\"more\"></a></p>\n<h3 id=\"使用范围\"><a href=\"#使用范围\" class=\"headerlink\" title=\"使用范围\"></a>使用范围</h3><p>两个命令在nginx.conf中的作用范围如下：</p>\n<ul>\n<li>root: http、server、location和if中均可配置，用于指定当前范围的根路径</li>\n<li>alias: 尽在location中有效</li>\n</ul>\n<h3 id=\"使用方法\"><a href=\"#使用方法\" class=\"headerlink\" title=\"使用方法\"></a>使用方法</h3><p>root与alias主要区别在于nginx如何解释location后面的uri，这会使两者分别以不同的方式将请求映射到服务器文件上，两者使用方法均为 <code>root/alisa path</code></p>\n<h4 id=\"root映射方式：\"><a href=\"#root映射方式：\" class=\"headerlink\" title=\"root映射方式：\"></a>root映射方式：</h4><p>root路径 + location路径<br>如下配置中，nginx将会把<code>/blog/***</code>这样的请求路径映射到<code>/root/deploy/static-file/blog</code>目录下的资源中，因此<code>/blog/index.html</code>请求得到的资源即为<code>/root/deploy/static-file/blog/index.html</code></p>\n<figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /blog &#123;</span><br><span class=\"line\">    root /root/deploy/static-file;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"alias映射方式：\"><a href=\"#alias映射方式：\" class=\"headerlink\" title=\"alias映射方式：\"></a>alias映射方式：</h4><p>  alias路径直接替换原请求路径<br>  对于alias，如下配置，<code>/blog/index.html</code>请求得到的资源仍然为<code>/root/deploy/static-file/blog/index.html</code>，</p>\n  <figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /blog &#123;</span><br><span class=\"line\">    alias /root/deploy/static-file/blog/;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>  因此root和alias主要区别在于，当映射文件路径的时候，前者用于指定根路径，将原请求在根路径的基础上组合成新的路径，而后者用于指定url别名，将该别名作为新路径替换掉原请求路径</p>\n<p><strong>注意</strong>：alias后面的路径必须加<code>/</code>正斜杠</p>\n<h3 id=\"proxy-pass\"><a href=\"#proxy-pass\" class=\"headerlink\" title=\"proxy_pass\"></a>proxy_pass</h3><p>相应的，<code>proxy_pass</code>作为反响代理时，对<code>/</code>正斜杠也有一定的讲究</p>\n<h5 id=\"path加正斜杠\"><a href=\"#path加正斜杠\" class=\"headerlink\" title=\"path加正斜杠\"></a>path加正斜杠</h5><p><code>proxy_pass</code>的path加正斜杠时，用法与<code>alias</code>一样，都是用新路径替换掉原路径：</p>\n<figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /tomcat &#123;</span><br><span class=\"line\">    proxy_pass http://localhost:<span class=\"number\">8080</span>/;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如上配置，nginx监听80端口，当请求<code>localhost/tomcat</code>时，请求转发到<code>http://localhost:8080/</code></p>\n<h5 id=\"path不加正斜杠\"><a href=\"#path不加正斜杠\" class=\"headerlink\" title=\"path不加正斜杠\"></a>path不加正斜杠</h5><figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /tomcat &#123;</span><br><span class=\"line\">    proxy_pass http://localhost:<span class=\"number\">8080</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果去掉正斜杠，当请求<code>localhost/tomcat</code>时，请求转发到<code>http://localhost:8080/tomcat</code></p>\n<p>这个地方容易踩坑，需要注意</p>\n","site":{"data":{}},"excerpt":"<p>在nginx中，<code>root</code>和<code>alias</code>命令都用于将http请求和服务器上的资源地址进行映射，不过使用方式不太相同<br>","more":"</p>\n<h3 id=\"使用范围\"><a href=\"#使用范围\" class=\"headerlink\" title=\"使用范围\"></a>使用范围</h3><p>两个命令在nginx.conf中的作用范围如下：</p>\n<ul>\n<li>root: http、server、location和if中均可配置，用于指定当前范围的根路径</li>\n<li>alias: 尽在location中有效</li>\n</ul>\n<h3 id=\"使用方法\"><a href=\"#使用方法\" class=\"headerlink\" title=\"使用方法\"></a>使用方法</h3><p>root与alias主要区别在于nginx如何解释location后面的uri，这会使两者分别以不同的方式将请求映射到服务器文件上，两者使用方法均为 <code>root/alisa path</code></p>\n<h4 id=\"root映射方式：\"><a href=\"#root映射方式：\" class=\"headerlink\" title=\"root映射方式：\"></a>root映射方式：</h4><p>root路径 + location路径<br>如下配置中，nginx将会把<code>/blog/***</code>这样的请求路径映射到<code>/root/deploy/static-file/blog</code>目录下的资源中，因此<code>/blog/index.html</code>请求得到的资源即为<code>/root/deploy/static-file/blog/index.html</code></p>\n<figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /blog &#123;</span><br><span class=\"line\">    root /root/deploy/static-file;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"alias映射方式：\"><a href=\"#alias映射方式：\" class=\"headerlink\" title=\"alias映射方式：\"></a>alias映射方式：</h4><p>  alias路径直接替换原请求路径<br>  对于alias，如下配置，<code>/blog/index.html</code>请求得到的资源仍然为<code>/root/deploy/static-file/blog/index.html</code>，</p>\n  <figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /blog &#123;</span><br><span class=\"line\">    alias /root/deploy/static-file/blog/;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>  因此root和alias主要区别在于，当映射文件路径的时候，前者用于指定根路径，将原请求在根路径的基础上组合成新的路径，而后者用于指定url别名，将该别名作为新路径替换掉原请求路径</p>\n<p><strong>注意</strong>：alias后面的路径必须加<code>/</code>正斜杠</p>\n<h3 id=\"proxy-pass\"><a href=\"#proxy-pass\" class=\"headerlink\" title=\"proxy_pass\"></a>proxy_pass</h3><p>相应的，<code>proxy_pass</code>作为反响代理时，对<code>/</code>正斜杠也有一定的讲究</p>\n<h5 id=\"path加正斜杠\"><a href=\"#path加正斜杠\" class=\"headerlink\" title=\"path加正斜杠\"></a>path加正斜杠</h5><p><code>proxy_pass</code>的path加正斜杠时，用法与<code>alias</code>一样，都是用新路径替换掉原路径：</p>\n<figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /tomcat &#123;</span><br><span class=\"line\">    proxy_pass http://localhost:<span class=\"number\">8080</span>/;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如上配置，nginx监听80端口，当请求<code>localhost/tomcat</code>时，请求转发到<code>http://localhost:8080/</code></p>\n<h5 id=\"path不加正斜杠\"><a href=\"#path不加正斜杠\" class=\"headerlink\" title=\"path不加正斜杠\"></a>path不加正斜杠</h5><figure class=\"highlight lua\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location /tomcat &#123;</span><br><span class=\"line\">    proxy_pass http://localhost:<span class=\"number\">8080</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果去掉正斜杠，当请求<code>localhost/tomcat</code>时，请求转发到<code>http://localhost:8080/tomcat</code></p>\n<p>这个地方容易踩坑，需要注意</p>"},{"title":"Yarn初探","author":"天渊","date":"2019-05-05T15:15:00.000Z","_content":"Yarn (Yet Another Resource Manager) 是Hadoop 2.0引入的集群计算资源管理系统，最初是为了改善MapReduce任务调度过程，同时也可以支持其他多种分布式计算模式，Yarn不与任何一种计算框架耦合，只参与集群计算资源（CPU，内存等）的分配以及计算任务的调度\n\n探究MapReduce任务从调用`submit()`提交任务到Yarn运行任务的整个过程是件很有意思的事\n\n<!--more-->\n\n### 初始化任务\n\n初始化任务包括以下四个阶段：\n\n1. `申请任务`：主要是向Yarn申请`jobId`\n2. `保存job执行文件`：保存job配置信息，分片信息和Jar包等文件\n3. `加入任务队列`：向`ResourceManager`提交任务，加入任务队列\n\n#### 申请任务\n\nMapReduce的Client在调用`job.submit()`后，交由`JobSubmitter`进行任务提交，调用`submitJobInternal`方法首先申请一个`jobId`:\n\n```java\n//... 略\nJobID jobId = submitClient.getNewJobID();\njob.setJobID(jobId);\n//... 并行度切分，保存Job执行文件，提交任务等\n```\n\n其中`submitClient`是mapreduce的RPC client，有两种实现\n\n- `LocalJobRunner`： 用于提交本地运行的任务，本地环境测试就是使用的这个client\n- `YARNRunner`：用于向Yarn集群提交任务\n\n如果配置Job时设置配置项`mapreduce.framework.name`为`yarn`，mapreduce将采用`YARNRunner`作为client进行任务提交工作，`YARNRunner`为当前Job分配一个`jobId`作为本次任务的唯一ID\n\n#### 保存Job执行文件\n\nJob执行文件包括job.splits, job.xml和job的Jar包等文件，mapreduce向文件系统（本地文件系统或者hdfs）申请一块区域用于存放执行文件：\n\n```java\n//... \nPath submitJobDir = new Path(jobStagingArea, jobId.toString());\n//...\nPath submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\n```\n\n`submitJobDir`是用于保存Job执行文件的目录，`submitJobFile`即为当前Job文件的目录，格式为`.../staging/jobId`\n\n#### 加入任务队列\n\n使用`submitClient`向Yarn集群（在这里为`ResourceManager`节点）发起RPC请求提交任务：\n\n```java\nstatus = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\n```\n\n`ResourceManager`会把当前Job加入到任务执行队列中待有可执行任务的资源可用后启动该任务\n\n### 运行Job\n\n集群中能够运行Job的资源是有限的，队列中待执行的Job想要运行需要满足一定的条件，目前Yarn提供了三种任务调度策略：`FIFO调度`，`容量调度`，`公平调度`，日后再分析\n\n#### 启动Container\n\n`ResourceManager`会定期接收各个`NodeManager`发来的节点资源使用信息，某个Job满足运行条件后首先需要申请一个可以运行任务的`NodeManager`，在之上启动一个“容器”：`Container`\n\n> 如何理解Yarn的Container？\n>\n> 可以理解为Yarn为Job运行而启动的一个运行环境，这个运行环境包含运行资源（程序运行所需要的数据，内存占用，还有Vcores虚拟核数，CPU占用的一个虚拟量化指标）\n\n如果Job配置了本地限制（即任务所需的优先需要加载本地HDFS资源，或者同一机架的HDFS副本），`ResourceManager`申请容器运行的节点时会优先申请存储有所需副本的节点，如果实在找不到再基于hadoop网络拓扑模型寻找当前机架的其他节点或者其他机架的节点，使得Job运行时所需要的数据尽量为本地数据，降低对集群带宽的依赖\n\n#### 启动MrAppMaster\n\n启动`Container`后，client会申请在这个“容器”中启动`MrAppMaster`，这个`MrAppMaster`读取Job执行文件，获取Job的配置文件和splits等信息，然后根据这些配置文件进行接下来的任务（直接运行任务，或者申请更多的`Container`并行启动任务）\n\n根据splits规划，如果需要申请更多节点运行并行任务，`MrAppMaster`会向`ResourceManager`申请启动更多的`Container`，然后在这些`Container`中启动mapTask（或者reduceTask），这些task进程在Yarn环境中统一称为`YarnChild`\n\n#### 启动Task\n\n`MrAppMaster`启动完成后，根据splits启动多个mapTask，待mapTask均完成后，再根据该job配置的reduce数目启动多个reduceTask，启动流程与mapTask完全一样，Yarn并不关心具体执行的什么任务，它只需要接收`MrAppMaster`的资源分配请求然后申请启动相应数量的`Container`即可，启动完成后任务内部的交互也不由Yarn负责，当Job完成后再向client返回任务执行结果\n\n### Yarn的特点\n\nYarn作为通用性很强的分布式计算资源调度框架，能够很好地和多种计算框架如MapReduce, Spark, Storm等进行集成，计算框架专注于计算逻辑的实现，Yarn则专注于集群资源的分配和调度\n\n对于除了MapReduce以外的其他计算框架，把上述的`MrAppMaster`替换为任何一种Master进程，把mapTask或者reduceTask替换为任何一个work进程，对于Yarn来说都没有问题，只要实现了Yarn的规范和api，都可以在Yarn上面运行\n\n","source":"_posts/Yarn初探.md","raw":"title: Yarn初探\nauthor: 天渊\ntags:\n  - Hadoop\n  - Yarn\ncategories:\n  - 大数据\ndate: 2019-05-05 23:15:00\n---\nYarn (Yet Another Resource Manager) 是Hadoop 2.0引入的集群计算资源管理系统，最初是为了改善MapReduce任务调度过程，同时也可以支持其他多种分布式计算模式，Yarn不与任何一种计算框架耦合，只参与集群计算资源（CPU，内存等）的分配以及计算任务的调度\n\n探究MapReduce任务从调用`submit()`提交任务到Yarn运行任务的整个过程是件很有意思的事\n\n<!--more-->\n\n### 初始化任务\n\n初始化任务包括以下四个阶段：\n\n1. `申请任务`：主要是向Yarn申请`jobId`\n2. `保存job执行文件`：保存job配置信息，分片信息和Jar包等文件\n3. `加入任务队列`：向`ResourceManager`提交任务，加入任务队列\n\n#### 申请任务\n\nMapReduce的Client在调用`job.submit()`后，交由`JobSubmitter`进行任务提交，调用`submitJobInternal`方法首先申请一个`jobId`:\n\n```java\n//... 略\nJobID jobId = submitClient.getNewJobID();\njob.setJobID(jobId);\n//... 并行度切分，保存Job执行文件，提交任务等\n```\n\n其中`submitClient`是mapreduce的RPC client，有两种实现\n\n- `LocalJobRunner`： 用于提交本地运行的任务，本地环境测试就是使用的这个client\n- `YARNRunner`：用于向Yarn集群提交任务\n\n如果配置Job时设置配置项`mapreduce.framework.name`为`yarn`，mapreduce将采用`YARNRunner`作为client进行任务提交工作，`YARNRunner`为当前Job分配一个`jobId`作为本次任务的唯一ID\n\n#### 保存Job执行文件\n\nJob执行文件包括job.splits, job.xml和job的Jar包等文件，mapreduce向文件系统（本地文件系统或者hdfs）申请一块区域用于存放执行文件：\n\n```java\n//... \nPath submitJobDir = new Path(jobStagingArea, jobId.toString());\n//...\nPath submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\n```\n\n`submitJobDir`是用于保存Job执行文件的目录，`submitJobFile`即为当前Job文件的目录，格式为`.../staging/jobId`\n\n#### 加入任务队列\n\n使用`submitClient`向Yarn集群（在这里为`ResourceManager`节点）发起RPC请求提交任务：\n\n```java\nstatus = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\n```\n\n`ResourceManager`会把当前Job加入到任务执行队列中待有可执行任务的资源可用后启动该任务\n\n### 运行Job\n\n集群中能够运行Job的资源是有限的，队列中待执行的Job想要运行需要满足一定的条件，目前Yarn提供了三种任务调度策略：`FIFO调度`，`容量调度`，`公平调度`，日后再分析\n\n#### 启动Container\n\n`ResourceManager`会定期接收各个`NodeManager`发来的节点资源使用信息，某个Job满足运行条件后首先需要申请一个可以运行任务的`NodeManager`，在之上启动一个“容器”：`Container`\n\n> 如何理解Yarn的Container？\n>\n> 可以理解为Yarn为Job运行而启动的一个运行环境，这个运行环境包含运行资源（程序运行所需要的数据，内存占用，还有Vcores虚拟核数，CPU占用的一个虚拟量化指标）\n\n如果Job配置了本地限制（即任务所需的优先需要加载本地HDFS资源，或者同一机架的HDFS副本），`ResourceManager`申请容器运行的节点时会优先申请存储有所需副本的节点，如果实在找不到再基于hadoop网络拓扑模型寻找当前机架的其他节点或者其他机架的节点，使得Job运行时所需要的数据尽量为本地数据，降低对集群带宽的依赖\n\n#### 启动MrAppMaster\n\n启动`Container`后，client会申请在这个“容器”中启动`MrAppMaster`，这个`MrAppMaster`读取Job执行文件，获取Job的配置文件和splits等信息，然后根据这些配置文件进行接下来的任务（直接运行任务，或者申请更多的`Container`并行启动任务）\n\n根据splits规划，如果需要申请更多节点运行并行任务，`MrAppMaster`会向`ResourceManager`申请启动更多的`Container`，然后在这些`Container`中启动mapTask（或者reduceTask），这些task进程在Yarn环境中统一称为`YarnChild`\n\n#### 启动Task\n\n`MrAppMaster`启动完成后，根据splits启动多个mapTask，待mapTask均完成后，再根据该job配置的reduce数目启动多个reduceTask，启动流程与mapTask完全一样，Yarn并不关心具体执行的什么任务，它只需要接收`MrAppMaster`的资源分配请求然后申请启动相应数量的`Container`即可，启动完成后任务内部的交互也不由Yarn负责，当Job完成后再向client返回任务执行结果\n\n### Yarn的特点\n\nYarn作为通用性很强的分布式计算资源调度框架，能够很好地和多种计算框架如MapReduce, Spark, Storm等进行集成，计算框架专注于计算逻辑的实现，Yarn则专注于集群资源的分配和调度\n\n对于除了MapReduce以外的其他计算框架，把上述的`MrAppMaster`替换为任何一种Master进程，把mapTask或者reduceTask替换为任何一个work进程，对于Yarn来说都没有问题，只要实现了Yarn的规范和api，都可以在Yarn上面运行\n\n","slug":"Yarn初探","published":1,"updated":"2019-05-05T15:16:31.806Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3h0008g0qrpaxeetmk","content":"<p>Yarn (Yet Another Resource Manager) 是Hadoop 2.0引入的集群计算资源管理系统，最初是为了改善MapReduce任务调度过程，同时也可以支持其他多种分布式计算模式，Yarn不与任何一种计算框架耦合，只参与集群计算资源（CPU，内存等）的分配以及计算任务的调度</p>\n<p>探究MapReduce任务从调用<code>submit()</code>提交任务到Yarn运行任务的整个过程是件很有意思的事</p>\n<a id=\"more\"></a>\n<h3 id=\"初始化任务\"><a href=\"#初始化任务\" class=\"headerlink\" title=\"初始化任务\"></a>初始化任务</h3><p>初始化任务包括以下四个阶段：</p>\n<ol>\n<li><code>申请任务</code>：主要是向Yarn申请<code>jobId</code></li>\n<li><code>保存job执行文件</code>：保存job配置信息，分片信息和Jar包等文件</li>\n<li><code>加入任务队列</code>：向<code>ResourceManager</code>提交任务，加入任务队列</li>\n</ol>\n<h4 id=\"申请任务\"><a href=\"#申请任务\" class=\"headerlink\" title=\"申请任务\"></a>申请任务</h4><p>MapReduce的Client在调用<code>job.submit()</code>后，交由<code>JobSubmitter</code>进行任务提交，调用<code>submitJobInternal</code>方法首先申请一个<code>jobId</code>:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//... 略</span></span><br><span class=\"line\">JobID jobId = submitClient.getNewJobID();</span><br><span class=\"line\">job.setJobID(jobId);</span><br><span class=\"line\"><span class=\"comment\">//... 并行度切分，保存Job执行文件，提交任务等</span></span><br></pre></td></tr></table></figure>\n<p>其中<code>submitClient</code>是mapreduce的RPC client，有两种实现</p>\n<ul>\n<li><code>LocalJobRunner</code>： 用于提交本地运行的任务，本地环境测试就是使用的这个client</li>\n<li><code>YARNRunner</code>：用于向Yarn集群提交任务</li>\n</ul>\n<p>如果配置Job时设置配置项<code>mapreduce.framework.name</code>为<code>yarn</code>，mapreduce将采用<code>YARNRunner</code>作为client进行任务提交工作，<code>YARNRunner</code>为当前Job分配一个<code>jobId</code>作为本次任务的唯一ID</p>\n<h4 id=\"保存Job执行文件\"><a href=\"#保存Job执行文件\" class=\"headerlink\" title=\"保存Job执行文件\"></a>保存Job执行文件</h4><p>Job执行文件包括job.splits, job.xml和job的Jar包等文件，mapreduce向文件系统（本地文件系统或者hdfs）申请一块区域用于存放执行文件：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//... </span></span><br><span class=\"line\">Path submitJobDir = <span class=\"keyword\">new</span> Path(jobStagingArea, jobId.toString());</span><br><span class=\"line\"><span class=\"comment\">//...</span></span><br><span class=\"line\">Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);</span><br></pre></td></tr></table></figure>\n<p><code>submitJobDir</code>是用于保存Job执行文件的目录，<code>submitJobFile</code>即为当前Job文件的目录，格式为<code>.../staging/jobId</code></p>\n<h4 id=\"加入任务队列\"><a href=\"#加入任务队列\" class=\"headerlink\" title=\"加入任务队列\"></a>加入任务队列</h4><p>使用<code>submitClient</code>向Yarn集群（在这里为<code>ResourceManager</code>节点）发起RPC请求提交任务：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>\n<p><code>ResourceManager</code>会把当前Job加入到任务执行队列中待有可执行任务的资源可用后启动该任务</p>\n<h3 id=\"运行Job\"><a href=\"#运行Job\" class=\"headerlink\" title=\"运行Job\"></a>运行Job</h3><p>集群中能够运行Job的资源是有限的，队列中待执行的Job想要运行需要满足一定的条件，目前Yarn提供了三种任务调度策略：<code>FIFO调度</code>，<code>容量调度</code>，<code>公平调度</code>，日后再分析</p>\n<h4 id=\"启动Container\"><a href=\"#启动Container\" class=\"headerlink\" title=\"启动Container\"></a>启动Container</h4><p><code>ResourceManager</code>会定期接收各个<code>NodeManager</code>发来的节点资源使用信息，某个Job满足运行条件后首先需要申请一个可以运行任务的<code>NodeManager</code>，在之上启动一个“容器”：<code>Container</code></p>\n<blockquote>\n<p>如何理解Yarn的Container？</p>\n<p>可以理解为Yarn为Job运行而启动的一个运行环境，这个运行环境包含运行资源（程序运行所需要的数据，内存占用，还有Vcores虚拟核数，CPU占用的一个虚拟量化指标）</p>\n</blockquote>\n<p>如果Job配置了本地限制（即任务所需的优先需要加载本地HDFS资源，或者同一机架的HDFS副本），<code>ResourceManager</code>申请容器运行的节点时会优先申请存储有所需副本的节点，如果实在找不到再基于hadoop网络拓扑模型寻找当前机架的其他节点或者其他机架的节点，使得Job运行时所需要的数据尽量为本地数据，降低对集群带宽的依赖</p>\n<h4 id=\"启动MrAppMaster\"><a href=\"#启动MrAppMaster\" class=\"headerlink\" title=\"启动MrAppMaster\"></a>启动MrAppMaster</h4><p>启动<code>Container</code>后，client会申请在这个“容器”中启动<code>MrAppMaster</code>，这个<code>MrAppMaster</code>读取Job执行文件，获取Job的配置文件和splits等信息，然后根据这些配置文件进行接下来的任务（直接运行任务，或者申请更多的<code>Container</code>并行启动任务）</p>\n<p>根据splits规划，如果需要申请更多节点运行并行任务，<code>MrAppMaster</code>会向<code>ResourceManager</code>申请启动更多的<code>Container</code>，然后在这些<code>Container</code>中启动mapTask（或者reduceTask），这些task进程在Yarn环境中统一称为<code>YarnChild</code></p>\n<h4 id=\"启动Task\"><a href=\"#启动Task\" class=\"headerlink\" title=\"启动Task\"></a>启动Task</h4><p><code>MrAppMaster</code>启动完成后，根据splits启动多个mapTask，待mapTask均完成后，再根据该job配置的reduce数目启动多个reduceTask，启动流程与mapTask完全一样，Yarn并不关心具体执行的什么任务，它只需要接收<code>MrAppMaster</code>的资源分配请求然后申请启动相应数量的<code>Container</code>即可，启动完成后任务内部的交互也不由Yarn负责，当Job完成后再向client返回任务执行结果</p>\n<h3 id=\"Yarn的特点\"><a href=\"#Yarn的特点\" class=\"headerlink\" title=\"Yarn的特点\"></a>Yarn的特点</h3><p>Yarn作为通用性很强的分布式计算资源调度框架，能够很好地和多种计算框架如MapReduce, Spark, Storm等进行集成，计算框架专注于计算逻辑的实现，Yarn则专注于集群资源的分配和调度</p>\n<p>对于除了MapReduce以外的其他计算框架，把上述的<code>MrAppMaster</code>替换为任何一种Master进程，把mapTask或者reduceTask替换为任何一个work进程，对于Yarn来说都没有问题，只要实现了Yarn的规范和api，都可以在Yarn上面运行</p>\n","site":{"data":{}},"excerpt":"<p>Yarn (Yet Another Resource Manager) 是Hadoop 2.0引入的集群计算资源管理系统，最初是为了改善MapReduce任务调度过程，同时也可以支持其他多种分布式计算模式，Yarn不与任何一种计算框架耦合，只参与集群计算资源（CPU，内存等）的分配以及计算任务的调度</p>\n<p>探究MapReduce任务从调用<code>submit()</code>提交任务到Yarn运行任务的整个过程是件很有意思的事</p>","more":"<h3 id=\"初始化任务\"><a href=\"#初始化任务\" class=\"headerlink\" title=\"初始化任务\"></a>初始化任务</h3><p>初始化任务包括以下四个阶段：</p>\n<ol>\n<li><code>申请任务</code>：主要是向Yarn申请<code>jobId</code></li>\n<li><code>保存job执行文件</code>：保存job配置信息，分片信息和Jar包等文件</li>\n<li><code>加入任务队列</code>：向<code>ResourceManager</code>提交任务，加入任务队列</li>\n</ol>\n<h4 id=\"申请任务\"><a href=\"#申请任务\" class=\"headerlink\" title=\"申请任务\"></a>申请任务</h4><p>MapReduce的Client在调用<code>job.submit()</code>后，交由<code>JobSubmitter</code>进行任务提交，调用<code>submitJobInternal</code>方法首先申请一个<code>jobId</code>:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//... 略</span></span><br><span class=\"line\">JobID jobId = submitClient.getNewJobID();</span><br><span class=\"line\">job.setJobID(jobId);</span><br><span class=\"line\"><span class=\"comment\">//... 并行度切分，保存Job执行文件，提交任务等</span></span><br></pre></td></tr></table></figure>\n<p>其中<code>submitClient</code>是mapreduce的RPC client，有两种实现</p>\n<ul>\n<li><code>LocalJobRunner</code>： 用于提交本地运行的任务，本地环境测试就是使用的这个client</li>\n<li><code>YARNRunner</code>：用于向Yarn集群提交任务</li>\n</ul>\n<p>如果配置Job时设置配置项<code>mapreduce.framework.name</code>为<code>yarn</code>，mapreduce将采用<code>YARNRunner</code>作为client进行任务提交工作，<code>YARNRunner</code>为当前Job分配一个<code>jobId</code>作为本次任务的唯一ID</p>\n<h4 id=\"保存Job执行文件\"><a href=\"#保存Job执行文件\" class=\"headerlink\" title=\"保存Job执行文件\"></a>保存Job执行文件</h4><p>Job执行文件包括job.splits, job.xml和job的Jar包等文件，mapreduce向文件系统（本地文件系统或者hdfs）申请一块区域用于存放执行文件：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//... </span></span><br><span class=\"line\">Path submitJobDir = <span class=\"keyword\">new</span> Path(jobStagingArea, jobId.toString());</span><br><span class=\"line\"><span class=\"comment\">//...</span></span><br><span class=\"line\">Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);</span><br></pre></td></tr></table></figure>\n<p><code>submitJobDir</code>是用于保存Job执行文件的目录，<code>submitJobFile</code>即为当前Job文件的目录，格式为<code>.../staging/jobId</code></p>\n<h4 id=\"加入任务队列\"><a href=\"#加入任务队列\" class=\"headerlink\" title=\"加入任务队列\"></a>加入任务队列</h4><p>使用<code>submitClient</code>向Yarn集群（在这里为<code>ResourceManager</code>节点）发起RPC请求提交任务：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>\n<p><code>ResourceManager</code>会把当前Job加入到任务执行队列中待有可执行任务的资源可用后启动该任务</p>\n<h3 id=\"运行Job\"><a href=\"#运行Job\" class=\"headerlink\" title=\"运行Job\"></a>运行Job</h3><p>集群中能够运行Job的资源是有限的，队列中待执行的Job想要运行需要满足一定的条件，目前Yarn提供了三种任务调度策略：<code>FIFO调度</code>，<code>容量调度</code>，<code>公平调度</code>，日后再分析</p>\n<h4 id=\"启动Container\"><a href=\"#启动Container\" class=\"headerlink\" title=\"启动Container\"></a>启动Container</h4><p><code>ResourceManager</code>会定期接收各个<code>NodeManager</code>发来的节点资源使用信息，某个Job满足运行条件后首先需要申请一个可以运行任务的<code>NodeManager</code>，在之上启动一个“容器”：<code>Container</code></p>\n<blockquote>\n<p>如何理解Yarn的Container？</p>\n<p>可以理解为Yarn为Job运行而启动的一个运行环境，这个运行环境包含运行资源（程序运行所需要的数据，内存占用，还有Vcores虚拟核数，CPU占用的一个虚拟量化指标）</p>\n</blockquote>\n<p>如果Job配置了本地限制（即任务所需的优先需要加载本地HDFS资源，或者同一机架的HDFS副本），<code>ResourceManager</code>申请容器运行的节点时会优先申请存储有所需副本的节点，如果实在找不到再基于hadoop网络拓扑模型寻找当前机架的其他节点或者其他机架的节点，使得Job运行时所需要的数据尽量为本地数据，降低对集群带宽的依赖</p>\n<h4 id=\"启动MrAppMaster\"><a href=\"#启动MrAppMaster\" class=\"headerlink\" title=\"启动MrAppMaster\"></a>启动MrAppMaster</h4><p>启动<code>Container</code>后，client会申请在这个“容器”中启动<code>MrAppMaster</code>，这个<code>MrAppMaster</code>读取Job执行文件，获取Job的配置文件和splits等信息，然后根据这些配置文件进行接下来的任务（直接运行任务，或者申请更多的<code>Container</code>并行启动任务）</p>\n<p>根据splits规划，如果需要申请更多节点运行并行任务，<code>MrAppMaster</code>会向<code>ResourceManager</code>申请启动更多的<code>Container</code>，然后在这些<code>Container</code>中启动mapTask（或者reduceTask），这些task进程在Yarn环境中统一称为<code>YarnChild</code></p>\n<h4 id=\"启动Task\"><a href=\"#启动Task\" class=\"headerlink\" title=\"启动Task\"></a>启动Task</h4><p><code>MrAppMaster</code>启动完成后，根据splits启动多个mapTask，待mapTask均完成后，再根据该job配置的reduce数目启动多个reduceTask，启动流程与mapTask完全一样，Yarn并不关心具体执行的什么任务，它只需要接收<code>MrAppMaster</code>的资源分配请求然后申请启动相应数量的<code>Container</code>即可，启动完成后任务内部的交互也不由Yarn负责，当Job完成后再向client返回任务执行结果</p>\n<h3 id=\"Yarn的特点\"><a href=\"#Yarn的特点\" class=\"headerlink\" title=\"Yarn的特点\"></a>Yarn的特点</h3><p>Yarn作为通用性很强的分布式计算资源调度框架，能够很好地和多种计算框架如MapReduce, Spark, Storm等进行集成，计算框架专注于计算逻辑的实现，Yarn则专注于集群资源的分配和调度</p>\n<p>对于除了MapReduce以外的其他计算框架，把上述的<code>MrAppMaster</code>替换为任何一种Master进程，把mapTask或者reduceTask替换为任何一个work进程，对于Yarn来说都没有问题，只要实现了Yarn的规范和api，都可以在Yarn上面运行</p>"},{"title":"Reactor-Kafka（1）","author":"天渊","date":"2019-02-02T10:10:00.000Z","_content":"reactor-kafka项目是遵循响应式流（Reactive Streams）规范的Kafka client，有Producer实现和Consumer实现。\n<!-- more -->\n\n### 首先讲讲Reactive Streams规范\n\n在传统的编程范式中，我们一般通过迭代器（Iterator）模式来遍历一个序列。这种遍历方式是由调用者来控制节奏的，采用的是拉的方式。每次由调用者通过 next()方法来获取序列中的下一个值。\n\n响应式流（Reactive Streams）规范则是推的方式，即常见的发布者-订阅者模式。当发布者有新的数据产生时，这些数据会被推送到订阅者来进行处理。在反应式流上可以添加各种不同的操作来对数据进行处理，形成数据处理链。这个以声明式的方式添加的处理链只在订阅者进行订阅操作时才会真正执行。\n\n响应式流规范体现到Jdk中即为Java 8的Stream Api和Java 9的Flow Api，再结合Java 8的Lambda函数式编程模型，形成了独特的Reactive响应式异步编程模型，目前最重要的Reactive实现项目即为Pivatol维护的`project-reactor`，诸多项目基于`project-reactor`对原有项目进行了遵循响应式规范的重构，包括`WebFlux`和`Reactor-mongodb`，以及这里要介绍的`Reactor-Kafka`。\n\n#### Flux和Mono\n\nFlux和Mono是project-reactor中最重要的两个基本概念，可以把他们理解为发布订阅模型中的发布者，他们均实现了`org.reactivestreams.Publisher`接口，Mono表示的是包含 0 到 1 个发布者的异步序列，Flux 表示的是包含 0 到 N 个发布者的异步序列。\n\n在Flux和Mono发布订阅序列中可以包含三种不同类型的消息通知：正常的包含元素的消息、序列结束的消息和序列出错的消息，分别包含`onNext()`, `onComplete()`和 `onError()`三个回调；当发布者产生需要消费的元素时，用户使用Stream流处理的方式对序列上的元素进行处理，比如`map()`或者`filter()`等等；最终调用`onSubscribe()`完成订阅。\n\n如下图所示，通过Flux（或者Mono）将整个数据库调用链以响应式异步序列的方式贯通起来，再由reactive的web服务器（通常是reactive-netty）发布给用户：\n\n![1548985987329](/blog/images/1548985987329.png)\n\n整个过程均为非阻塞，吞吐量能得到极大提升。\n\n### Reactor-Kafka\n\n关于`Reactor-Kafka`，官方文档描述如下：\n\n> [Reactor Kafka](https://projectreactor.io/docs/kafka/release/api/index.html) is a reactive API for Kafka based on Reactor and the Kafka Producer/Consumer API. Reactor Kafka API enables messages to be published to Kafka and consumed from Kafka using functional APIs with non-blocking back-pressure and very low overheads. This enables applications using Reactor to use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline.\n\n> Reactor-Kafka是基于Reactor和kafka Producer/Consumer API开发的kafka响应式API客户端，可以通过非阻塞的函数式API来发布消息到kafka并消费kafka的消息，性能消耗很低。这套API能够让那些使用Reactor标准库（即Flux和Mono）的应用程序像message bus或者streaming platform一样使用kafka，并且和其他系统集成，提供端到端的响应式管道。\n\n也就是说这套Reactor-Kafka的API能够让使用者很轻易地将Kafka与其他使用Reactor api的系统集成起来，形成一套完整的响应式流处理管路。\n\n#### 加入java依赖\n\nmaven：\n\n```xml\n<dependency>\n    <groupId>io.projectreactor.kafka</groupId>\n    <artifactId>reactor-kafka</artifactId>\n    <version>1.1.0.RELEASE</version>\n</dependency>\n```\n\ngradle:\n\n```groovy\ndependencies {\n    compile \"io.projectreactor.kafka:reactor-kafka:1.1.0.RELEASE\"\n}\n```\n\n#### reactor consumer api\n\nReactor-Kafka的consumer的核心api是`reactor.kafka.receiver.KafkaReceiver`\n\n**属性配置**：\n\n```java\nMap<String, Object> consumerProps = new HashMap<>();\nconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\nconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\nconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\nconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n```\n\n对基本的bootstrapServers和groupId以及serializer等基本属性进行配置，其他的consumer属性参考Kafka官方文档；创建一个`ReceiverOptions`对象对属性进行封装：\n\n```java\nReceiverOptions<String, String> receiverOptions = ReceiverOptions.<String, String>create(consumerProps())\n    .subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n`.subscription`方法可以订阅多个topic\n\n**KafkaReceiver**：\n\n`KafkaReceiver`是reactor-kafka的consumer核心api：\n\n```java\nFlux<ReceiverRecord<String, String>> recordFlux = KafkaReceiver.create(receiverOptions()).receive();\n```\n\n`FLux`对象用于多个发布者，也就是说在`KafkaReceiver`中对应多个topic进行消费，其中key和value均设置为`String`类型：\n\n```java\nrecordFlux\n\t.log()\t//打印日志\n    .doOnNext(r -> r.receiverOffset().acknowledge()) //将当前record标记为已处理\n    .map(ReceiverRecord::value) //将元素由ReceiverRecord对象替换为其value\n    .doOnNext(r -> handler.saveResource(r)) //处理获得的record的value\n    .doOnError(e -> log.warn(\"消费出错\", e)) //处理过程中出错则打印日志\n    .subscribe(); //启动订阅\n```\n\n在该调用链上设置多个回调函数，对发布的消息进行顺序消费；\n\n该方法是非阻塞的，调用完后立即返回，不会阻塞用户线程，而是由reactor-kafka启动异步的EventLoop进行消息的获取并由worker线程调用用户设置的回调函数进行消费；\n\n如果`KafkaReceiver`接收到某个topic的一条消息则会顺序地调用回调函数进行处理；\n\n需要注意的是调用链最后都要调用`subscribe()`方法启动订阅，否则整个调用链并不会生效，并且一个`Flux`或者`Mono`对象只能订阅一次，如果多次订阅的话会报错：\n\n```java\njava.lang.IllegalStateException: Multiple subscribers are not supported for KafkaReceiver flux\n```\n\n如果不想马上结束整个流处理过程的话，可以不用立即调用`subscribe()`方法，而是将`Flux`对象作为一个载体传下去，这也就是官网提到的`use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline`的意义所在，比如，我想将消费得到的String存入mongodb，则可以当前`Flux`对象传给`reactor-mongodb`，再由`reactor-mongodb`返回一个`Flux`对象，形成完整的Stream链：\n\n```java\n// 从kafka消费得到String数据\nFlux<String> receivedStrFlux = KafkaReceiver\n    .create(receiverOptions())\n    .receive()\n    .log()\n    .doOnNext(r -> r.receiverOffset().acknowledge())\n    .map(ReceiverRecord::value)\n    .doOnError(e -> log.warn(\"消费出错\", e));\n// 将String经过一系列转换得到一个包含Resource对象的Flux\nFlux<Resource> resourceFlux = receivedStrFlux\n\t.map(this::handleResourceString)\n    .flatMap(Flux::fromIterable)\n    .map(resourceData -> {\n        Resource resource = new Resource();\n        BeanUtils.copyProperties(resourceData, resource);\n        return resource;\n    });\n// 将包含Resource对象的Flux通过reactor-mongodb进行存储\n// 得到另一个Flux<Resource>\nFlux<Resource> resourceSavedFlux = resourceRepository.saveAll(resourceFlux);\n// 对这个Flux<Resource>进行订阅\nresourceSavedFlux\n\t.doOnError(e -> log.warn(\"出错啦！\", e))\n    .doOnComplete(() -> log.info(\"都存完啦！\"))\n    .log()\n    .subscribe();\n```\n\n以上就是一个完整的reactor-kafka+reactor-mongodb的异步响应式流处理链\n\n#### reactor consumer api其他配置\n\n除了基本的配置，reactor consumer api还可以进行一些进一步的设置\n\n单独订阅序号为0的partition：\n\n```java\nreceiverOptions = receiverOptions.assignment(Collections.singleton(new TopicPartition(topic, 0)));\n```\n\n当调用`.doOnNext(r -> r.receiverOffset().acknowledge())`时，该record的offset并不会立即提交，而是加入一个等待提交队列进行周期性自动提交，当然也可以调用`r.receiverOffset().commit()`方法手动提交该offset，`commit()`后依然返回的是一个`Mono`对象：\n\n```java\n.doOnNext(r -> r.receiverOffset().commit().doOnSuccess(aVoid -> log.info(\"offset提交成功！\")).subscribe())\n```\n\n`KafkaReceiver`接收kafka信息除了使用`receive()`消费最新的record，还可以手动指定`offset`进行消费：\n\n```java\nReceiverOptions.<String, String>create(consumerProps())\t.addAssignListener(receiverPartitions -> receiverPartitions.forEach(ReceiverPartition::seekToBeginning))\t\t.subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n使用`seekToBeginning`在每次初始化后从头开始消费，或者直接指定offset：\n\n```java\nReceiverOptions.<String, String>create(consumerProps())\t.addAssignListener(receiverPartitions -> receiverPartitions.forEach(r -> r.seek(140))).subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n从offset=140的位置开始消费\n\n#### reactor-kafka-consumer的生命周期\n\n每个`KafkaReceiver`实例的生命周期都跟对应的`Flux`相关，`Flux`结束消费则相应的`KafkaReceiver`就会被关闭.","source":"_posts/Reactor-Kafka（1）.md","raw":"title: Reactor-Kafka（1）\nauthor: 天渊\ntags:\n  - Kafka\n  - reactor\ncategories:\n  - 基础知识\ndate: 2019-02-02 18:10:00\n---\nreactor-kafka项目是遵循响应式流（Reactive Streams）规范的Kafka client，有Producer实现和Consumer实现。\n<!-- more -->\n\n### 首先讲讲Reactive Streams规范\n\n在传统的编程范式中，我们一般通过迭代器（Iterator）模式来遍历一个序列。这种遍历方式是由调用者来控制节奏的，采用的是拉的方式。每次由调用者通过 next()方法来获取序列中的下一个值。\n\n响应式流（Reactive Streams）规范则是推的方式，即常见的发布者-订阅者模式。当发布者有新的数据产生时，这些数据会被推送到订阅者来进行处理。在反应式流上可以添加各种不同的操作来对数据进行处理，形成数据处理链。这个以声明式的方式添加的处理链只在订阅者进行订阅操作时才会真正执行。\n\n响应式流规范体现到Jdk中即为Java 8的Stream Api和Java 9的Flow Api，再结合Java 8的Lambda函数式编程模型，形成了独特的Reactive响应式异步编程模型，目前最重要的Reactive实现项目即为Pivatol维护的`project-reactor`，诸多项目基于`project-reactor`对原有项目进行了遵循响应式规范的重构，包括`WebFlux`和`Reactor-mongodb`，以及这里要介绍的`Reactor-Kafka`。\n\n#### Flux和Mono\n\nFlux和Mono是project-reactor中最重要的两个基本概念，可以把他们理解为发布订阅模型中的发布者，他们均实现了`org.reactivestreams.Publisher`接口，Mono表示的是包含 0 到 1 个发布者的异步序列，Flux 表示的是包含 0 到 N 个发布者的异步序列。\n\n在Flux和Mono发布订阅序列中可以包含三种不同类型的消息通知：正常的包含元素的消息、序列结束的消息和序列出错的消息，分别包含`onNext()`, `onComplete()`和 `onError()`三个回调；当发布者产生需要消费的元素时，用户使用Stream流处理的方式对序列上的元素进行处理，比如`map()`或者`filter()`等等；最终调用`onSubscribe()`完成订阅。\n\n如下图所示，通过Flux（或者Mono）将整个数据库调用链以响应式异步序列的方式贯通起来，再由reactive的web服务器（通常是reactive-netty）发布给用户：\n\n![1548985987329](/blog/images/1548985987329.png)\n\n整个过程均为非阻塞，吞吐量能得到极大提升。\n\n### Reactor-Kafka\n\n关于`Reactor-Kafka`，官方文档描述如下：\n\n> [Reactor Kafka](https://projectreactor.io/docs/kafka/release/api/index.html) is a reactive API for Kafka based on Reactor and the Kafka Producer/Consumer API. Reactor Kafka API enables messages to be published to Kafka and consumed from Kafka using functional APIs with non-blocking back-pressure and very low overheads. This enables applications using Reactor to use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline.\n\n> Reactor-Kafka是基于Reactor和kafka Producer/Consumer API开发的kafka响应式API客户端，可以通过非阻塞的函数式API来发布消息到kafka并消费kafka的消息，性能消耗很低。这套API能够让那些使用Reactor标准库（即Flux和Mono）的应用程序像message bus或者streaming platform一样使用kafka，并且和其他系统集成，提供端到端的响应式管道。\n\n也就是说这套Reactor-Kafka的API能够让使用者很轻易地将Kafka与其他使用Reactor api的系统集成起来，形成一套完整的响应式流处理管路。\n\n#### 加入java依赖\n\nmaven：\n\n```xml\n<dependency>\n    <groupId>io.projectreactor.kafka</groupId>\n    <artifactId>reactor-kafka</artifactId>\n    <version>1.1.0.RELEASE</version>\n</dependency>\n```\n\ngradle:\n\n```groovy\ndependencies {\n    compile \"io.projectreactor.kafka:reactor-kafka:1.1.0.RELEASE\"\n}\n```\n\n#### reactor consumer api\n\nReactor-Kafka的consumer的核心api是`reactor.kafka.receiver.KafkaReceiver`\n\n**属性配置**：\n\n```java\nMap<String, Object> consumerProps = new HashMap<>();\nconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\nconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\nconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\nconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n```\n\n对基本的bootstrapServers和groupId以及serializer等基本属性进行配置，其他的consumer属性参考Kafka官方文档；创建一个`ReceiverOptions`对象对属性进行封装：\n\n```java\nReceiverOptions<String, String> receiverOptions = ReceiverOptions.<String, String>create(consumerProps())\n    .subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n`.subscription`方法可以订阅多个topic\n\n**KafkaReceiver**：\n\n`KafkaReceiver`是reactor-kafka的consumer核心api：\n\n```java\nFlux<ReceiverRecord<String, String>> recordFlux = KafkaReceiver.create(receiverOptions()).receive();\n```\n\n`FLux`对象用于多个发布者，也就是说在`KafkaReceiver`中对应多个topic进行消费，其中key和value均设置为`String`类型：\n\n```java\nrecordFlux\n\t.log()\t//打印日志\n    .doOnNext(r -> r.receiverOffset().acknowledge()) //将当前record标记为已处理\n    .map(ReceiverRecord::value) //将元素由ReceiverRecord对象替换为其value\n    .doOnNext(r -> handler.saveResource(r)) //处理获得的record的value\n    .doOnError(e -> log.warn(\"消费出错\", e)) //处理过程中出错则打印日志\n    .subscribe(); //启动订阅\n```\n\n在该调用链上设置多个回调函数，对发布的消息进行顺序消费；\n\n该方法是非阻塞的，调用完后立即返回，不会阻塞用户线程，而是由reactor-kafka启动异步的EventLoop进行消息的获取并由worker线程调用用户设置的回调函数进行消费；\n\n如果`KafkaReceiver`接收到某个topic的一条消息则会顺序地调用回调函数进行处理；\n\n需要注意的是调用链最后都要调用`subscribe()`方法启动订阅，否则整个调用链并不会生效，并且一个`Flux`或者`Mono`对象只能订阅一次，如果多次订阅的话会报错：\n\n```java\njava.lang.IllegalStateException: Multiple subscribers are not supported for KafkaReceiver flux\n```\n\n如果不想马上结束整个流处理过程的话，可以不用立即调用`subscribe()`方法，而是将`Flux`对象作为一个载体传下去，这也就是官网提到的`use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline`的意义所在，比如，我想将消费得到的String存入mongodb，则可以当前`Flux`对象传给`reactor-mongodb`，再由`reactor-mongodb`返回一个`Flux`对象，形成完整的Stream链：\n\n```java\n// 从kafka消费得到String数据\nFlux<String> receivedStrFlux = KafkaReceiver\n    .create(receiverOptions())\n    .receive()\n    .log()\n    .doOnNext(r -> r.receiverOffset().acknowledge())\n    .map(ReceiverRecord::value)\n    .doOnError(e -> log.warn(\"消费出错\", e));\n// 将String经过一系列转换得到一个包含Resource对象的Flux\nFlux<Resource> resourceFlux = receivedStrFlux\n\t.map(this::handleResourceString)\n    .flatMap(Flux::fromIterable)\n    .map(resourceData -> {\n        Resource resource = new Resource();\n        BeanUtils.copyProperties(resourceData, resource);\n        return resource;\n    });\n// 将包含Resource对象的Flux通过reactor-mongodb进行存储\n// 得到另一个Flux<Resource>\nFlux<Resource> resourceSavedFlux = resourceRepository.saveAll(resourceFlux);\n// 对这个Flux<Resource>进行订阅\nresourceSavedFlux\n\t.doOnError(e -> log.warn(\"出错啦！\", e))\n    .doOnComplete(() -> log.info(\"都存完啦！\"))\n    .log()\n    .subscribe();\n```\n\n以上就是一个完整的reactor-kafka+reactor-mongodb的异步响应式流处理链\n\n#### reactor consumer api其他配置\n\n除了基本的配置，reactor consumer api还可以进行一些进一步的设置\n\n单独订阅序号为0的partition：\n\n```java\nreceiverOptions = receiverOptions.assignment(Collections.singleton(new TopicPartition(topic, 0)));\n```\n\n当调用`.doOnNext(r -> r.receiverOffset().acknowledge())`时，该record的offset并不会立即提交，而是加入一个等待提交队列进行周期性自动提交，当然也可以调用`r.receiverOffset().commit()`方法手动提交该offset，`commit()`后依然返回的是一个`Mono`对象：\n\n```java\n.doOnNext(r -> r.receiverOffset().commit().doOnSuccess(aVoid -> log.info(\"offset提交成功！\")).subscribe())\n```\n\n`KafkaReceiver`接收kafka信息除了使用`receive()`消费最新的record，还可以手动指定`offset`进行消费：\n\n```java\nReceiverOptions.<String, String>create(consumerProps())\t.addAssignListener(receiverPartitions -> receiverPartitions.forEach(ReceiverPartition::seekToBeginning))\t\t.subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n使用`seekToBeginning`在每次初始化后从头开始消费，或者直接指定offset：\n\n```java\nReceiverOptions.<String, String>create(consumerProps())\t.addAssignListener(receiverPartitions -> receiverPartitions.forEach(r -> r.seek(140))).subscription(Collections.singleton(\"resource-v1-TestProduct-TestType\"));\n```\n\n从offset=140的位置开始消费\n\n#### reactor-kafka-consumer的生命周期\n\n每个`KafkaReceiver`实例的生命周期都跟对应的`Flux`相关，`Flux`结束消费则相应的`KafkaReceiver`就会被关闭.","slug":"Reactor-Kafka（1）","published":1,"updated":"2019-03-19T13:30:58.420Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3r000xg0qr2aca0ugn","content":"<p>reactor-kafka项目是遵循响应式流（Reactive Streams）规范的Kafka client，有Producer实现和Consumer实现。<br><a id=\"more\"></a></p>\n<h3 id=\"首先讲讲Reactive-Streams规范\"><a href=\"#首先讲讲Reactive-Streams规范\" class=\"headerlink\" title=\"首先讲讲Reactive Streams规范\"></a>首先讲讲Reactive Streams规范</h3><p>在传统的编程范式中，我们一般通过迭代器（Iterator）模式来遍历一个序列。这种遍历方式是由调用者来控制节奏的，采用的是拉的方式。每次由调用者通过 next()方法来获取序列中的下一个值。</p>\n<p>响应式流（Reactive Streams）规范则是推的方式，即常见的发布者-订阅者模式。当发布者有新的数据产生时，这些数据会被推送到订阅者来进行处理。在反应式流上可以添加各种不同的操作来对数据进行处理，形成数据处理链。这个以声明式的方式添加的处理链只在订阅者进行订阅操作时才会真正执行。</p>\n<p>响应式流规范体现到Jdk中即为Java 8的Stream Api和Java 9的Flow Api，再结合Java 8的Lambda函数式编程模型，形成了独特的Reactive响应式异步编程模型，目前最重要的Reactive实现项目即为Pivatol维护的<code>project-reactor</code>，诸多项目基于<code>project-reactor</code>对原有项目进行了遵循响应式规范的重构，包括<code>WebFlux</code>和<code>Reactor-mongodb</code>，以及这里要介绍的<code>Reactor-Kafka</code>。</p>\n<h4 id=\"Flux和Mono\"><a href=\"#Flux和Mono\" class=\"headerlink\" title=\"Flux和Mono\"></a>Flux和Mono</h4><p>Flux和Mono是project-reactor中最重要的两个基本概念，可以把他们理解为发布订阅模型中的发布者，他们均实现了<code>org.reactivestreams.Publisher</code>接口，Mono表示的是包含 0 到 1 个发布者的异步序列，Flux 表示的是包含 0 到 N 个发布者的异步序列。</p>\n<p>在Flux和Mono发布订阅序列中可以包含三种不同类型的消息通知：正常的包含元素的消息、序列结束的消息和序列出错的消息，分别包含<code>onNext()</code>, <code>onComplete()</code>和 <code>onError()</code>三个回调；当发布者产生需要消费的元素时，用户使用Stream流处理的方式对序列上的元素进行处理，比如<code>map()</code>或者<code>filter()</code>等等；最终调用<code>onSubscribe()</code>完成订阅。</p>\n<p>如下图所示，通过Flux（或者Mono）将整个数据库调用链以响应式异步序列的方式贯通起来，再由reactive的web服务器（通常是reactive-netty）发布给用户：</p>\n<p><img src=\"/blog/images/1548985987329.png\" alt=\"1548985987329\"></p>\n<p>整个过程均为非阻塞，吞吐量能得到极大提升。</p>\n<h3 id=\"Reactor-Kafka\"><a href=\"#Reactor-Kafka\" class=\"headerlink\" title=\"Reactor-Kafka\"></a>Reactor-Kafka</h3><p>关于<code>Reactor-Kafka</code>，官方文档描述如下：</p>\n<blockquote>\n<p><a href=\"https://projectreactor.io/docs/kafka/release/api/index.html\" target=\"_blank\" rel=\"noopener\">Reactor Kafka</a> is a reactive API for Kafka based on Reactor and the Kafka Producer/Consumer API. Reactor Kafka API enables messages to be published to Kafka and consumed from Kafka using functional APIs with non-blocking back-pressure and very low overheads. This enables applications using Reactor to use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline.</p>\n</blockquote>\n<blockquote>\n<p>Reactor-Kafka是基于Reactor和kafka Producer/Consumer API开发的kafka响应式API客户端，可以通过非阻塞的函数式API来发布消息到kafka并消费kafka的消息，性能消耗很低。这套API能够让那些使用Reactor标准库（即Flux和Mono）的应用程序像message bus或者streaming platform一样使用kafka，并且和其他系统集成，提供端到端的响应式管道。</p>\n</blockquote>\n<p>也就是说这套Reactor-Kafka的API能够让使用者很轻易地将Kafka与其他使用Reactor api的系统集成起来，形成一套完整的响应式流处理管路。</p>\n<h4 id=\"加入java依赖\"><a href=\"#加入java依赖\" class=\"headerlink\" title=\"加入java依赖\"></a>加入java依赖</h4><p>maven：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>io.projectreactor.kafka<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>reactor-kafka<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.1.0.RELEASE<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>gradle:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    compile <span class=\"string\">\"io.projectreactor.kafka:reactor-kafka:1.1.0.RELEASE\"</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"reactor-consumer-api\"><a href=\"#reactor-consumer-api\" class=\"headerlink\" title=\"reactor consumer api\"></a>reactor consumer api</h4><p>Reactor-Kafka的consumer的核心api是<code>reactor.kafka.receiver.KafkaReceiver</code></p>\n<p><strong>属性配置</strong>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; consumerProps = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br></pre></td></tr></table></figure>\n<p>对基本的bootstrapServers和groupId以及serializer等基本属性进行配置，其他的consumer属性参考Kafka官方文档；创建一个<code>ReceiverOptions</code>对象对属性进行封装：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions&lt;String, String&gt; receiverOptions = ReceiverOptions.&lt;String, String&gt;create(consumerProps())</span><br><span class=\"line\">    .subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p><code>.subscription</code>方法可以订阅多个topic</p>\n<p><strong>KafkaReceiver</strong>：</p>\n<p><code>KafkaReceiver</code>是reactor-kafka的consumer核心api：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Flux&lt;ReceiverRecord&lt;String, String&gt;&gt; recordFlux = KafkaReceiver.create(receiverOptions()).receive();</span><br></pre></td></tr></table></figure>\n<p><code>FLux</code>对象用于多个发布者，也就是说在<code>KafkaReceiver</code>中对应多个topic进行消费，其中key和value均设置为<code>String</code>类型：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">recordFlux</span><br><span class=\"line\">\t.log()\t<span class=\"comment\">//打印日志</span></span><br><span class=\"line\">    .doOnNext(r -&gt; r.receiverOffset().acknowledge()) <span class=\"comment\">//将当前record标记为已处理</span></span><br><span class=\"line\">    .map(ReceiverRecord::value) <span class=\"comment\">//将元素由ReceiverRecord对象替换为其value</span></span><br><span class=\"line\">    .doOnNext(r -&gt; handler.saveResource(r)) <span class=\"comment\">//处理获得的record的value</span></span><br><span class=\"line\">    .doOnError(e -&gt; log.warn(<span class=\"string\">\"消费出错\"</span>, e)) <span class=\"comment\">//处理过程中出错则打印日志</span></span><br><span class=\"line\">    .subscribe(); <span class=\"comment\">//启动订阅</span></span><br></pre></td></tr></table></figure>\n<p>在该调用链上设置多个回调函数，对发布的消息进行顺序消费；</p>\n<p>该方法是非阻塞的，调用完后立即返回，不会阻塞用户线程，而是由reactor-kafka启动异步的EventLoop进行消息的获取并由worker线程调用用户设置的回调函数进行消费；</p>\n<p>如果<code>KafkaReceiver</code>接收到某个topic的一条消息则会顺序地调用回调函数进行处理；</p>\n<p>需要注意的是调用链最后都要调用<code>subscribe()</code>方法启动订阅，否则整个调用链并不会生效，并且一个<code>Flux</code>或者<code>Mono</code>对象只能订阅一次，如果多次订阅的话会报错：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.IllegalStateException: Multiple subscribers are not supported <span class=\"keyword\">for</span> KafkaReceiver flux</span><br></pre></td></tr></table></figure>\n<p>如果不想马上结束整个流处理过程的话，可以不用立即调用<code>subscribe()</code>方法，而是将<code>Flux</code>对象作为一个载体传下去，这也就是官网提到的<code>use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline</code>的意义所在，比如，我想将消费得到的String存入mongodb，则可以当前<code>Flux</code>对象传给<code>reactor-mongodb</code>，再由<code>reactor-mongodb</code>返回一个<code>Flux</code>对象，形成完整的Stream链：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 从kafka消费得到String数据</span></span><br><span class=\"line\">Flux&lt;String&gt; receivedStrFlux = KafkaReceiver</span><br><span class=\"line\">    .create(receiverOptions())</span><br><span class=\"line\">    .receive()</span><br><span class=\"line\">    .log()</span><br><span class=\"line\">    .doOnNext(r -&gt; r.receiverOffset().acknowledge())</span><br><span class=\"line\">    .map(ReceiverRecord::value)</span><br><span class=\"line\">    .doOnError(e -&gt; log.warn(<span class=\"string\">\"消费出错\"</span>, e));</span><br><span class=\"line\"><span class=\"comment\">// 将String经过一系列转换得到一个包含Resource对象的Flux</span></span><br><span class=\"line\">Flux&lt;Resource&gt; resourceFlux = receivedStrFlux</span><br><span class=\"line\">\t.map(<span class=\"keyword\">this</span>::handleResourceString)</span><br><span class=\"line\">    .flatMap(Flux::fromIterable)</span><br><span class=\"line\">    .map(resourceData -&gt; &#123;</span><br><span class=\"line\">        Resource resource = <span class=\"keyword\">new</span> Resource();</span><br><span class=\"line\">        BeanUtils.copyProperties(resourceData, resource);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> resource;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"><span class=\"comment\">// 将包含Resource对象的Flux通过reactor-mongodb进行存储</span></span><br><span class=\"line\"><span class=\"comment\">// 得到另一个Flux&lt;Resource&gt;</span></span><br><span class=\"line\">Flux&lt;Resource&gt; resourceSavedFlux = resourceRepository.saveAll(resourceFlux);</span><br><span class=\"line\"><span class=\"comment\">// 对这个Flux&lt;Resource&gt;进行订阅</span></span><br><span class=\"line\">resourceSavedFlux</span><br><span class=\"line\">\t.doOnError(e -&gt; log.warn(<span class=\"string\">\"出错啦！\"</span>, e))</span><br><span class=\"line\">    .doOnComplete(() -&gt; log.info(<span class=\"string\">\"都存完啦！\"</span>))</span><br><span class=\"line\">    .log()</span><br><span class=\"line\">    .subscribe();</span><br></pre></td></tr></table></figure>\n<p>以上就是一个完整的reactor-kafka+reactor-mongodb的异步响应式流处理链</p>\n<h4 id=\"reactor-consumer-api其他配置\"><a href=\"#reactor-consumer-api其他配置\" class=\"headerlink\" title=\"reactor consumer api其他配置\"></a>reactor consumer api其他配置</h4><p>除了基本的配置，reactor consumer api还可以进行一些进一步的设置</p>\n<p>单独订阅序号为0的partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">receiverOptions = receiverOptions.assignment(Collections.singleton(<span class=\"keyword\">new</span> TopicPartition(topic, <span class=\"number\">0</span>)));</span><br></pre></td></tr></table></figure>\n<p>当调用<code>.doOnNext(r -&gt; r.receiverOffset().acknowledge())</code>时，该record的offset并不会立即提交，而是加入一个等待提交队列进行周期性自动提交，当然也可以调用<code>r.receiverOffset().commit()</code>方法手动提交该offset，<code>commit()</code>后依然返回的是一个<code>Mono</code>对象：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.doOnNext(r -&gt; r.receiverOffset().commit().doOnSuccess(aVoid -&gt; log.info(<span class=\"string\">\"offset提交成功！\"</span>)).subscribe())</span><br></pre></td></tr></table></figure>\n<p><code>KafkaReceiver</code>接收kafka信息除了使用<code>receive()</code>消费最新的record，还可以手动指定<code>offset</code>进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions.&lt;String, String&gt;create(consumerProps())\t.addAssignListener(receiverPartitions -&gt; receiverPartitions.forEach(ReceiverPartition::seekToBeginning))\t\t.subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p>使用<code>seekToBeginning</code>在每次初始化后从头开始消费，或者直接指定offset：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions.&lt;String, String&gt;create(consumerProps())\t.addAssignListener(receiverPartitions -&gt; receiverPartitions.forEach(r -&gt; r.seek(<span class=\"number\">140</span>))).subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p>从offset=140的位置开始消费</p>\n<h4 id=\"reactor-kafka-consumer的生命周期\"><a href=\"#reactor-kafka-consumer的生命周期\" class=\"headerlink\" title=\"reactor-kafka-consumer的生命周期\"></a>reactor-kafka-consumer的生命周期</h4><p>每个<code>KafkaReceiver</code>实例的生命周期都跟对应的<code>Flux</code>相关，<code>Flux</code>结束消费则相应的<code>KafkaReceiver</code>就会被关闭.</p>\n","site":{"data":{}},"excerpt":"<p>reactor-kafka项目是遵循响应式流（Reactive Streams）规范的Kafka client，有Producer实现和Consumer实现。<br>","more":"</p>\n<h3 id=\"首先讲讲Reactive-Streams规范\"><a href=\"#首先讲讲Reactive-Streams规范\" class=\"headerlink\" title=\"首先讲讲Reactive Streams规范\"></a>首先讲讲Reactive Streams规范</h3><p>在传统的编程范式中，我们一般通过迭代器（Iterator）模式来遍历一个序列。这种遍历方式是由调用者来控制节奏的，采用的是拉的方式。每次由调用者通过 next()方法来获取序列中的下一个值。</p>\n<p>响应式流（Reactive Streams）规范则是推的方式，即常见的发布者-订阅者模式。当发布者有新的数据产生时，这些数据会被推送到订阅者来进行处理。在反应式流上可以添加各种不同的操作来对数据进行处理，形成数据处理链。这个以声明式的方式添加的处理链只在订阅者进行订阅操作时才会真正执行。</p>\n<p>响应式流规范体现到Jdk中即为Java 8的Stream Api和Java 9的Flow Api，再结合Java 8的Lambda函数式编程模型，形成了独特的Reactive响应式异步编程模型，目前最重要的Reactive实现项目即为Pivatol维护的<code>project-reactor</code>，诸多项目基于<code>project-reactor</code>对原有项目进行了遵循响应式规范的重构，包括<code>WebFlux</code>和<code>Reactor-mongodb</code>，以及这里要介绍的<code>Reactor-Kafka</code>。</p>\n<h4 id=\"Flux和Mono\"><a href=\"#Flux和Mono\" class=\"headerlink\" title=\"Flux和Mono\"></a>Flux和Mono</h4><p>Flux和Mono是project-reactor中最重要的两个基本概念，可以把他们理解为发布订阅模型中的发布者，他们均实现了<code>org.reactivestreams.Publisher</code>接口，Mono表示的是包含 0 到 1 个发布者的异步序列，Flux 表示的是包含 0 到 N 个发布者的异步序列。</p>\n<p>在Flux和Mono发布订阅序列中可以包含三种不同类型的消息通知：正常的包含元素的消息、序列结束的消息和序列出错的消息，分别包含<code>onNext()</code>, <code>onComplete()</code>和 <code>onError()</code>三个回调；当发布者产生需要消费的元素时，用户使用Stream流处理的方式对序列上的元素进行处理，比如<code>map()</code>或者<code>filter()</code>等等；最终调用<code>onSubscribe()</code>完成订阅。</p>\n<p>如下图所示，通过Flux（或者Mono）将整个数据库调用链以响应式异步序列的方式贯通起来，再由reactive的web服务器（通常是reactive-netty）发布给用户：</p>\n<p><img src=\"/blog/images/1548985987329.png\" alt=\"1548985987329\"></p>\n<p>整个过程均为非阻塞，吞吐量能得到极大提升。</p>\n<h3 id=\"Reactor-Kafka\"><a href=\"#Reactor-Kafka\" class=\"headerlink\" title=\"Reactor-Kafka\"></a>Reactor-Kafka</h3><p>关于<code>Reactor-Kafka</code>，官方文档描述如下：</p>\n<blockquote>\n<p><a href=\"https://projectreactor.io/docs/kafka/release/api/index.html\" target=\"_blank\" rel=\"noopener\">Reactor Kafka</a> is a reactive API for Kafka based on Reactor and the Kafka Producer/Consumer API. Reactor Kafka API enables messages to be published to Kafka and consumed from Kafka using functional APIs with non-blocking back-pressure and very low overheads. This enables applications using Reactor to use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline.</p>\n</blockquote>\n<blockquote>\n<p>Reactor-Kafka是基于Reactor和kafka Producer/Consumer API开发的kafka响应式API客户端，可以通过非阻塞的函数式API来发布消息到kafka并消费kafka的消息，性能消耗很低。这套API能够让那些使用Reactor标准库（即Flux和Mono）的应用程序像message bus或者streaming platform一样使用kafka，并且和其他系统集成，提供端到端的响应式管道。</p>\n</blockquote>\n<p>也就是说这套Reactor-Kafka的API能够让使用者很轻易地将Kafka与其他使用Reactor api的系统集成起来，形成一套完整的响应式流处理管路。</p>\n<h4 id=\"加入java依赖\"><a href=\"#加入java依赖\" class=\"headerlink\" title=\"加入java依赖\"></a>加入java依赖</h4><p>maven：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>io.projectreactor.kafka<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>reactor-kafka<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.1.0.RELEASE<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>gradle:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    compile <span class=\"string\">\"io.projectreactor.kafka:reactor-kafka:1.1.0.RELEASE\"</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"reactor-consumer-api\"><a href=\"#reactor-consumer-api\" class=\"headerlink\" title=\"reactor consumer api\"></a>reactor consumer api</h4><p>Reactor-Kafka的consumer的核心api是<code>reactor.kafka.receiver.KafkaReceiver</code></p>\n<p><strong>属性配置</strong>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; consumerProps = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br></pre></td></tr></table></figure>\n<p>对基本的bootstrapServers和groupId以及serializer等基本属性进行配置，其他的consumer属性参考Kafka官方文档；创建一个<code>ReceiverOptions</code>对象对属性进行封装：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions&lt;String, String&gt; receiverOptions = ReceiverOptions.&lt;String, String&gt;create(consumerProps())</span><br><span class=\"line\">    .subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p><code>.subscription</code>方法可以订阅多个topic</p>\n<p><strong>KafkaReceiver</strong>：</p>\n<p><code>KafkaReceiver</code>是reactor-kafka的consumer核心api：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Flux&lt;ReceiverRecord&lt;String, String&gt;&gt; recordFlux = KafkaReceiver.create(receiverOptions()).receive();</span><br></pre></td></tr></table></figure>\n<p><code>FLux</code>对象用于多个发布者，也就是说在<code>KafkaReceiver</code>中对应多个topic进行消费，其中key和value均设置为<code>String</code>类型：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">recordFlux</span><br><span class=\"line\">\t.log()\t<span class=\"comment\">//打印日志</span></span><br><span class=\"line\">    .doOnNext(r -&gt; r.receiverOffset().acknowledge()) <span class=\"comment\">//将当前record标记为已处理</span></span><br><span class=\"line\">    .map(ReceiverRecord::value) <span class=\"comment\">//将元素由ReceiverRecord对象替换为其value</span></span><br><span class=\"line\">    .doOnNext(r -&gt; handler.saveResource(r)) <span class=\"comment\">//处理获得的record的value</span></span><br><span class=\"line\">    .doOnError(e -&gt; log.warn(<span class=\"string\">\"消费出错\"</span>, e)) <span class=\"comment\">//处理过程中出错则打印日志</span></span><br><span class=\"line\">    .subscribe(); <span class=\"comment\">//启动订阅</span></span><br></pre></td></tr></table></figure>\n<p>在该调用链上设置多个回调函数，对发布的消息进行顺序消费；</p>\n<p>该方法是非阻塞的，调用完后立即返回，不会阻塞用户线程，而是由reactor-kafka启动异步的EventLoop进行消息的获取并由worker线程调用用户设置的回调函数进行消费；</p>\n<p>如果<code>KafkaReceiver</code>接收到某个topic的一条消息则会顺序地调用回调函数进行处理；</p>\n<p>需要注意的是调用链最后都要调用<code>subscribe()</code>方法启动订阅，否则整个调用链并不会生效，并且一个<code>Flux</code>或者<code>Mono</code>对象只能订阅一次，如果多次订阅的话会报错：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.IllegalStateException: Multiple subscribers are not supported <span class=\"keyword\">for</span> KafkaReceiver flux</span><br></pre></td></tr></table></figure>\n<p>如果不想马上结束整个流处理过程的话，可以不用立即调用<code>subscribe()</code>方法，而是将<code>Flux</code>对象作为一个载体传下去，这也就是官网提到的<code>use Kafka as a message bus or streaming platform and integrate with other systems to provide an end-to-end reactive pipeline</code>的意义所在，比如，我想将消费得到的String存入mongodb，则可以当前<code>Flux</code>对象传给<code>reactor-mongodb</code>，再由<code>reactor-mongodb</code>返回一个<code>Flux</code>对象，形成完整的Stream链：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 从kafka消费得到String数据</span></span><br><span class=\"line\">Flux&lt;String&gt; receivedStrFlux = KafkaReceiver</span><br><span class=\"line\">    .create(receiverOptions())</span><br><span class=\"line\">    .receive()</span><br><span class=\"line\">    .log()</span><br><span class=\"line\">    .doOnNext(r -&gt; r.receiverOffset().acknowledge())</span><br><span class=\"line\">    .map(ReceiverRecord::value)</span><br><span class=\"line\">    .doOnError(e -&gt; log.warn(<span class=\"string\">\"消费出错\"</span>, e));</span><br><span class=\"line\"><span class=\"comment\">// 将String经过一系列转换得到一个包含Resource对象的Flux</span></span><br><span class=\"line\">Flux&lt;Resource&gt; resourceFlux = receivedStrFlux</span><br><span class=\"line\">\t.map(<span class=\"keyword\">this</span>::handleResourceString)</span><br><span class=\"line\">    .flatMap(Flux::fromIterable)</span><br><span class=\"line\">    .map(resourceData -&gt; &#123;</span><br><span class=\"line\">        Resource resource = <span class=\"keyword\">new</span> Resource();</span><br><span class=\"line\">        BeanUtils.copyProperties(resourceData, resource);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> resource;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"><span class=\"comment\">// 将包含Resource对象的Flux通过reactor-mongodb进行存储</span></span><br><span class=\"line\"><span class=\"comment\">// 得到另一个Flux&lt;Resource&gt;</span></span><br><span class=\"line\">Flux&lt;Resource&gt; resourceSavedFlux = resourceRepository.saveAll(resourceFlux);</span><br><span class=\"line\"><span class=\"comment\">// 对这个Flux&lt;Resource&gt;进行订阅</span></span><br><span class=\"line\">resourceSavedFlux</span><br><span class=\"line\">\t.doOnError(e -&gt; log.warn(<span class=\"string\">\"出错啦！\"</span>, e))</span><br><span class=\"line\">    .doOnComplete(() -&gt; log.info(<span class=\"string\">\"都存完啦！\"</span>))</span><br><span class=\"line\">    .log()</span><br><span class=\"line\">    .subscribe();</span><br></pre></td></tr></table></figure>\n<p>以上就是一个完整的reactor-kafka+reactor-mongodb的异步响应式流处理链</p>\n<h4 id=\"reactor-consumer-api其他配置\"><a href=\"#reactor-consumer-api其他配置\" class=\"headerlink\" title=\"reactor consumer api其他配置\"></a>reactor consumer api其他配置</h4><p>除了基本的配置，reactor consumer api还可以进行一些进一步的设置</p>\n<p>单独订阅序号为0的partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">receiverOptions = receiverOptions.assignment(Collections.singleton(<span class=\"keyword\">new</span> TopicPartition(topic, <span class=\"number\">0</span>)));</span><br></pre></td></tr></table></figure>\n<p>当调用<code>.doOnNext(r -&gt; r.receiverOffset().acknowledge())</code>时，该record的offset并不会立即提交，而是加入一个等待提交队列进行周期性自动提交，当然也可以调用<code>r.receiverOffset().commit()</code>方法手动提交该offset，<code>commit()</code>后依然返回的是一个<code>Mono</code>对象：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.doOnNext(r -&gt; r.receiverOffset().commit().doOnSuccess(aVoid -&gt; log.info(<span class=\"string\">\"offset提交成功！\"</span>)).subscribe())</span><br></pre></td></tr></table></figure>\n<p><code>KafkaReceiver</code>接收kafka信息除了使用<code>receive()</code>消费最新的record，还可以手动指定<code>offset</code>进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions.&lt;String, String&gt;create(consumerProps())\t.addAssignListener(receiverPartitions -&gt; receiverPartitions.forEach(ReceiverPartition::seekToBeginning))\t\t.subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p>使用<code>seekToBeginning</code>在每次初始化后从头开始消费，或者直接指定offset：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ReceiverOptions.&lt;String, String&gt;create(consumerProps())\t.addAssignListener(receiverPartitions -&gt; receiverPartitions.forEach(r -&gt; r.seek(<span class=\"number\">140</span>))).subscription(Collections.singleton(<span class=\"string\">\"resource-v1-TestProduct-TestType\"</span>));</span><br></pre></td></tr></table></figure>\n<p>从offset=140的位置开始消费</p>\n<h4 id=\"reactor-kafka-consumer的生命周期\"><a href=\"#reactor-kafka-consumer的生命周期\" class=\"headerlink\" title=\"reactor-kafka-consumer的生命周期\"></a>reactor-kafka-consumer的生命周期</h4><p>每个<code>KafkaReceiver</code>实例的生命周期都跟对应的<code>Flux</code>相关，<code>Flux</code>结束消费则相应的<code>KafkaReceiver</code>就会被关闭.</p>"},{"title":"java线程池源码分析 --- submit()的过程","author":"天渊hyominnLover","date":"2019-01-23T05:49:00.000Z","_content":"在jdk线程池中，`submit()`是`ExecutorService`的基础api，用于提交新任务给线程池进行执行，现对`submit()`执行过程一探究竟，这里主要对`AbstractExecutorService`及其子类`ThreadPoolExecutor`的实现进行讨论。\n<!-- more -->\n### submit()\n\njdk 8中，submit()有三个重载，分别是：\n\n```JAVA\n<T> Future<T> submit(Callable<T> task);\n<T> Future<T> submit(Runnable task, T result);\nFuture<?> submit(Runnable task);\n```\n\n三者大同小异，最终都会返回`Future`对象来获取异步执行结果，即便传进来的是Runnable对象，也会包装为Callable进行执行，下面仅探讨第一个重载，源码如下：\n\n```JAVA\npublic <T> Future<T> submit(Callable<T> task) {\n    if (task == null) throw new NullPointerException();\n    RunnableFuture<T> ftask = newTaskFor(task);\n    execute(ftask);\n    return ftask;\n}\n```\n\n任务提交进来后，调用`newTaskFor`方法构建了一个`RunnableFuture`对象，最终会调用`execute`方法提交这个RunnableFuture，其实`RunnableFuture`是一个同时继承了`Runnable`和`Future`的接口，同时具有这两者的功能，在`submit()`中构建的是其实现类：`FutureTask`：\n\n```JAVA\npublic FutureTask(Callable<V> callable) {\n    if (callable == null)\n        throw new NullPointerException();\n    this.callable = callable;\n    this.state = NEW;       // ensure visibility of callable\n}\n```\n\n这个对象是线程池的主要操作对象，是客户提交的任务的执行载体，其中封装了客户提交的callable (Runnable)任务\n\n任务提交进来后会统一交给`execute()`方法进行执行，这个方法`AbstractExecutorService`交给了子类去实现\n\n### execute()\n\n在`ThreadPoolExecutor`中，源码如下：\n\n```JAVA\npublic void execute(Runnable command) {\n    if (command == null)\n        throw new NullPointerException();\n    // 获取ctl值，ctl对同时对线程池的两个状态进行控制：\n    // 1. 当前线程池状态 2. 存活的工作线程总数（即worker数）\n    int c = ctl.get();\n    // 通过ctl获取当前工作线程数目\n    if (workerCountOf(c) < corePoolSize) {\n        // 如果小于核心线程数则增加worker，增加并提交任务成功则直接返回\n        // 本次使用核心线程数来判断worker能否增加成功\n        if (addWorker(command, true))\n            return;\n        // 增加失败，继续获取当前ctl值\n        c = ctl.get();\n    }\n    // 检查当前线程池状态是否为RUNNING，并向workQueue缓存当前任务\n    if (isRunning(c) && workQueue.offer(command)) {\n        int recheck = ctl.get();\n        // 如果当前状态不为RUNNING，尝试从workQueue移除本次任务\n        // 移除成功后执行拒绝策略\n        if (! isRunning(recheck) && remove(command))\n            reject(command);\n        // 如果当前存活worker总数为0则继续尝试增加worker\n        else if (workerCountOf(recheck) == 0)\n            // 第二个参数为false，说明本次使用最大线程数来判断worker能否增加成功\n            addWorker(null, false);\n    }\n    // 若当前状态不为RUNNING或者向workQueue缓存当前任务失败，则尝试增加worker\n    // 若增加worker失败（通常为已达到最大线程数）\n    else if (!addWorker(command, false))\n        reject(command);\n}\n```\n\n整个execute()的过程很复杂，涉及到线程池中各组件比较复杂的交互过程，参考官方注释的说法，整个过程分为三步：\n\n1. 如果worker数量少于**核心线程数**，则尝试增加worker并把当前任务作为新worker的firstTask并执行\n2. 如果以上路线走不通，则尝试向`workqueue`缓存任务，待空闲的worker取任务，在这个过程中对ctl进行双重检查，防止ctl出现不一致（因为以上过程中并没有做同步处理），如果ctl状态不为RUNNING则将刚才的任务弹出workqueue并执行拒绝策略；若成功缓存任务后，且当前worker数为0，则尝试继续增加worker，用**最大线程数**来判断worker能否增加成功\n3. 如果第2步走不通（比如状态非RUNNING或者缓存任务失败），尝试继续增加worker，用**最大线程数**来判断worker能否增加成功，如果这一步都走不通，那直接进行拒绝策略，整个过程结束\n\n可以看出，整个过程非常依赖`addWorker`这个方法，主要用于新建worker并且提交firstTask，该方法执行成功与否直接关系到整个流程的走向，以下情况会导致增加worker失败：\n\n`状态为Stop、Tidying或者Terminate`\n\n`状态为Shutdown，提交任务为null并且workqueue为空`\n\n`达到核心线程数或者最大线程数，或最大容量限制(2的29次方减1)`\n\n`创建新worker时出现其他异常`\n\n### addWorker\n\n`addWorker`是整个任务提交过程中最重要的方法，以下是源码：\n\n```JAVA\nprivate boolean addWorker(Runnable firstTask, boolean core) {\n    retry:\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n        // 以下条件判断能否增加worker\n        if (rs >= SHUTDOWN &&\n            ! (rs == SHUTDOWN &&\n               firstTask == null &&\n               ! workQueue.isEmpty()))\n            return false;\n\t\t// 内嵌循环，\n        // 每一次循环都要重新判断worker数目，worker达到数量限制则直接返回false\n        for (;;) {\n            int wc = workerCountOf(c);\n            if (wc >= CAPACITY ||\n                wc >= (core ? corePoolSize : maximumPoolSize))\n                return false;\n            // cas方式增加worker数目，成功后直接退出外层循环\n            if (compareAndIncrementWorkerCount(c))\n                break retry;\n            c = ctl.get();  // Re-read ctl\n            // 若内嵌循环过程中状态改变，则推出内嵌循环开始外层循环\n            if (runStateOf(c) != rs)\n                continue retry;\n            // 如果仅仅是因为worker数目改变导致cas失败，则仅进行内嵌循环\n            // 不需要进行外层循环重新获取ctl状态\n        }\n    }\n\n    boolean workerStarted = false;\n    boolean workerAdded = false;\n    Worker w = null;\n    try {\n        // 新建worker对象并将任务作为其firstTask\n        w = new Worker(firstTask);\n        final Thread t = w.thread;\n        if (t != null) {\n            // 同步操作\n            final ReentrantLock mainLock = this.mainLock;\n            mainLock.lock();\n            try {\n                // 重新获取ctl状态\n                int rs = runStateOf(ctl.get());\n\t\t\t\t// 仅当状态为RUNNING或者为SHUTDOWN时提交的任务是null，才继续执行\n                if (rs < SHUTDOWN ||\n                    (rs == SHUTDOWN && firstTask == null)) {\n                    // 若该worker线程已经启动则抛出异常\n                    if (t.isAlive())\n                        throw new IllegalThreadStateException();\n                    workers.add(w);\n                    int s = workers.size();\n                    // 增加largestPoolSize，仅作为统计用处\n                    if (s > largestPoolSize)\n                        largestPoolSize = s;\n                    workerAdded = true;\n                }\n            } finally {\n                mainLock.unlock();\n            }\n            if (workerAdded) {\n                // 若增加worker成功则启动其线程，执行的是Worker对象的run方法\n                t.start();\n                workerStarted = true;\n            }\n        }\n    } finally {\n        if (! workerStarted)\n            addWorkerFailed(w);\n    }\n    return workerStarted;\n}\n```\n\n该方法第一部分的for循环略微有些绕，总的说来就是对线程池状态有可能随时变化作出的双重保障，内层循环服务于增加worker数目的cas操作，外层循环在此基础上加上了对ctl状态的重新获取及判断。\n\n可以看出，整个submit过程离不开对线程池ctl状态的多次核查，保证了线程池的顺利运行，接下来对worker启动后做的工作进行简要分析\n\n### Worker\n\nWorker类继承了Runnable以及`AQS(AbstractQueuedSynchronizer)`，在他的run方法中调用了线程池对象的`runWorker`方法：\n\n```JAVA\nfinal void runWorker(Worker w) {\n    Thread wt = Thread.currentThread();\n    // 这个task变量很重要，是worker本次执行中的主要执行对象\n    // 首先将worker的firstTask赋值给他\n    // 赋值完后将worker的firstTask置为null\n    Runnable task = w.firstTask;\n    w.firstTask = null;\n    w.unlock(); // allow interrupts\n    boolean completedAbruptly = true;\n    try {\n        // 进入循环，执行任务，如果任务为null则从workqueue里面取\n        while (task != null || (task = getTask()) != null) {\n            // 对当前worker执行同步\n            w.lock();\n            // 如果当前worker线程未被打断，且状态为STOP及其以上（Tyding或者terminated），\n            // 则将当前worker线程中断\n            if ((runStateAtLeast(ctl.get(), STOP) ||\n                 (Thread.interrupted() &&\n                  runStateAtLeast(ctl.get(), STOP))) &&\n                !wt.isInterrupted())\n                wt.interrupt();\n            try {\n                // 执行前预处理，留给子类定制，通常用来对资源进行初始化，或者打印日志\n                beforeExecute(wt, task);\n                Throwable thrown = null;\n                try {\n                    // 真正执行任务\n                    task.run();\n                } catch (RuntimeException x) {\n                    thrown = x; throw x;\n                } catch (Error x) {\n                    thrown = x; throw x;\n                } catch (Throwable x) {\n                    thrown = x; throw new Error(x);\n                } finally {\n                    // 跟beforeExecute类似，也是执行资源释放或打印错误日志\n                    afterExecute(task, thrown);\n                }\n            } finally {\n                // 将task重置为null\n                task = null;\n                // 统计当前worker的执行任务数目\n                w.completedTasks++;\n                // 释放worker同步\n                w.unlock();\n            }\n        }\n        // 如果推出了该循环，则将completedAbruptly参数置为false\n        completedAbruptly = false;\n    } finally {\n        // 执行worker退出操作\n        processWorkerExit(w, completedAbruptly);\n    }\n}\n```\n\n整个过程大致分为以下几个步骤\n\n1. 初始化，将firstTask作为初始任务\n2. worker执行unlock()，调整AQS状态使其可以被打断\n3. 进入循环，执行任务，如果任务为null则从workqueue里面取：`task = getTask()`\n4. 判断是否需要将当前worker打断，满足条件则interrupt该worker的线程\n5. 执行任务\n6. worker循环执行任务，直到无任务可以执行，则正常退出循环，将completedAbruptly置为false；又或者执行了打断线程操作等原因抛出了异常，属于非正常推出循环，这时候completedAbruptly仍为true\n7. 执行worker退出操作\n\n其中最重要的两项操作分别是`getTask()`和`processWorkerExit(w, completedAbruptly)`，worker会持续尝试从`workQueue`中拿任务，worker拿不到任务或者非正常退出时则会执行退出操作，退出操作也比较重要，直接决定接下来线程池中是否保留以及保留多少个worker，现在对`getTask()`进行分析\n\n### getTask()\n\n源码如下：\n\n```JAVA\nprivate Runnable getTask() {\n    boolean timedOut = false; // 判断poll()获取任务的过程是否超时\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n        // 若状态不为Running，并且workQueue为空或者状态为Stop，表明已经不需要执行任何任务了\n        // 这时会直接减少workerCount并直接返回null，本次getTask提前结束\n        if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) {\n            decrementWorkerCount();\n            return null;\n        }\n        // 重新计算workerCount\n        int wc = workerCountOf(c);\n        // Are workers subject to culling?\n        // 官方注释的意思是，用timer参数标记当前worker是否需要保留，timed为true则不需要保留\n        boolean timed = allowCoreThreadTimeOut || wc > corePoolSize;\n        // 如果workQueue为空或者workerCount大于1，有两种情况当前worker不需要保留：\n        // 1. workerCount已经超出了最大线程数\n        // 2. 获取任务超时并且不需要保留为核心线程\n        if ((wc > maximumPoolSize || (timed && timedOut))\n            && (wc > 1 || workQueue.isEmpty())) {\n            // cas方式减少workerCount，如果cas失败则循环重试\n            if (compareAndDecrementWorkerCount(c))\n                return null;\n            continue;\n        }\n        try {\n            // 从workQueue取任务，根据timed不同又分为两种情况\n            // 1. timed为true，当前worker在keepAliveTime时间内拿不到任务则会被抛弃\n            // 2. timed为false，则当前worker作为核心线程保留下来并尝试拿任务\n            // 由于workQueue是BlockingQueue，所以执行take()拿不到任务的话会阻塞直到队列中有任务可用\n            // take()和poll()的过程都是可以被interrupt的\n            Runnable r = timed ?\n                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :\n            workQueue.take();\n            // 如果拿到任务后会返回，拿不到任务则将timedOut标记为true\n            if (r != null)\n                return r;\n            // 拿不到任务，说明poll()超时了\n            timedOut = true;\n        } catch (InterruptedException retry) {\n            // 说明拿任务的过程被interrupt了，将timedOut标记为false\n            // 表明并不是因为poll()超时而获取不了任务\n            timedOut = false;\n        }\n    }\n}\n```\n\n`getTask()`成功与否直接关系到该worker是否会被抛弃，其中，`timed`这个boolean变量对worker是否需要保留为核心线程进行标记，还涉及到`allowCoreThreadTimeOut`这个属性，分为两种情况：\n\n`allowCoreThreadTimeOut为false`：默认情况，线程池种会保留`corePoolSize`数量的线程作为核心线程，从上述代码种可以看出，只要当前workerCount不大于corePoolSize，那该worker就可以作为核心线程保留下来，取任务时调用`workQueue.take()`，持续阻塞直到有任务可以执行\n\n`allowCoreThreadTimeOut为true`：需要手动调用`allowCoreThreadTimeOut(boolean value)`方法进行设置，这种情况下线程池不会保留核心线程，所有worker在取任务时均会调用`workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)`方法，在`keepAliveTime`时间后若还未取到任务则会被抛弃\n\n以下几种情况会导致getTask方法返回null，即该worker无任务可以执行，将被抛弃：\n\n1. 线程池状态不为Running，并且workQueue为空\n2. 线程池状态为Stop\n3. 线程池状态为Running，workQueue为空，并且workerCount已经超出了最大线程数\n4. 线程池状态为Running，workQueue为空，获取任务超时并且当前worker不需要保留为核心线程\n\n整个流程走下来，以上4种情况下该worker会被抛弃，进行下面的退出操作`processWorkerExit`，这种情况worker均为正常退出，`completedAbruptly`为false\n\n### processWorkerExit\n\nprocessWorkerExit源码如下：\n\n```JAVA\nprivate void processWorkerExit(Worker w, boolean completedAbruptly) {\n    // 如果worker是非正常退出任务执行循环，则减少workerCount\n    // 若是正常退出，则worker在getTask获取任务失败退出后已经减少了workerCount，可以正常移除该worker了\n    if (completedAbruptly)\n        decrementWorkerCount();\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        completedTaskCount += w.completedTasks;\n        // 移除worker\n        workers.remove(w);\n    } finally {\n        mainLock.unlock();\n    }\n    // 尝试执行终止操作\n    tryTerminate();\n    int c = ctl.get();\n    // 如果当前状态为Running或者Shutdown，则执行以下流程\n    if (runStateLessThan(c, STOP)) {\n        // 若worker为正常退出任务执行循环，则需要额外判断是否需要新增worker\n        // 分两种情况：\n        // 1. 若需要将核心线程在一定闲置时间后被移除，则当前worker最多保留一个\n        // 2. 如果不需要将核心线程闲置一段时间后移除，则可以保留不超过核心线程数的worker\n        if (!completedAbruptly) {\n            int min = allowCoreThreadTimeOut ? 0 : corePoolSize;\n            if (min == 0 && ! workQueue.isEmpty())\n                min = 1;\n            // 如果worker已经够用了就不用addWorker了\n            if (workerCountOf(c) >= min)\n                return; // replacement not needed\n        }\n        // 执行以上判断后依然需要增加worker的话就调用addWorker，不传入任何task\n        addWorker(null, false);\n    }\n}\n```\n\n逻辑相对比较复杂，总结来说，根据`completedAbruptly`参数将退出操作分为两条路线：\n\n`正常退出任务执行循环：`\n\n1. 不需要减少worker数目\n2. 将当前worker移除，并尝试执行终止操作\n3. 如果当前状态为`Running`或者`Shutdown`，表示如果`workQueue`里面还有任务要执行的话，是需要继续执行的，那么接下来尝试新增worker\n4. 计算当前需要保留的worker数目（min变量），如果`workerCount`已经满足需求则不额外增加worker了（这里依然使用`allowCoreThreadTimeOut`判断是否保留一定数量的核心线程，如果为true，则worker最多保留一个），直接退出\n5. 如果`workerCount`数目不满足需求，则新增一个worker然后让他去`workQueue`里面取任务执行\n\n`非正常退出任务执行循环`\n\n1. 减少worker数目\n2. 移除当前worker并尝试执行终止操作，如果当前状态为`Running`或者`Shutdown`，则直接新增worker\n\n至于为什么将worker退出操作分为正常和非正常，我是这么理解的：\n\n`正常退出`：说明worker调用`getTask()`没有成功取到任务，将被抛弃，`getTask`方法已经对`workCount`进行了扣减，这里就不需要对`workerCount`作任何变动，此外需要判断当前`workerCount`数目够不够\n\n`非正常退出`：这种情况下需要对`workerCount`进行扣减并立即补充一个worker，当然如果当前状态为`Stop`或者`Tyding`甚至`Terminated`的话就没必要补充了","source":"_posts/java线程池源码分析--submit-的过程.md","raw":"title: java线程池源码分析 --- submit()的过程\nauthor: 天渊hyominnLover\ntags:\n\n  - Java\n  - 多线程\n  - Java并发包\ncategories: [基础知识]\ndate: 2019-01-23 13:49:00\n---\n在jdk线程池中，`submit()`是`ExecutorService`的基础api，用于提交新任务给线程池进行执行，现对`submit()`执行过程一探究竟，这里主要对`AbstractExecutorService`及其子类`ThreadPoolExecutor`的实现进行讨论。\n<!-- more -->\n### submit()\n\njdk 8中，submit()有三个重载，分别是：\n\n```JAVA\n<T> Future<T> submit(Callable<T> task);\n<T> Future<T> submit(Runnable task, T result);\nFuture<?> submit(Runnable task);\n```\n\n三者大同小异，最终都会返回`Future`对象来获取异步执行结果，即便传进来的是Runnable对象，也会包装为Callable进行执行，下面仅探讨第一个重载，源码如下：\n\n```JAVA\npublic <T> Future<T> submit(Callable<T> task) {\n    if (task == null) throw new NullPointerException();\n    RunnableFuture<T> ftask = newTaskFor(task);\n    execute(ftask);\n    return ftask;\n}\n```\n\n任务提交进来后，调用`newTaskFor`方法构建了一个`RunnableFuture`对象，最终会调用`execute`方法提交这个RunnableFuture，其实`RunnableFuture`是一个同时继承了`Runnable`和`Future`的接口，同时具有这两者的功能，在`submit()`中构建的是其实现类：`FutureTask`：\n\n```JAVA\npublic FutureTask(Callable<V> callable) {\n    if (callable == null)\n        throw new NullPointerException();\n    this.callable = callable;\n    this.state = NEW;       // ensure visibility of callable\n}\n```\n\n这个对象是线程池的主要操作对象，是客户提交的任务的执行载体，其中封装了客户提交的callable (Runnable)任务\n\n任务提交进来后会统一交给`execute()`方法进行执行，这个方法`AbstractExecutorService`交给了子类去实现\n\n### execute()\n\n在`ThreadPoolExecutor`中，源码如下：\n\n```JAVA\npublic void execute(Runnable command) {\n    if (command == null)\n        throw new NullPointerException();\n    // 获取ctl值，ctl对同时对线程池的两个状态进行控制：\n    // 1. 当前线程池状态 2. 存活的工作线程总数（即worker数）\n    int c = ctl.get();\n    // 通过ctl获取当前工作线程数目\n    if (workerCountOf(c) < corePoolSize) {\n        // 如果小于核心线程数则增加worker，增加并提交任务成功则直接返回\n        // 本次使用核心线程数来判断worker能否增加成功\n        if (addWorker(command, true))\n            return;\n        // 增加失败，继续获取当前ctl值\n        c = ctl.get();\n    }\n    // 检查当前线程池状态是否为RUNNING，并向workQueue缓存当前任务\n    if (isRunning(c) && workQueue.offer(command)) {\n        int recheck = ctl.get();\n        // 如果当前状态不为RUNNING，尝试从workQueue移除本次任务\n        // 移除成功后执行拒绝策略\n        if (! isRunning(recheck) && remove(command))\n            reject(command);\n        // 如果当前存活worker总数为0则继续尝试增加worker\n        else if (workerCountOf(recheck) == 0)\n            // 第二个参数为false，说明本次使用最大线程数来判断worker能否增加成功\n            addWorker(null, false);\n    }\n    // 若当前状态不为RUNNING或者向workQueue缓存当前任务失败，则尝试增加worker\n    // 若增加worker失败（通常为已达到最大线程数）\n    else if (!addWorker(command, false))\n        reject(command);\n}\n```\n\n整个execute()的过程很复杂，涉及到线程池中各组件比较复杂的交互过程，参考官方注释的说法，整个过程分为三步：\n\n1. 如果worker数量少于**核心线程数**，则尝试增加worker并把当前任务作为新worker的firstTask并执行\n2. 如果以上路线走不通，则尝试向`workqueue`缓存任务，待空闲的worker取任务，在这个过程中对ctl进行双重检查，防止ctl出现不一致（因为以上过程中并没有做同步处理），如果ctl状态不为RUNNING则将刚才的任务弹出workqueue并执行拒绝策略；若成功缓存任务后，且当前worker数为0，则尝试继续增加worker，用**最大线程数**来判断worker能否增加成功\n3. 如果第2步走不通（比如状态非RUNNING或者缓存任务失败），尝试继续增加worker，用**最大线程数**来判断worker能否增加成功，如果这一步都走不通，那直接进行拒绝策略，整个过程结束\n\n可以看出，整个过程非常依赖`addWorker`这个方法，主要用于新建worker并且提交firstTask，该方法执行成功与否直接关系到整个流程的走向，以下情况会导致增加worker失败：\n\n`状态为Stop、Tidying或者Terminate`\n\n`状态为Shutdown，提交任务为null并且workqueue为空`\n\n`达到核心线程数或者最大线程数，或最大容量限制(2的29次方减1)`\n\n`创建新worker时出现其他异常`\n\n### addWorker\n\n`addWorker`是整个任务提交过程中最重要的方法，以下是源码：\n\n```JAVA\nprivate boolean addWorker(Runnable firstTask, boolean core) {\n    retry:\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n        // 以下条件判断能否增加worker\n        if (rs >= SHUTDOWN &&\n            ! (rs == SHUTDOWN &&\n               firstTask == null &&\n               ! workQueue.isEmpty()))\n            return false;\n\t\t// 内嵌循环，\n        // 每一次循环都要重新判断worker数目，worker达到数量限制则直接返回false\n        for (;;) {\n            int wc = workerCountOf(c);\n            if (wc >= CAPACITY ||\n                wc >= (core ? corePoolSize : maximumPoolSize))\n                return false;\n            // cas方式增加worker数目，成功后直接退出外层循环\n            if (compareAndIncrementWorkerCount(c))\n                break retry;\n            c = ctl.get();  // Re-read ctl\n            // 若内嵌循环过程中状态改变，则推出内嵌循环开始外层循环\n            if (runStateOf(c) != rs)\n                continue retry;\n            // 如果仅仅是因为worker数目改变导致cas失败，则仅进行内嵌循环\n            // 不需要进行外层循环重新获取ctl状态\n        }\n    }\n\n    boolean workerStarted = false;\n    boolean workerAdded = false;\n    Worker w = null;\n    try {\n        // 新建worker对象并将任务作为其firstTask\n        w = new Worker(firstTask);\n        final Thread t = w.thread;\n        if (t != null) {\n            // 同步操作\n            final ReentrantLock mainLock = this.mainLock;\n            mainLock.lock();\n            try {\n                // 重新获取ctl状态\n                int rs = runStateOf(ctl.get());\n\t\t\t\t// 仅当状态为RUNNING或者为SHUTDOWN时提交的任务是null，才继续执行\n                if (rs < SHUTDOWN ||\n                    (rs == SHUTDOWN && firstTask == null)) {\n                    // 若该worker线程已经启动则抛出异常\n                    if (t.isAlive())\n                        throw new IllegalThreadStateException();\n                    workers.add(w);\n                    int s = workers.size();\n                    // 增加largestPoolSize，仅作为统计用处\n                    if (s > largestPoolSize)\n                        largestPoolSize = s;\n                    workerAdded = true;\n                }\n            } finally {\n                mainLock.unlock();\n            }\n            if (workerAdded) {\n                // 若增加worker成功则启动其线程，执行的是Worker对象的run方法\n                t.start();\n                workerStarted = true;\n            }\n        }\n    } finally {\n        if (! workerStarted)\n            addWorkerFailed(w);\n    }\n    return workerStarted;\n}\n```\n\n该方法第一部分的for循环略微有些绕，总的说来就是对线程池状态有可能随时变化作出的双重保障，内层循环服务于增加worker数目的cas操作，外层循环在此基础上加上了对ctl状态的重新获取及判断。\n\n可以看出，整个submit过程离不开对线程池ctl状态的多次核查，保证了线程池的顺利运行，接下来对worker启动后做的工作进行简要分析\n\n### Worker\n\nWorker类继承了Runnable以及`AQS(AbstractQueuedSynchronizer)`，在他的run方法中调用了线程池对象的`runWorker`方法：\n\n```JAVA\nfinal void runWorker(Worker w) {\n    Thread wt = Thread.currentThread();\n    // 这个task变量很重要，是worker本次执行中的主要执行对象\n    // 首先将worker的firstTask赋值给他\n    // 赋值完后将worker的firstTask置为null\n    Runnable task = w.firstTask;\n    w.firstTask = null;\n    w.unlock(); // allow interrupts\n    boolean completedAbruptly = true;\n    try {\n        // 进入循环，执行任务，如果任务为null则从workqueue里面取\n        while (task != null || (task = getTask()) != null) {\n            // 对当前worker执行同步\n            w.lock();\n            // 如果当前worker线程未被打断，且状态为STOP及其以上（Tyding或者terminated），\n            // 则将当前worker线程中断\n            if ((runStateAtLeast(ctl.get(), STOP) ||\n                 (Thread.interrupted() &&\n                  runStateAtLeast(ctl.get(), STOP))) &&\n                !wt.isInterrupted())\n                wt.interrupt();\n            try {\n                // 执行前预处理，留给子类定制，通常用来对资源进行初始化，或者打印日志\n                beforeExecute(wt, task);\n                Throwable thrown = null;\n                try {\n                    // 真正执行任务\n                    task.run();\n                } catch (RuntimeException x) {\n                    thrown = x; throw x;\n                } catch (Error x) {\n                    thrown = x; throw x;\n                } catch (Throwable x) {\n                    thrown = x; throw new Error(x);\n                } finally {\n                    // 跟beforeExecute类似，也是执行资源释放或打印错误日志\n                    afterExecute(task, thrown);\n                }\n            } finally {\n                // 将task重置为null\n                task = null;\n                // 统计当前worker的执行任务数目\n                w.completedTasks++;\n                // 释放worker同步\n                w.unlock();\n            }\n        }\n        // 如果推出了该循环，则将completedAbruptly参数置为false\n        completedAbruptly = false;\n    } finally {\n        // 执行worker退出操作\n        processWorkerExit(w, completedAbruptly);\n    }\n}\n```\n\n整个过程大致分为以下几个步骤\n\n1. 初始化，将firstTask作为初始任务\n2. worker执行unlock()，调整AQS状态使其可以被打断\n3. 进入循环，执行任务，如果任务为null则从workqueue里面取：`task = getTask()`\n4. 判断是否需要将当前worker打断，满足条件则interrupt该worker的线程\n5. 执行任务\n6. worker循环执行任务，直到无任务可以执行，则正常退出循环，将completedAbruptly置为false；又或者执行了打断线程操作等原因抛出了异常，属于非正常推出循环，这时候completedAbruptly仍为true\n7. 执行worker退出操作\n\n其中最重要的两项操作分别是`getTask()`和`processWorkerExit(w, completedAbruptly)`，worker会持续尝试从`workQueue`中拿任务，worker拿不到任务或者非正常退出时则会执行退出操作，退出操作也比较重要，直接决定接下来线程池中是否保留以及保留多少个worker，现在对`getTask()`进行分析\n\n### getTask()\n\n源码如下：\n\n```JAVA\nprivate Runnable getTask() {\n    boolean timedOut = false; // 判断poll()获取任务的过程是否超时\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n        // 若状态不为Running，并且workQueue为空或者状态为Stop，表明已经不需要执行任何任务了\n        // 这时会直接减少workerCount并直接返回null，本次getTask提前结束\n        if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) {\n            decrementWorkerCount();\n            return null;\n        }\n        // 重新计算workerCount\n        int wc = workerCountOf(c);\n        // Are workers subject to culling?\n        // 官方注释的意思是，用timer参数标记当前worker是否需要保留，timed为true则不需要保留\n        boolean timed = allowCoreThreadTimeOut || wc > corePoolSize;\n        // 如果workQueue为空或者workerCount大于1，有两种情况当前worker不需要保留：\n        // 1. workerCount已经超出了最大线程数\n        // 2. 获取任务超时并且不需要保留为核心线程\n        if ((wc > maximumPoolSize || (timed && timedOut))\n            && (wc > 1 || workQueue.isEmpty())) {\n            // cas方式减少workerCount，如果cas失败则循环重试\n            if (compareAndDecrementWorkerCount(c))\n                return null;\n            continue;\n        }\n        try {\n            // 从workQueue取任务，根据timed不同又分为两种情况\n            // 1. timed为true，当前worker在keepAliveTime时间内拿不到任务则会被抛弃\n            // 2. timed为false，则当前worker作为核心线程保留下来并尝试拿任务\n            // 由于workQueue是BlockingQueue，所以执行take()拿不到任务的话会阻塞直到队列中有任务可用\n            // take()和poll()的过程都是可以被interrupt的\n            Runnable r = timed ?\n                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :\n            workQueue.take();\n            // 如果拿到任务后会返回，拿不到任务则将timedOut标记为true\n            if (r != null)\n                return r;\n            // 拿不到任务，说明poll()超时了\n            timedOut = true;\n        } catch (InterruptedException retry) {\n            // 说明拿任务的过程被interrupt了，将timedOut标记为false\n            // 表明并不是因为poll()超时而获取不了任务\n            timedOut = false;\n        }\n    }\n}\n```\n\n`getTask()`成功与否直接关系到该worker是否会被抛弃，其中，`timed`这个boolean变量对worker是否需要保留为核心线程进行标记，还涉及到`allowCoreThreadTimeOut`这个属性，分为两种情况：\n\n`allowCoreThreadTimeOut为false`：默认情况，线程池种会保留`corePoolSize`数量的线程作为核心线程，从上述代码种可以看出，只要当前workerCount不大于corePoolSize，那该worker就可以作为核心线程保留下来，取任务时调用`workQueue.take()`，持续阻塞直到有任务可以执行\n\n`allowCoreThreadTimeOut为true`：需要手动调用`allowCoreThreadTimeOut(boolean value)`方法进行设置，这种情况下线程池不会保留核心线程，所有worker在取任务时均会调用`workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)`方法，在`keepAliveTime`时间后若还未取到任务则会被抛弃\n\n以下几种情况会导致getTask方法返回null，即该worker无任务可以执行，将被抛弃：\n\n1. 线程池状态不为Running，并且workQueue为空\n2. 线程池状态为Stop\n3. 线程池状态为Running，workQueue为空，并且workerCount已经超出了最大线程数\n4. 线程池状态为Running，workQueue为空，获取任务超时并且当前worker不需要保留为核心线程\n\n整个流程走下来，以上4种情况下该worker会被抛弃，进行下面的退出操作`processWorkerExit`，这种情况worker均为正常退出，`completedAbruptly`为false\n\n### processWorkerExit\n\nprocessWorkerExit源码如下：\n\n```JAVA\nprivate void processWorkerExit(Worker w, boolean completedAbruptly) {\n    // 如果worker是非正常退出任务执行循环，则减少workerCount\n    // 若是正常退出，则worker在getTask获取任务失败退出后已经减少了workerCount，可以正常移除该worker了\n    if (completedAbruptly)\n        decrementWorkerCount();\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        completedTaskCount += w.completedTasks;\n        // 移除worker\n        workers.remove(w);\n    } finally {\n        mainLock.unlock();\n    }\n    // 尝试执行终止操作\n    tryTerminate();\n    int c = ctl.get();\n    // 如果当前状态为Running或者Shutdown，则执行以下流程\n    if (runStateLessThan(c, STOP)) {\n        // 若worker为正常退出任务执行循环，则需要额外判断是否需要新增worker\n        // 分两种情况：\n        // 1. 若需要将核心线程在一定闲置时间后被移除，则当前worker最多保留一个\n        // 2. 如果不需要将核心线程闲置一段时间后移除，则可以保留不超过核心线程数的worker\n        if (!completedAbruptly) {\n            int min = allowCoreThreadTimeOut ? 0 : corePoolSize;\n            if (min == 0 && ! workQueue.isEmpty())\n                min = 1;\n            // 如果worker已经够用了就不用addWorker了\n            if (workerCountOf(c) >= min)\n                return; // replacement not needed\n        }\n        // 执行以上判断后依然需要增加worker的话就调用addWorker，不传入任何task\n        addWorker(null, false);\n    }\n}\n```\n\n逻辑相对比较复杂，总结来说，根据`completedAbruptly`参数将退出操作分为两条路线：\n\n`正常退出任务执行循环：`\n\n1. 不需要减少worker数目\n2. 将当前worker移除，并尝试执行终止操作\n3. 如果当前状态为`Running`或者`Shutdown`，表示如果`workQueue`里面还有任务要执行的话，是需要继续执行的，那么接下来尝试新增worker\n4. 计算当前需要保留的worker数目（min变量），如果`workerCount`已经满足需求则不额外增加worker了（这里依然使用`allowCoreThreadTimeOut`判断是否保留一定数量的核心线程，如果为true，则worker最多保留一个），直接退出\n5. 如果`workerCount`数目不满足需求，则新增一个worker然后让他去`workQueue`里面取任务执行\n\n`非正常退出任务执行循环`\n\n1. 减少worker数目\n2. 移除当前worker并尝试执行终止操作，如果当前状态为`Running`或者`Shutdown`，则直接新增worker\n\n至于为什么将worker退出操作分为正常和非正常，我是这么理解的：\n\n`正常退出`：说明worker调用`getTask()`没有成功取到任务，将被抛弃，`getTask`方法已经对`workCount`进行了扣减，这里就不需要对`workerCount`作任何变动，此外需要判断当前`workerCount`数目够不够\n\n`非正常退出`：这种情况下需要对`workerCount`进行扣减并立即补充一个worker，当然如果当前状态为`Stop`或者`Tyding`甚至`Terminated`的话就没必要补充了","slug":"java线程池源码分析--submit-的过程","published":1,"updated":"2019-03-19T13:30:58.444Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3t000yg0qr4wmehvlk","content":"<p>在jdk线程池中，<code>submit()</code>是<code>ExecutorService</code>的基础api，用于提交新任务给线程池进行执行，现对<code>submit()</code>执行过程一探究竟，这里主要对<code>AbstractExecutorService</code>及其子类<code>ThreadPoolExecutor</code>的实现进行讨论。<br><a id=\"more\"></a></p>\n<h3 id=\"submit\"><a href=\"#submit\" class=\"headerlink\" title=\"submit()\"></a>submit()</h3><p>jdk 8中，submit()有三个重载，分别是：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Callable&lt;T&gt; task)</span></span>;</span><br><span class=\"line\">&lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Runnable task, T result)</span></span>;</span><br><span class=\"line\">Future&lt;?&gt; submit(Runnable task);</span><br></pre></td></tr></table></figure>\n<p>三者大同小异，最终都会返回<code>Future</code>对象来获取异步执行结果，即便传进来的是Runnable对象，也会包装为Callable进行执行，下面仅探讨第一个重载，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> &lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Callable&lt;T&gt; task)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (task == <span class=\"keyword\">null</span>) <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    RunnableFuture&lt;T&gt; ftask = newTaskFor(task);</span><br><span class=\"line\">    execute(ftask);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ftask;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>任务提交进来后，调用<code>newTaskFor</code>方法构建了一个<code>RunnableFuture</code>对象，最终会调用<code>execute</code>方法提交这个RunnableFuture，其实<code>RunnableFuture</code>是一个同时继承了<code>Runnable</code>和<code>Future</code>的接口，同时具有这两者的功能，在<code>submit()</code>中构建的是其实现类：<code>FutureTask</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">FutureTask</span><span class=\"params\">(Callable&lt;V&gt; callable)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (callable == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.callable = callable;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.state = NEW;       <span class=\"comment\">// ensure visibility of callable</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个对象是线程池的主要操作对象，是客户提交的任务的执行载体，其中封装了客户提交的callable (Runnable)任务</p>\n<p>任务提交进来后会统一交给<code>execute()</code>方法进行执行，这个方法<code>AbstractExecutorService</code>交给了子类去实现</p>\n<h3 id=\"execute\"><a href=\"#execute\" class=\"headerlink\" title=\"execute()\"></a>execute()</h3><p>在<code>ThreadPoolExecutor</code>中，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Runnable command)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (command == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"comment\">// 获取ctl值，ctl对同时对线程池的两个状态进行控制：</span></span><br><span class=\"line\">    <span class=\"comment\">// 1. 当前线程池状态 2. 存活的工作线程总数（即worker数）</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">    <span class=\"comment\">// 通过ctl获取当前工作线程数目</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (workerCountOf(c) &lt; corePoolSize) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果小于核心线程数则增加worker，增加并提交任务成功则直接返回</span></span><br><span class=\"line\">        <span class=\"comment\">// 本次使用核心线程数来判断worker能否增加成功</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (addWorker(command, <span class=\"keyword\">true</span>))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 增加失败，继续获取当前ctl值</span></span><br><span class=\"line\">        c = ctl.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 检查当前线程池状态是否为RUNNING，并向workQueue缓存当前任务</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> recheck = ctl.get();</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前状态不为RUNNING，尝试从workQueue移除本次任务</span></span><br><span class=\"line\">        <span class=\"comment\">// 移除成功后执行拒绝策略</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (! isRunning(recheck) &amp;&amp; remove(command))</span><br><span class=\"line\">            reject(command);</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前存活worker总数为0则继续尝试增加worker</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (workerCountOf(recheck) == <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"comment\">// 第二个参数为false，说明本次使用最大线程数来判断worker能否增加成功</span></span><br><span class=\"line\">            addWorker(<span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 若当前状态不为RUNNING或者向workQueue缓存当前任务失败，则尝试增加worker</span></span><br><span class=\"line\">    <span class=\"comment\">// 若增加worker失败（通常为已达到最大线程数）</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!addWorker(command, <span class=\"keyword\">false</span>))</span><br><span class=\"line\">        reject(command);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个execute()的过程很复杂，涉及到线程池中各组件比较复杂的交互过程，参考官方注释的说法，整个过程分为三步：</p>\n<ol>\n<li>如果worker数量少于<strong>核心线程数</strong>，则尝试增加worker并把当前任务作为新worker的firstTask并执行</li>\n<li>如果以上路线走不通，则尝试向<code>workqueue</code>缓存任务，待空闲的worker取任务，在这个过程中对ctl进行双重检查，防止ctl出现不一致（因为以上过程中并没有做同步处理），如果ctl状态不为RUNNING则将刚才的任务弹出workqueue并执行拒绝策略；若成功缓存任务后，且当前worker数为0，则尝试继续增加worker，用<strong>最大线程数</strong>来判断worker能否增加成功</li>\n<li>如果第2步走不通（比如状态非RUNNING或者缓存任务失败），尝试继续增加worker，用<strong>最大线程数</strong>来判断worker能否增加成功，如果这一步都走不通，那直接进行拒绝策略，整个过程结束</li>\n</ol>\n<p>可以看出，整个过程非常依赖<code>addWorker</code>这个方法，主要用于新建worker并且提交firstTask，该方法执行成功与否直接关系到整个流程的走向，以下情况会导致增加worker失败：</p>\n<p><code>状态为Stop、Tidying或者Terminate</code></p>\n<p><code>状态为Shutdown，提交任务为null并且workqueue为空</code></p>\n<p><code>达到核心线程数或者最大线程数，或最大容量限制(2的29次方减1)</code></p>\n<p><code>创建新worker时出现其他异常</code></p>\n<h3 id=\"addWorker\"><a href=\"#addWorker\" class=\"headerlink\" title=\"addWorker\"></a>addWorker</h3><p><code>addWorker</code>是整个任务提交过程中最重要的方法，以下是源码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">addWorker</span><span class=\"params\">(Runnable firstTask, <span class=\"keyword\">boolean</span> core)</span> </span>&#123;</span><br><span class=\"line\">    retry:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> rs = runStateOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// 以下条件判断能否增加worker</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (rs &gt;= SHUTDOWN &amp;&amp;</span><br><span class=\"line\">            ! (rs == SHUTDOWN &amp;&amp;</span><br><span class=\"line\">               firstTask == <span class=\"keyword\">null</span> &amp;&amp;</span><br><span class=\"line\">               ! workQueue.isEmpty()))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 内嵌循环，</span></span><br><span class=\"line\">        <span class=\"comment\">// 每一次循环都要重新判断worker数目，worker达到数量限制则直接返回false</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> wc = workerCountOf(c);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (wc &gt;= CAPACITY ||</span><br><span class=\"line\">                wc &gt;= (core ? corePoolSize : maximumPoolSize))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">            <span class=\"comment\">// cas方式增加worker数目，成功后直接退出外层循环</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndIncrementWorkerCount(c))</span><br><span class=\"line\">                <span class=\"keyword\">break</span> retry;</span><br><span class=\"line\">            c = ctl.get();  <span class=\"comment\">// Re-read ctl</span></span><br><span class=\"line\">            <span class=\"comment\">// 若内嵌循环过程中状态改变，则推出内嵌循环开始外层循环</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (runStateOf(c) != rs)</span><br><span class=\"line\">                <span class=\"keyword\">continue</span> retry;</span><br><span class=\"line\">            <span class=\"comment\">// 如果仅仅是因为worker数目改变导致cas失败，则仅进行内嵌循环</span></span><br><span class=\"line\">            <span class=\"comment\">// 不需要进行外层循环重新获取ctl状态</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> workerStarted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> workerAdded = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    Worker w = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 新建worker对象并将任务作为其firstTask</span></span><br><span class=\"line\">        w = <span class=\"keyword\">new</span> Worker(firstTask);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Thread t = w.thread;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 同步操作</span></span><br><span class=\"line\">            <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">            mainLock.lock();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 重新获取ctl状态</span></span><br><span class=\"line\">                <span class=\"keyword\">int</span> rs = runStateOf(ctl.get());</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 仅当状态为RUNNING或者为SHUTDOWN时提交的任务是null，才继续执行</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (rs &lt; SHUTDOWN ||</span><br><span class=\"line\">                    (rs == SHUTDOWN &amp;&amp; firstTask == <span class=\"keyword\">null</span>)) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 若该worker线程已经启动则抛出异常</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (t.isAlive())</span><br><span class=\"line\">                        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalThreadStateException();</span><br><span class=\"line\">                    workers.add(w);</span><br><span class=\"line\">                    <span class=\"keyword\">int</span> s = workers.size();</span><br><span class=\"line\">                    <span class=\"comment\">// 增加largestPoolSize，仅作为统计用处</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (s &gt; largestPoolSize)</span><br><span class=\"line\">                        largestPoolSize = s;</span><br><span class=\"line\">                    workerAdded = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                mainLock.unlock();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (workerAdded) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 若增加worker成功则启动其线程，执行的是Worker对象的run方法</span></span><br><span class=\"line\">                t.start();</span><br><span class=\"line\">                workerStarted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (! workerStarted)</span><br><span class=\"line\">            addWorkerFailed(w);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> workerStarted;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该方法第一部分的for循环略微有些绕，总的说来就是对线程池状态有可能随时变化作出的双重保障，内层循环服务于增加worker数目的cas操作，外层循环在此基础上加上了对ctl状态的重新获取及判断。</p>\n<p>可以看出，整个submit过程离不开对线程池ctl状态的多次核查，保证了线程池的顺利运行，接下来对worker启动后做的工作进行简要分析</p>\n<h3 id=\"Worker\"><a href=\"#Worker\" class=\"headerlink\" title=\"Worker\"></a>Worker</h3><p>Worker类继承了Runnable以及<code>AQS(AbstractQueuedSynchronizer)</code>，在他的run方法中调用了线程池对象的<code>runWorker</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">runWorker</span><span class=\"params\">(Worker w)</span> </span>&#123;</span><br><span class=\"line\">    Thread wt = Thread.currentThread();</span><br><span class=\"line\">    <span class=\"comment\">// 这个task变量很重要，是worker本次执行中的主要执行对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 首先将worker的firstTask赋值给他</span></span><br><span class=\"line\">    <span class=\"comment\">// 赋值完后将worker的firstTask置为null</span></span><br><span class=\"line\">    Runnable task = w.firstTask;</span><br><span class=\"line\">    w.firstTask = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    w.unlock(); <span class=\"comment\">// allow interrupts</span></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> completedAbruptly = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 进入循环，执行任务，如果任务为null则从workqueue里面取</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (task != <span class=\"keyword\">null</span> || (task = getTask()) != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 对当前worker执行同步</span></span><br><span class=\"line\">            w.lock();</span><br><span class=\"line\">            <span class=\"comment\">// 如果当前worker线程未被打断，且状态为STOP及其以上（Tyding或者terminated），</span></span><br><span class=\"line\">            <span class=\"comment\">// 则将当前worker线程中断</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class=\"line\">                 (Thread.interrupted() &amp;&amp;</span><br><span class=\"line\">                  runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class=\"line\">                !wt.isInterrupted())</span><br><span class=\"line\">                wt.interrupt();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 执行前预处理，留给子类定制，通常用来对资源进行初始化，或者打印日志</span></span><br><span class=\"line\">                beforeExecute(wt, task);</span><br><span class=\"line\">                Throwable thrown = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 真正执行任务</span></span><br><span class=\"line\">                    task.run();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (RuntimeException x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> x;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Error x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> x;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Throwable x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(x);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 跟beforeExecute类似，也是执行资源释放或打印错误日志</span></span><br><span class=\"line\">                    afterExecute(task, thrown);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 将task重置为null</span></span><br><span class=\"line\">                task = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"comment\">// 统计当前worker的执行任务数目</span></span><br><span class=\"line\">                w.completedTasks++;</span><br><span class=\"line\">                <span class=\"comment\">// 释放worker同步</span></span><br><span class=\"line\">                w.unlock();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 如果推出了该循环，则将completedAbruptly参数置为false</span></span><br><span class=\"line\">        completedAbruptly = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 执行worker退出操作</span></span><br><span class=\"line\">        processWorkerExit(w, completedAbruptly);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个过程大致分为以下几个步骤</p>\n<ol>\n<li>初始化，将firstTask作为初始任务</li>\n<li>worker执行unlock()，调整AQS状态使其可以被打断</li>\n<li>进入循环，执行任务，如果任务为null则从workqueue里面取：<code>task = getTask()</code></li>\n<li>判断是否需要将当前worker打断，满足条件则interrupt该worker的线程</li>\n<li>执行任务</li>\n<li>worker循环执行任务，直到无任务可以执行，则正常退出循环，将completedAbruptly置为false；又或者执行了打断线程操作等原因抛出了异常，属于非正常推出循环，这时候completedAbruptly仍为true</li>\n<li>执行worker退出操作</li>\n</ol>\n<p>其中最重要的两项操作分别是<code>getTask()</code>和<code>processWorkerExit(w, completedAbruptly)</code>，worker会持续尝试从<code>workQueue</code>中拿任务，worker拿不到任务或者非正常退出时则会执行退出操作，退出操作也比较重要，直接决定接下来线程池中是否保留以及保留多少个worker，现在对<code>getTask()</code>进行分析</p>\n<h3 id=\"getTask\"><a href=\"#getTask\" class=\"headerlink\" title=\"getTask()\"></a>getTask()</h3><p>源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Runnable <span class=\"title\">getTask</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> timedOut = <span class=\"keyword\">false</span>; <span class=\"comment\">// 判断poll()获取任务的过程是否超时</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> rs = runStateOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// 若状态不为Running，并且workQueue为空或者状态为Stop，表明已经不需要执行任何任务了</span></span><br><span class=\"line\">        <span class=\"comment\">// 这时会直接减少workerCount并直接返回null，本次getTask提前结束</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123;</span><br><span class=\"line\">            decrementWorkerCount();</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 重新计算workerCount</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> wc = workerCountOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// Are workers subject to culling?</span></span><br><span class=\"line\">        <span class=\"comment\">// 官方注释的意思是，用timer参数标记当前worker是否需要保留，timed为true则不需要保留</span></span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;</span><br><span class=\"line\">        <span class=\"comment\">// 如果workQueue为空或者workerCount大于1，有两种情况当前worker不需要保留：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1. workerCount已经超出了最大线程数</span></span><br><span class=\"line\">        <span class=\"comment\">// 2. 获取任务超时并且不需要保留为核心线程</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut))</span><br><span class=\"line\">            &amp;&amp; (wc &gt; <span class=\"number\">1</span> || workQueue.isEmpty())) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// cas方式减少workerCount，如果cas失败则循环重试</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndDecrementWorkerCount(c))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 从workQueue取任务，根据timed不同又分为两种情况</span></span><br><span class=\"line\">            <span class=\"comment\">// 1. timed为true，当前worker在keepAliveTime时间内拿不到任务则会被抛弃</span></span><br><span class=\"line\">            <span class=\"comment\">// 2. timed为false，则当前worker作为核心线程保留下来并尝试拿任务</span></span><br><span class=\"line\">            <span class=\"comment\">// 由于workQueue是BlockingQueue，所以执行take()拿不到任务的话会阻塞直到队列中有任务可用</span></span><br><span class=\"line\">            <span class=\"comment\">// take()和poll()的过程都是可以被interrupt的</span></span><br><span class=\"line\">            Runnable r = timed ?</span><br><span class=\"line\">                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :</span><br><span class=\"line\">            workQueue.take();</span><br><span class=\"line\">            <span class=\"comment\">// 如果拿到任务后会返回，拿不到任务则将timedOut标记为true</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (r != <span class=\"keyword\">null</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> r;</span><br><span class=\"line\">            <span class=\"comment\">// 拿不到任务，说明poll()超时了</span></span><br><span class=\"line\">            timedOut = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (InterruptedException retry) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 说明拿任务的过程被interrupt了，将timedOut标记为false</span></span><br><span class=\"line\">            <span class=\"comment\">// 表明并不是因为poll()超时而获取不了任务</span></span><br><span class=\"line\">            timedOut = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>getTask()</code>成功与否直接关系到该worker是否会被抛弃，其中，<code>timed</code>这个boolean变量对worker是否需要保留为核心线程进行标记，还涉及到<code>allowCoreThreadTimeOut</code>这个属性，分为两种情况：</p>\n<p><code>allowCoreThreadTimeOut为false</code>：默认情况，线程池种会保留<code>corePoolSize</code>数量的线程作为核心线程，从上述代码种可以看出，只要当前workerCount不大于corePoolSize，那该worker就可以作为核心线程保留下来，取任务时调用<code>workQueue.take()</code>，持续阻塞直到有任务可以执行</p>\n<p><code>allowCoreThreadTimeOut为true</code>：需要手动调用<code>allowCoreThreadTimeOut(boolean value)</code>方法进行设置，这种情况下线程池不会保留核心线程，所有worker在取任务时均会调用<code>workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)</code>方法，在<code>keepAliveTime</code>时间后若还未取到任务则会被抛弃</p>\n<p>以下几种情况会导致getTask方法返回null，即该worker无任务可以执行，将被抛弃：</p>\n<ol>\n<li>线程池状态不为Running，并且workQueue为空</li>\n<li>线程池状态为Stop</li>\n<li>线程池状态为Running，workQueue为空，并且workerCount已经超出了最大线程数</li>\n<li>线程池状态为Running，workQueue为空，获取任务超时并且当前worker不需要保留为核心线程</li>\n</ol>\n<p>整个流程走下来，以上4种情况下该worker会被抛弃，进行下面的退出操作<code>processWorkerExit</code>，这种情况worker均为正常退出，<code>completedAbruptly</code>为false</p>\n<h3 id=\"processWorkerExit\"><a href=\"#processWorkerExit\" class=\"headerlink\" title=\"processWorkerExit\"></a>processWorkerExit</h3><p>processWorkerExit源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">processWorkerExit</span><span class=\"params\">(Worker w, <span class=\"keyword\">boolean</span> completedAbruptly)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 如果worker是非正常退出任务执行循环，则减少workerCount</span></span><br><span class=\"line\">    <span class=\"comment\">// 若是正常退出，则worker在getTask获取任务失败退出后已经减少了workerCount，可以正常移除该worker了</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (completedAbruptly)</span><br><span class=\"line\">        decrementWorkerCount();</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        completedTaskCount += w.completedTasks;</span><br><span class=\"line\">        <span class=\"comment\">// 移除worker</span></span><br><span class=\"line\">        workers.remove(w);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 尝试执行终止操作</span></span><br><span class=\"line\">    tryTerminate();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">    <span class=\"comment\">// 如果当前状态为Running或者Shutdown，则执行以下流程</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (runStateLessThan(c, STOP)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 若worker为正常退出任务执行循环，则需要额外判断是否需要新增worker</span></span><br><span class=\"line\">        <span class=\"comment\">// 分两种情况：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1. 若需要将核心线程在一定闲置时间后被移除，则当前worker最多保留一个</span></span><br><span class=\"line\">        <span class=\"comment\">// 2. 如果不需要将核心线程闲置一段时间后移除，则可以保留不超过核心线程数的worker</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!completedAbruptly) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> min = allowCoreThreadTimeOut ? <span class=\"number\">0</span> : corePoolSize;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (min == <span class=\"number\">0</span> &amp;&amp; ! workQueue.isEmpty())</span><br><span class=\"line\">                min = <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"comment\">// 如果worker已经够用了就不用addWorker了</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (workerCountOf(c) &gt;= min)</span><br><span class=\"line\">                <span class=\"keyword\">return</span>; <span class=\"comment\">// replacement not needed</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 执行以上判断后依然需要增加worker的话就调用addWorker，不传入任何task</span></span><br><span class=\"line\">        addWorker(<span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>逻辑相对比较复杂，总结来说，根据<code>completedAbruptly</code>参数将退出操作分为两条路线：</p>\n<p><code>正常退出任务执行循环：</code></p>\n<ol>\n<li>不需要减少worker数目</li>\n<li>将当前worker移除，并尝试执行终止操作</li>\n<li>如果当前状态为<code>Running</code>或者<code>Shutdown</code>，表示如果<code>workQueue</code>里面还有任务要执行的话，是需要继续执行的，那么接下来尝试新增worker</li>\n<li>计算当前需要保留的worker数目（min变量），如果<code>workerCount</code>已经满足需求则不额外增加worker了（这里依然使用<code>allowCoreThreadTimeOut</code>判断是否保留一定数量的核心线程，如果为true，则worker最多保留一个），直接退出</li>\n<li>如果<code>workerCount</code>数目不满足需求，则新增一个worker然后让他去<code>workQueue</code>里面取任务执行</li>\n</ol>\n<p><code>非正常退出任务执行循环</code></p>\n<ol>\n<li>减少worker数目</li>\n<li>移除当前worker并尝试执行终止操作，如果当前状态为<code>Running</code>或者<code>Shutdown</code>，则直接新增worker</li>\n</ol>\n<p>至于为什么将worker退出操作分为正常和非正常，我是这么理解的：</p>\n<p><code>正常退出</code>：说明worker调用<code>getTask()</code>没有成功取到任务，将被抛弃，<code>getTask</code>方法已经对<code>workCount</code>进行了扣减，这里就不需要对<code>workerCount</code>作任何变动，此外需要判断当前<code>workerCount</code>数目够不够</p>\n<p><code>非正常退出</code>：这种情况下需要对<code>workerCount</code>进行扣减并立即补充一个worker，当然如果当前状态为<code>Stop</code>或者<code>Tyding</code>甚至<code>Terminated</code>的话就没必要补充了</p>\n","site":{"data":{}},"excerpt":"<p>在jdk线程池中，<code>submit()</code>是<code>ExecutorService</code>的基础api，用于提交新任务给线程池进行执行，现对<code>submit()</code>执行过程一探究竟，这里主要对<code>AbstractExecutorService</code>及其子类<code>ThreadPoolExecutor</code>的实现进行讨论。<br>","more":"</p>\n<h3 id=\"submit\"><a href=\"#submit\" class=\"headerlink\" title=\"submit()\"></a>submit()</h3><p>jdk 8中，submit()有三个重载，分别是：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Callable&lt;T&gt; task)</span></span>;</span><br><span class=\"line\">&lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Runnable task, T result)</span></span>;</span><br><span class=\"line\">Future&lt;?&gt; submit(Runnable task);</span><br></pre></td></tr></table></figure>\n<p>三者大同小异，最终都会返回<code>Future</code>对象来获取异步执行结果，即便传进来的是Runnable对象，也会包装为Callable进行执行，下面仅探讨第一个重载，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> &lt;T&gt; <span class=\"function\">Future&lt;T&gt; <span class=\"title\">submit</span><span class=\"params\">(Callable&lt;T&gt; task)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (task == <span class=\"keyword\">null</span>) <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    RunnableFuture&lt;T&gt; ftask = newTaskFor(task);</span><br><span class=\"line\">    execute(ftask);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ftask;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>任务提交进来后，调用<code>newTaskFor</code>方法构建了一个<code>RunnableFuture</code>对象，最终会调用<code>execute</code>方法提交这个RunnableFuture，其实<code>RunnableFuture</code>是一个同时继承了<code>Runnable</code>和<code>Future</code>的接口，同时具有这两者的功能，在<code>submit()</code>中构建的是其实现类：<code>FutureTask</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">FutureTask</span><span class=\"params\">(Callable&lt;V&gt; callable)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (callable == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.callable = callable;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.state = NEW;       <span class=\"comment\">// ensure visibility of callable</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个对象是线程池的主要操作对象，是客户提交的任务的执行载体，其中封装了客户提交的callable (Runnable)任务</p>\n<p>任务提交进来后会统一交给<code>execute()</code>方法进行执行，这个方法<code>AbstractExecutorService</code>交给了子类去实现</p>\n<h3 id=\"execute\"><a href=\"#execute\" class=\"headerlink\" title=\"execute()\"></a>execute()</h3><p>在<code>ThreadPoolExecutor</code>中，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Runnable command)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (command == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"comment\">// 获取ctl值，ctl对同时对线程池的两个状态进行控制：</span></span><br><span class=\"line\">    <span class=\"comment\">// 1. 当前线程池状态 2. 存活的工作线程总数（即worker数）</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">    <span class=\"comment\">// 通过ctl获取当前工作线程数目</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (workerCountOf(c) &lt; corePoolSize) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果小于核心线程数则增加worker，增加并提交任务成功则直接返回</span></span><br><span class=\"line\">        <span class=\"comment\">// 本次使用核心线程数来判断worker能否增加成功</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (addWorker(command, <span class=\"keyword\">true</span>))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 增加失败，继续获取当前ctl值</span></span><br><span class=\"line\">        c = ctl.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 检查当前线程池状态是否为RUNNING，并向workQueue缓存当前任务</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> recheck = ctl.get();</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前状态不为RUNNING，尝试从workQueue移除本次任务</span></span><br><span class=\"line\">        <span class=\"comment\">// 移除成功后执行拒绝策略</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (! isRunning(recheck) &amp;&amp; remove(command))</span><br><span class=\"line\">            reject(command);</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前存活worker总数为0则继续尝试增加worker</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (workerCountOf(recheck) == <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"comment\">// 第二个参数为false，说明本次使用最大线程数来判断worker能否增加成功</span></span><br><span class=\"line\">            addWorker(<span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 若当前状态不为RUNNING或者向workQueue缓存当前任务失败，则尝试增加worker</span></span><br><span class=\"line\">    <span class=\"comment\">// 若增加worker失败（通常为已达到最大线程数）</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!addWorker(command, <span class=\"keyword\">false</span>))</span><br><span class=\"line\">        reject(command);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个execute()的过程很复杂，涉及到线程池中各组件比较复杂的交互过程，参考官方注释的说法，整个过程分为三步：</p>\n<ol>\n<li>如果worker数量少于<strong>核心线程数</strong>，则尝试增加worker并把当前任务作为新worker的firstTask并执行</li>\n<li>如果以上路线走不通，则尝试向<code>workqueue</code>缓存任务，待空闲的worker取任务，在这个过程中对ctl进行双重检查，防止ctl出现不一致（因为以上过程中并没有做同步处理），如果ctl状态不为RUNNING则将刚才的任务弹出workqueue并执行拒绝策略；若成功缓存任务后，且当前worker数为0，则尝试继续增加worker，用<strong>最大线程数</strong>来判断worker能否增加成功</li>\n<li>如果第2步走不通（比如状态非RUNNING或者缓存任务失败），尝试继续增加worker，用<strong>最大线程数</strong>来判断worker能否增加成功，如果这一步都走不通，那直接进行拒绝策略，整个过程结束</li>\n</ol>\n<p>可以看出，整个过程非常依赖<code>addWorker</code>这个方法，主要用于新建worker并且提交firstTask，该方法执行成功与否直接关系到整个流程的走向，以下情况会导致增加worker失败：</p>\n<p><code>状态为Stop、Tidying或者Terminate</code></p>\n<p><code>状态为Shutdown，提交任务为null并且workqueue为空</code></p>\n<p><code>达到核心线程数或者最大线程数，或最大容量限制(2的29次方减1)</code></p>\n<p><code>创建新worker时出现其他异常</code></p>\n<h3 id=\"addWorker\"><a href=\"#addWorker\" class=\"headerlink\" title=\"addWorker\"></a>addWorker</h3><p><code>addWorker</code>是整个任务提交过程中最重要的方法，以下是源码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">addWorker</span><span class=\"params\">(Runnable firstTask, <span class=\"keyword\">boolean</span> core)</span> </span>&#123;</span><br><span class=\"line\">    retry:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> rs = runStateOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// 以下条件判断能否增加worker</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (rs &gt;= SHUTDOWN &amp;&amp;</span><br><span class=\"line\">            ! (rs == SHUTDOWN &amp;&amp;</span><br><span class=\"line\">               firstTask == <span class=\"keyword\">null</span> &amp;&amp;</span><br><span class=\"line\">               ! workQueue.isEmpty()))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 内嵌循环，</span></span><br><span class=\"line\">        <span class=\"comment\">// 每一次循环都要重新判断worker数目，worker达到数量限制则直接返回false</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> wc = workerCountOf(c);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (wc &gt;= CAPACITY ||</span><br><span class=\"line\">                wc &gt;= (core ? corePoolSize : maximumPoolSize))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">            <span class=\"comment\">// cas方式增加worker数目，成功后直接退出外层循环</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndIncrementWorkerCount(c))</span><br><span class=\"line\">                <span class=\"keyword\">break</span> retry;</span><br><span class=\"line\">            c = ctl.get();  <span class=\"comment\">// Re-read ctl</span></span><br><span class=\"line\">            <span class=\"comment\">// 若内嵌循环过程中状态改变，则推出内嵌循环开始外层循环</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (runStateOf(c) != rs)</span><br><span class=\"line\">                <span class=\"keyword\">continue</span> retry;</span><br><span class=\"line\">            <span class=\"comment\">// 如果仅仅是因为worker数目改变导致cas失败，则仅进行内嵌循环</span></span><br><span class=\"line\">            <span class=\"comment\">// 不需要进行外层循环重新获取ctl状态</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> workerStarted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> workerAdded = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    Worker w = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 新建worker对象并将任务作为其firstTask</span></span><br><span class=\"line\">        w = <span class=\"keyword\">new</span> Worker(firstTask);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Thread t = w.thread;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 同步操作</span></span><br><span class=\"line\">            <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">            mainLock.lock();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 重新获取ctl状态</span></span><br><span class=\"line\">                <span class=\"keyword\">int</span> rs = runStateOf(ctl.get());</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 仅当状态为RUNNING或者为SHUTDOWN时提交的任务是null，才继续执行</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (rs &lt; SHUTDOWN ||</span><br><span class=\"line\">                    (rs == SHUTDOWN &amp;&amp; firstTask == <span class=\"keyword\">null</span>)) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 若该worker线程已经启动则抛出异常</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (t.isAlive())</span><br><span class=\"line\">                        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalThreadStateException();</span><br><span class=\"line\">                    workers.add(w);</span><br><span class=\"line\">                    <span class=\"keyword\">int</span> s = workers.size();</span><br><span class=\"line\">                    <span class=\"comment\">// 增加largestPoolSize，仅作为统计用处</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (s &gt; largestPoolSize)</span><br><span class=\"line\">                        largestPoolSize = s;</span><br><span class=\"line\">                    workerAdded = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                mainLock.unlock();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (workerAdded) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 若增加worker成功则启动其线程，执行的是Worker对象的run方法</span></span><br><span class=\"line\">                t.start();</span><br><span class=\"line\">                workerStarted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (! workerStarted)</span><br><span class=\"line\">            addWorkerFailed(w);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> workerStarted;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该方法第一部分的for循环略微有些绕，总的说来就是对线程池状态有可能随时变化作出的双重保障，内层循环服务于增加worker数目的cas操作，外层循环在此基础上加上了对ctl状态的重新获取及判断。</p>\n<p>可以看出，整个submit过程离不开对线程池ctl状态的多次核查，保证了线程池的顺利运行，接下来对worker启动后做的工作进行简要分析</p>\n<h3 id=\"Worker\"><a href=\"#Worker\" class=\"headerlink\" title=\"Worker\"></a>Worker</h3><p>Worker类继承了Runnable以及<code>AQS(AbstractQueuedSynchronizer)</code>，在他的run方法中调用了线程池对象的<code>runWorker</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">runWorker</span><span class=\"params\">(Worker w)</span> </span>&#123;</span><br><span class=\"line\">    Thread wt = Thread.currentThread();</span><br><span class=\"line\">    <span class=\"comment\">// 这个task变量很重要，是worker本次执行中的主要执行对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 首先将worker的firstTask赋值给他</span></span><br><span class=\"line\">    <span class=\"comment\">// 赋值完后将worker的firstTask置为null</span></span><br><span class=\"line\">    Runnable task = w.firstTask;</span><br><span class=\"line\">    w.firstTask = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    w.unlock(); <span class=\"comment\">// allow interrupts</span></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> completedAbruptly = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 进入循环，执行任务，如果任务为null则从workqueue里面取</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (task != <span class=\"keyword\">null</span> || (task = getTask()) != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 对当前worker执行同步</span></span><br><span class=\"line\">            w.lock();</span><br><span class=\"line\">            <span class=\"comment\">// 如果当前worker线程未被打断，且状态为STOP及其以上（Tyding或者terminated），</span></span><br><span class=\"line\">            <span class=\"comment\">// 则将当前worker线程中断</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class=\"line\">                 (Thread.interrupted() &amp;&amp;</span><br><span class=\"line\">                  runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class=\"line\">                !wt.isInterrupted())</span><br><span class=\"line\">                wt.interrupt();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 执行前预处理，留给子类定制，通常用来对资源进行初始化，或者打印日志</span></span><br><span class=\"line\">                beforeExecute(wt, task);</span><br><span class=\"line\">                Throwable thrown = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 真正执行任务</span></span><br><span class=\"line\">                    task.run();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (RuntimeException x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> x;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Error x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> x;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Throwable x) &#123;</span><br><span class=\"line\">                    thrown = x; <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(x);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 跟beforeExecute类似，也是执行资源释放或打印错误日志</span></span><br><span class=\"line\">                    afterExecute(task, thrown);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 将task重置为null</span></span><br><span class=\"line\">                task = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"comment\">// 统计当前worker的执行任务数目</span></span><br><span class=\"line\">                w.completedTasks++;</span><br><span class=\"line\">                <span class=\"comment\">// 释放worker同步</span></span><br><span class=\"line\">                w.unlock();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 如果推出了该循环，则将completedAbruptly参数置为false</span></span><br><span class=\"line\">        completedAbruptly = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 执行worker退出操作</span></span><br><span class=\"line\">        processWorkerExit(w, completedAbruptly);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个过程大致分为以下几个步骤</p>\n<ol>\n<li>初始化，将firstTask作为初始任务</li>\n<li>worker执行unlock()，调整AQS状态使其可以被打断</li>\n<li>进入循环，执行任务，如果任务为null则从workqueue里面取：<code>task = getTask()</code></li>\n<li>判断是否需要将当前worker打断，满足条件则interrupt该worker的线程</li>\n<li>执行任务</li>\n<li>worker循环执行任务，直到无任务可以执行，则正常退出循环，将completedAbruptly置为false；又或者执行了打断线程操作等原因抛出了异常，属于非正常推出循环，这时候completedAbruptly仍为true</li>\n<li>执行worker退出操作</li>\n</ol>\n<p>其中最重要的两项操作分别是<code>getTask()</code>和<code>processWorkerExit(w, completedAbruptly)</code>，worker会持续尝试从<code>workQueue</code>中拿任务，worker拿不到任务或者非正常退出时则会执行退出操作，退出操作也比较重要，直接决定接下来线程池中是否保留以及保留多少个worker，现在对<code>getTask()</code>进行分析</p>\n<h3 id=\"getTask\"><a href=\"#getTask\" class=\"headerlink\" title=\"getTask()\"></a>getTask()</h3><p>源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Runnable <span class=\"title\">getTask</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> timedOut = <span class=\"keyword\">false</span>; <span class=\"comment\">// 判断poll()获取任务的过程是否超时</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> rs = runStateOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// 若状态不为Running，并且workQueue为空或者状态为Stop，表明已经不需要执行任何任务了</span></span><br><span class=\"line\">        <span class=\"comment\">// 这时会直接减少workerCount并直接返回null，本次getTask提前结束</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123;</span><br><span class=\"line\">            decrementWorkerCount();</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 重新计算workerCount</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> wc = workerCountOf(c);</span><br><span class=\"line\">        <span class=\"comment\">// Are workers subject to culling?</span></span><br><span class=\"line\">        <span class=\"comment\">// 官方注释的意思是，用timer参数标记当前worker是否需要保留，timed为true则不需要保留</span></span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;</span><br><span class=\"line\">        <span class=\"comment\">// 如果workQueue为空或者workerCount大于1，有两种情况当前worker不需要保留：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1. workerCount已经超出了最大线程数</span></span><br><span class=\"line\">        <span class=\"comment\">// 2. 获取任务超时并且不需要保留为核心线程</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut))</span><br><span class=\"line\">            &amp;&amp; (wc &gt; <span class=\"number\">1</span> || workQueue.isEmpty())) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// cas方式减少workerCount，如果cas失败则循环重试</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndDecrementWorkerCount(c))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 从workQueue取任务，根据timed不同又分为两种情况</span></span><br><span class=\"line\">            <span class=\"comment\">// 1. timed为true，当前worker在keepAliveTime时间内拿不到任务则会被抛弃</span></span><br><span class=\"line\">            <span class=\"comment\">// 2. timed为false，则当前worker作为核心线程保留下来并尝试拿任务</span></span><br><span class=\"line\">            <span class=\"comment\">// 由于workQueue是BlockingQueue，所以执行take()拿不到任务的话会阻塞直到队列中有任务可用</span></span><br><span class=\"line\">            <span class=\"comment\">// take()和poll()的过程都是可以被interrupt的</span></span><br><span class=\"line\">            Runnable r = timed ?</span><br><span class=\"line\">                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :</span><br><span class=\"line\">            workQueue.take();</span><br><span class=\"line\">            <span class=\"comment\">// 如果拿到任务后会返回，拿不到任务则将timedOut标记为true</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (r != <span class=\"keyword\">null</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> r;</span><br><span class=\"line\">            <span class=\"comment\">// 拿不到任务，说明poll()超时了</span></span><br><span class=\"line\">            timedOut = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (InterruptedException retry) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 说明拿任务的过程被interrupt了，将timedOut标记为false</span></span><br><span class=\"line\">            <span class=\"comment\">// 表明并不是因为poll()超时而获取不了任务</span></span><br><span class=\"line\">            timedOut = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>getTask()</code>成功与否直接关系到该worker是否会被抛弃，其中，<code>timed</code>这个boolean变量对worker是否需要保留为核心线程进行标记，还涉及到<code>allowCoreThreadTimeOut</code>这个属性，分为两种情况：</p>\n<p><code>allowCoreThreadTimeOut为false</code>：默认情况，线程池种会保留<code>corePoolSize</code>数量的线程作为核心线程，从上述代码种可以看出，只要当前workerCount不大于corePoolSize，那该worker就可以作为核心线程保留下来，取任务时调用<code>workQueue.take()</code>，持续阻塞直到有任务可以执行</p>\n<p><code>allowCoreThreadTimeOut为true</code>：需要手动调用<code>allowCoreThreadTimeOut(boolean value)</code>方法进行设置，这种情况下线程池不会保留核心线程，所有worker在取任务时均会调用<code>workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)</code>方法，在<code>keepAliveTime</code>时间后若还未取到任务则会被抛弃</p>\n<p>以下几种情况会导致getTask方法返回null，即该worker无任务可以执行，将被抛弃：</p>\n<ol>\n<li>线程池状态不为Running，并且workQueue为空</li>\n<li>线程池状态为Stop</li>\n<li>线程池状态为Running，workQueue为空，并且workerCount已经超出了最大线程数</li>\n<li>线程池状态为Running，workQueue为空，获取任务超时并且当前worker不需要保留为核心线程</li>\n</ol>\n<p>整个流程走下来，以上4种情况下该worker会被抛弃，进行下面的退出操作<code>processWorkerExit</code>，这种情况worker均为正常退出，<code>completedAbruptly</code>为false</p>\n<h3 id=\"processWorkerExit\"><a href=\"#processWorkerExit\" class=\"headerlink\" title=\"processWorkerExit\"></a>processWorkerExit</h3><p>processWorkerExit源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">processWorkerExit</span><span class=\"params\">(Worker w, <span class=\"keyword\">boolean</span> completedAbruptly)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 如果worker是非正常退出任务执行循环，则减少workerCount</span></span><br><span class=\"line\">    <span class=\"comment\">// 若是正常退出，则worker在getTask获取任务失败退出后已经减少了workerCount，可以正常移除该worker了</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (completedAbruptly)</span><br><span class=\"line\">        decrementWorkerCount();</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        completedTaskCount += w.completedTasks;</span><br><span class=\"line\">        <span class=\"comment\">// 移除worker</span></span><br><span class=\"line\">        workers.remove(w);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 尝试执行终止操作</span></span><br><span class=\"line\">    tryTerminate();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">    <span class=\"comment\">// 如果当前状态为Running或者Shutdown，则执行以下流程</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (runStateLessThan(c, STOP)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 若worker为正常退出任务执行循环，则需要额外判断是否需要新增worker</span></span><br><span class=\"line\">        <span class=\"comment\">// 分两种情况：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1. 若需要将核心线程在一定闲置时间后被移除，则当前worker最多保留一个</span></span><br><span class=\"line\">        <span class=\"comment\">// 2. 如果不需要将核心线程闲置一段时间后移除，则可以保留不超过核心线程数的worker</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!completedAbruptly) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> min = allowCoreThreadTimeOut ? <span class=\"number\">0</span> : corePoolSize;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (min == <span class=\"number\">0</span> &amp;&amp; ! workQueue.isEmpty())</span><br><span class=\"line\">                min = <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"comment\">// 如果worker已经够用了就不用addWorker了</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (workerCountOf(c) &gt;= min)</span><br><span class=\"line\">                <span class=\"keyword\">return</span>; <span class=\"comment\">// replacement not needed</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 执行以上判断后依然需要增加worker的话就调用addWorker，不传入任何task</span></span><br><span class=\"line\">        addWorker(<span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>逻辑相对比较复杂，总结来说，根据<code>completedAbruptly</code>参数将退出操作分为两条路线：</p>\n<p><code>正常退出任务执行循环：</code></p>\n<ol>\n<li>不需要减少worker数目</li>\n<li>将当前worker移除，并尝试执行终止操作</li>\n<li>如果当前状态为<code>Running</code>或者<code>Shutdown</code>，表示如果<code>workQueue</code>里面还有任务要执行的话，是需要继续执行的，那么接下来尝试新增worker</li>\n<li>计算当前需要保留的worker数目（min变量），如果<code>workerCount</code>已经满足需求则不额外增加worker了（这里依然使用<code>allowCoreThreadTimeOut</code>判断是否保留一定数量的核心线程，如果为true，则worker最多保留一个），直接退出</li>\n<li>如果<code>workerCount</code>数目不满足需求，则新增一个worker然后让他去<code>workQueue</code>里面取任务执行</li>\n</ol>\n<p><code>非正常退出任务执行循环</code></p>\n<ol>\n<li>减少worker数目</li>\n<li>移除当前worker并尝试执行终止操作，如果当前状态为<code>Running</code>或者<code>Shutdown</code>，则直接新增worker</li>\n</ol>\n<p>至于为什么将worker退出操作分为正常和非正常，我是这么理解的：</p>\n<p><code>正常退出</code>：说明worker调用<code>getTask()</code>没有成功取到任务，将被抛弃，<code>getTask</code>方法已经对<code>workCount</code>进行了扣减，这里就不需要对<code>workerCount</code>作任何变动，此外需要判断当前<code>workerCount</code>数目够不够</p>\n<p><code>非正常退出</code>：这种情况下需要对<code>workerCount</code>进行扣减并立即补充一个worker，当然如果当前状态为<code>Stop</code>或者<code>Tyding</code>甚至<code>Terminated</code>的话就没必要补充了</p>"},{"title":"java线程池源码分析--shutdown, shutdownNow, awaitTermination","date":"2019-01-21T02:31:00.000Z","_content":"谈到jdk线程池的生命周期就不得不说shutdown，shutdownNow和awaitTermination这三个方法，一般用来进行线程池的停止和资源的释放，以下例子主要讨论`ThreadPoolExecutor`的实现\n\n<!-- more -->\n\n### 程序演示\n\n- shutdown\n\n  该方法用于在结束线程池的任务提交后关闭线程池，让线程池拒绝后续的任务提交，以下提交5个任务到线程池，并调用`shutdown`方法，接着尝试再次提交任务：\n\n  ```JAVA\n  @Before\n  public void setup() {\n  \texecutorService = Executors.newFixedThreadPool(5);\n  \trunnable = new Runnable() {\n  \t\tpublic void run() {\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\ttry {\n  \t\t\t\tThread.sleep(3000);\n  \t\t\t} catch (InterruptedException e) {\n  \t\t\t\te.printStackTrace();\n  \t\t\t}\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t}\n  \t};\n  \tfor (int i = 0; i < 5; i++) {\n  \t\texecutorService.submit(runnable);\n  \t}\n  }\n  \n  @Test\n  public void test2() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \texecutorService.shutdown();\n  \t// here, jump out RejectedExecutionException\n  \texecutorService.submit(runnable);\n  }\n  ```\n\n  输出结果：\n\n  ![1547708979007](/blog/images/1547708979007.png)\n\n  可以看出，调用`shutdown`后是无法再次提交任务的，资源会被回收，只能重新创建一个新的线程池重新提交任务\n\n- shutdown配合awaitTermination\n\n  通常情况下`shutdown`方法不会独立使用，因为调用这个方法后无法得知立即线程池是否已经停止了，可能还有任务未执行完，官方注释如下：\n\n  ![1547709651604](/blog/images/1547709651604.png)\n\n  > 该方法并不会等待之前提交的任务完成执行，如果想达到这个目的请使用awaitTermination\n\n  这个解释我认为有一点歧义，因为`shutdown`方法是会等待之前提交的任务完成执行的，只不过调用了该方法后并不会留时间给用户一个反馈，而`awaitTermination`会等待一段时间并查看线程池是否已经停止了:\n\n  ```JAVA\n  @Test\n  public void test1() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \t// wait for the tasks which is in execution to finish their work\n  \texecutorService.shutdown();\n  \twhile (!executorService.awaitTermination(1, TimeUnit.SECONDS)) {\n  \t\tSystem.out.println(\"threadPool is not terminated!\");\n  \t}\n  \tSystem.out.println(\"threadPool is terminated!\");\n  }\n  ```\n\n  结果：\n\n  ![1547710015291](/blog/images/1547710015291.png)\n\n  提交5个任务后马上调用`shutdown`, 并不会中止任务，让`awaitTermination`每隔一秒检查一下是否停止，以此达到确认线程池成功停止的目的\n\n\n\n- shutdownNow\n\n  `shutdown`会等到提交过的任务完成执行， 但`shutdownNow`会尝试停止当前所有正在执行的任务\n\n  ```JAVA\n  @Before\n  public void setup() {\n  \texecutorService = Executors.newFixedThreadPool(5);\n  \trunnable = new Runnable() {\n  \t\tpublic void run() {\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\tboolean isInterrupted = false;\n  \t\t\ttry {\n  \t\t\t\tThread.sleep(3000);\n  \t\t\t} catch (InterruptedException e) {\n  \t\t\t\tisInterrupted = true;\n  \t\t\t}\n  \t\t\tif (!isInterrupted) {\n  \t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t\t}\n  \t\t}\n  \t};\n  \tfor (int i = 0; i < 5; i++) {\n  \t\texecutorService.submit(runnable);\n  \t}\n  }\n  \n  @Test\n  public void test3() throws Exception {\n  \tSystem.out.println(\"try to shutdownNow threadPool!\");\n  \t// if the Runnable or Callable can jump out InterruptedException\n  \t// the thread in ThreadPool can be terminated by this method\n  \texecutorService.shutdownNow();\n  }\n  ```\n\n  程序对比之前的例子，做了一些修改，当出现`InterruptedException`则中断线程，结果如下：\n\n  ![1547712156480](/blog/images/1547712156480.png)\n\n  官方文档指出，当调用`shutdownNow`后，每个正在执行任务的线程调用了`interrupt()`方法，但这种方式只有对能正常反馈`InterruptedException`的线程使用，否则正在运行的线程依然无法停止：\n\n  ![1547712312524](/blog/images/1547712312524.png)\n\n\n\n  `shutdownNow`最终会返回还未执行的任务集合，比如如下例子：\n\n  ```JAVA\n  @Test\n  \tpublic void test1() {\n  \t\texecutorService = Executors.newFixedThreadPool(1);\n  \t\trunnable = new Runnable() {\n  \t\t\tpublic void run() {\n  \t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\t\tboolean isInterrupted = false;\n  \t\t\t\ttry {\n  \t\t\t\t\tThread.sleep(3000);\n  \t\t\t\t} catch (InterruptedException e) {\n  \t\t\t\t\tisInterrupted = true;\n  \t\t\t\t}\n  \t\t\t\tif (!isInterrupted) {\n  \t\t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t\t\t}\n  \t\t\t}\n  \t\t};\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\tList<Runnable> noExecutedTasks = executorService.shutdownNow();\n  \t\tSystem.out.println(noExecutedTasks.size());\n  \t}\n  ```\n\n  结果如下：\n\n  ![1547712873168](/blog/images/1547712873168.png)\n\n  线程池最大线程数只有1个，提交5个任务后马上调用`shutdownNow`，正在执行的任务被终止，并导致剩下4个任务没有机会执行，则将这四个任务集合返还给用户\n\n\n\n- isShutdown和isTerminated\n\n  线程池生命周期方法中的`isShutdown`和`isTerminated`也比较重要，前者在线程池调用`shutdown`后返回true，后者在线程池中任务全部执行结束后才返回true：\n\n  ```JAVA\n  @Test\n  public void test4() throws Exception {\n  \tSystem.out.println(\"try to shutdownNow threadPool!\");\n  \texecutorService.shutdown();\n  \t// still RejectedExecutionException\n  \tSystem.out.println(executorService.isShutdown());\n  \tSystem.out.println(executorService.isTerminated());\n  }\n  ```\n\n  ![1547713322805](/blog/images/1547713322805.png)\n\n  调用`isShutdown`后，`isShutdown`为true，但`isTerminated`为false\n\n  ```JAVA\n  @Test\n  public void test5() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \t// wait for the tasks which is in execution to finish their work\n  \texecutorService.shutdown();\n  \tSystem.out.println(executorService.isShutdown());\n  \tSystem.out.println(executorService.isTerminated());\n  \twhile (!executorService.awaitTermination(1, TimeUnit.SECONDS)) {\n  \t\tSystem.out.println(\"threadPool is not terminated!\");\n  \t}\n  \tSystem.out.println(\"threadPool is terminated!\");\n  \tSystem.out.println(executorService.isTerminated());\n  }\n  ```\n\n  ![1547713402558](/blog/images/1547713402558.png)\n\n  第一次调用`isTerminated`，返回false，最后等所有任务都执行完了，该方法便返回true\n\n\n\n### 源码浅析\n\n调用上述几种线程池生命周期方法后，线程池内部做的怎样的实现呢？jdk线程池内部实现机理还是挺复杂的，现在从`isShutdown`入手来一探究竟\n\n- shutdown()\n\n  ```JAVA\n  public void shutdown() {\n      final ReentrantLock mainLock = this.mainLock;\n      mainLock.lock();\n      try {\n          checkShutdownAccess();\n          advanceRunState(SHUTDOWN);\n          interruptIdleWorkers();\n          onShutdown(); // hook for ScheduledThreadPoolExecutor\n      } finally {\n          mainLock.unlock();\n      }\n      tryTerminate();\n  }\n  ```\n\n  `mainLock`用于将线程的停止操作进行同步处理，然后调用`checkShutdownAccess`对用户是否有线程访问权限\n\n- advanceRunState(SHUTDOWN)\n\n  `advanceRunState`方法用于切换线程池状态，调用该方法将线程池状态设置为`SHUTDOWN`\n\n  ```JAVA\n      private void advanceRunState(int targetState) {\n          for (;;) {\n              int c = ctl.get();\n              if (runStateAtLeast(c, targetState) ||\n                  ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c))))\n                  break;\n          }\n      }\n  ```\n\n  内部使用了CAS原子操作来进行状态切换\n\n- interruptIdleWorkers()\n\n  interruptIdleWorkers()方法用来给每个空闲的`worker`线程打上interrupt标记:\n\n  ```JAVA\n      private void interruptIdleWorkers(boolean onlyOne) {\n          final ReentrantLock mainLock = this.mainLock;\n          mainLock.lock();\n          try {\n              for (Worker w : workers) {\n                  Thread t = w.thread;\n                  if (!t.isInterrupted() && w.tryLock()) {\n                      try {\n                          t.interrupt();\n                      } catch (SecurityException ignore) {\n                      } finally {\n                          w.unlock();\n                      }\n                  }\n                  if (onlyOne)\n                      break;\n              }\n          } finally {\n              mainLock.unlock();\n          }\n      }\n  ```\n\n  `!t.isInterrupted() && w.tryLock() `这个判断很重要，首先判断当前worker的执行线程是否已经interrupt，然后判断worker是否能成功获取锁，如果返回true则说明当前worker没有执行任务（），最后执行interrupt()方法并释放work锁和mainLock同步锁\n\n- onShutdown()\n\n- tryTerminate()\n\n  interruptIdleWorkers方法只能保证将空闲的worker线程置为interruptted，但正在工作的worker还是会继续执行任务，这时候需要启用`tryTerminate`方法，进行终止操作：\n\n  ```JAVA\n      final void tryTerminate() {\n          for (;;) {\n              int c = ctl.get();\n              // 以下三种状态不进行终止操作：\n              // 1.RUNNING状态 2.TIDING或者TERMINATED状态 3.SHUTDOWN状态并且workQueue不为空\n              if (isRunning(c) ||\n                  runStateAtLeast(c, TIDYING) ||\n                  (runStateOf(c) == SHUTDOWN && ! workQueue.isEmpty()))\n                  return;\n              // 尝试interrupt空闲的worker，只中断一个\n              if (workerCountOf(c) != 0) { // Eligible to terminate\n                  interruptIdleWorkers(ONLY_ONE);\n                  return;\n              }\n  \t\t\t\n              final ReentrantLock mainLock = this.mainLock;\n              mainLock.lock();\n              try {\n                  // 真正开始终止操作，通过CAS将当前状态置为TIDYNG，即高于STOP的一种状态\n                  if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) {\n                      try {\n                          //终止操作，在ThreadPoolExecutor中啥也没做，留给子类用作额外的资源回收\n                          terminated();\n                      } finally {\n                          //将状态置为TERMINATED\n                          ctl.set(ctlOf(TERMINATED, 0));\n                          //将调用了awaitTermination()的线程唤醒\n                          termination.signalAll();\n                      }\n                      return;\n                  }\n              } finally {\n                  mainLock.unlock();\n              }\n              // else retry on failed CAS\n          }\n      }\n  ```","source":"_posts/java线程池源码分析--shutdown, shutdownNow, awaitTermination.md","raw":"title: 'java线程池源码分析--shutdown, shutdownNow, awaitTermination'\ntags:\n  - 多线程\n  - Java并发包\n  - Java\ncategories:\n  - 基础知识\ndate: 2019-01-21 10:31:00\n---\n谈到jdk线程池的生命周期就不得不说shutdown，shutdownNow和awaitTermination这三个方法，一般用来进行线程池的停止和资源的释放，以下例子主要讨论`ThreadPoolExecutor`的实现\n\n<!-- more -->\n\n### 程序演示\n\n- shutdown\n\n  该方法用于在结束线程池的任务提交后关闭线程池，让线程池拒绝后续的任务提交，以下提交5个任务到线程池，并调用`shutdown`方法，接着尝试再次提交任务：\n\n  ```JAVA\n  @Before\n  public void setup() {\n  \texecutorService = Executors.newFixedThreadPool(5);\n  \trunnable = new Runnable() {\n  \t\tpublic void run() {\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\ttry {\n  \t\t\t\tThread.sleep(3000);\n  \t\t\t} catch (InterruptedException e) {\n  \t\t\t\te.printStackTrace();\n  \t\t\t}\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t}\n  \t};\n  \tfor (int i = 0; i < 5; i++) {\n  \t\texecutorService.submit(runnable);\n  \t}\n  }\n  \n  @Test\n  public void test2() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \texecutorService.shutdown();\n  \t// here, jump out RejectedExecutionException\n  \texecutorService.submit(runnable);\n  }\n  ```\n\n  输出结果：\n\n  ![1547708979007](/blog/images/1547708979007.png)\n\n  可以看出，调用`shutdown`后是无法再次提交任务的，资源会被回收，只能重新创建一个新的线程池重新提交任务\n\n- shutdown配合awaitTermination\n\n  通常情况下`shutdown`方法不会独立使用，因为调用这个方法后无法得知立即线程池是否已经停止了，可能还有任务未执行完，官方注释如下：\n\n  ![1547709651604](/blog/images/1547709651604.png)\n\n  > 该方法并不会等待之前提交的任务完成执行，如果想达到这个目的请使用awaitTermination\n\n  这个解释我认为有一点歧义，因为`shutdown`方法是会等待之前提交的任务完成执行的，只不过调用了该方法后并不会留时间给用户一个反馈，而`awaitTermination`会等待一段时间并查看线程池是否已经停止了:\n\n  ```JAVA\n  @Test\n  public void test1() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \t// wait for the tasks which is in execution to finish their work\n  \texecutorService.shutdown();\n  \twhile (!executorService.awaitTermination(1, TimeUnit.SECONDS)) {\n  \t\tSystem.out.println(\"threadPool is not terminated!\");\n  \t}\n  \tSystem.out.println(\"threadPool is terminated!\");\n  }\n  ```\n\n  结果：\n\n  ![1547710015291](/blog/images/1547710015291.png)\n\n  提交5个任务后马上调用`shutdown`, 并不会中止任务，让`awaitTermination`每隔一秒检查一下是否停止，以此达到确认线程池成功停止的目的\n\n\n\n- shutdownNow\n\n  `shutdown`会等到提交过的任务完成执行， 但`shutdownNow`会尝试停止当前所有正在执行的任务\n\n  ```JAVA\n  @Before\n  public void setup() {\n  \texecutorService = Executors.newFixedThreadPool(5);\n  \trunnable = new Runnable() {\n  \t\tpublic void run() {\n  \t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\tboolean isInterrupted = false;\n  \t\t\ttry {\n  \t\t\t\tThread.sleep(3000);\n  \t\t\t} catch (InterruptedException e) {\n  \t\t\t\tisInterrupted = true;\n  \t\t\t}\n  \t\t\tif (!isInterrupted) {\n  \t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t\t}\n  \t\t}\n  \t};\n  \tfor (int i = 0; i < 5; i++) {\n  \t\texecutorService.submit(runnable);\n  \t}\n  }\n  \n  @Test\n  public void test3() throws Exception {\n  \tSystem.out.println(\"try to shutdownNow threadPool!\");\n  \t// if the Runnable or Callable can jump out InterruptedException\n  \t// the thread in ThreadPool can be terminated by this method\n  \texecutorService.shutdownNow();\n  }\n  ```\n\n  程序对比之前的例子，做了一些修改，当出现`InterruptedException`则中断线程，结果如下：\n\n  ![1547712156480](/blog/images/1547712156480.png)\n\n  官方文档指出，当调用`shutdownNow`后，每个正在执行任务的线程调用了`interrupt()`方法，但这种方式只有对能正常反馈`InterruptedException`的线程使用，否则正在运行的线程依然无法停止：\n\n  ![1547712312524](/blog/images/1547712312524.png)\n\n\n\n  `shutdownNow`最终会返回还未执行的任务集合，比如如下例子：\n\n  ```JAVA\n  @Test\n  \tpublic void test1() {\n  \t\texecutorService = Executors.newFixedThreadPool(1);\n  \t\trunnable = new Runnable() {\n  \t\t\tpublic void run() {\n  \t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} start!\");\n  \t\t\t\tboolean isInterrupted = false;\n  \t\t\t\ttry {\n  \t\t\t\t\tThread.sleep(3000);\n  \t\t\t\t} catch (InterruptedException e) {\n  \t\t\t\t\tisInterrupted = true;\n  \t\t\t\t}\n  \t\t\t\tif (!isInterrupted) {\n  \t\t\t\t\tSystem.out.println(\"Thread: {\" + Thread.currentThread().getName() + \"} stop!\");\n  \t\t\t\t}\n  \t\t\t}\n  \t\t};\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\texecutorService.submit(runnable);\n  \t\tList<Runnable> noExecutedTasks = executorService.shutdownNow();\n  \t\tSystem.out.println(noExecutedTasks.size());\n  \t}\n  ```\n\n  结果如下：\n\n  ![1547712873168](/blog/images/1547712873168.png)\n\n  线程池最大线程数只有1个，提交5个任务后马上调用`shutdownNow`，正在执行的任务被终止，并导致剩下4个任务没有机会执行，则将这四个任务集合返还给用户\n\n\n\n- isShutdown和isTerminated\n\n  线程池生命周期方法中的`isShutdown`和`isTerminated`也比较重要，前者在线程池调用`shutdown`后返回true，后者在线程池中任务全部执行结束后才返回true：\n\n  ```JAVA\n  @Test\n  public void test4() throws Exception {\n  \tSystem.out.println(\"try to shutdownNow threadPool!\");\n  \texecutorService.shutdown();\n  \t// still RejectedExecutionException\n  \tSystem.out.println(executorService.isShutdown());\n  \tSystem.out.println(executorService.isTerminated());\n  }\n  ```\n\n  ![1547713322805](/blog/images/1547713322805.png)\n\n  调用`isShutdown`后，`isShutdown`为true，但`isTerminated`为false\n\n  ```JAVA\n  @Test\n  public void test5() throws Exception {\n  \tSystem.out.println(\"try to shutdown threadPool!\");\n  \t// wait for the tasks which is in execution to finish their work\n  \texecutorService.shutdown();\n  \tSystem.out.println(executorService.isShutdown());\n  \tSystem.out.println(executorService.isTerminated());\n  \twhile (!executorService.awaitTermination(1, TimeUnit.SECONDS)) {\n  \t\tSystem.out.println(\"threadPool is not terminated!\");\n  \t}\n  \tSystem.out.println(\"threadPool is terminated!\");\n  \tSystem.out.println(executorService.isTerminated());\n  }\n  ```\n\n  ![1547713402558](/blog/images/1547713402558.png)\n\n  第一次调用`isTerminated`，返回false，最后等所有任务都执行完了，该方法便返回true\n\n\n\n### 源码浅析\n\n调用上述几种线程池生命周期方法后，线程池内部做的怎样的实现呢？jdk线程池内部实现机理还是挺复杂的，现在从`isShutdown`入手来一探究竟\n\n- shutdown()\n\n  ```JAVA\n  public void shutdown() {\n      final ReentrantLock mainLock = this.mainLock;\n      mainLock.lock();\n      try {\n          checkShutdownAccess();\n          advanceRunState(SHUTDOWN);\n          interruptIdleWorkers();\n          onShutdown(); // hook for ScheduledThreadPoolExecutor\n      } finally {\n          mainLock.unlock();\n      }\n      tryTerminate();\n  }\n  ```\n\n  `mainLock`用于将线程的停止操作进行同步处理，然后调用`checkShutdownAccess`对用户是否有线程访问权限\n\n- advanceRunState(SHUTDOWN)\n\n  `advanceRunState`方法用于切换线程池状态，调用该方法将线程池状态设置为`SHUTDOWN`\n\n  ```JAVA\n      private void advanceRunState(int targetState) {\n          for (;;) {\n              int c = ctl.get();\n              if (runStateAtLeast(c, targetState) ||\n                  ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c))))\n                  break;\n          }\n      }\n  ```\n\n  内部使用了CAS原子操作来进行状态切换\n\n- interruptIdleWorkers()\n\n  interruptIdleWorkers()方法用来给每个空闲的`worker`线程打上interrupt标记:\n\n  ```JAVA\n      private void interruptIdleWorkers(boolean onlyOne) {\n          final ReentrantLock mainLock = this.mainLock;\n          mainLock.lock();\n          try {\n              for (Worker w : workers) {\n                  Thread t = w.thread;\n                  if (!t.isInterrupted() && w.tryLock()) {\n                      try {\n                          t.interrupt();\n                      } catch (SecurityException ignore) {\n                      } finally {\n                          w.unlock();\n                      }\n                  }\n                  if (onlyOne)\n                      break;\n              }\n          } finally {\n              mainLock.unlock();\n          }\n      }\n  ```\n\n  `!t.isInterrupted() && w.tryLock() `这个判断很重要，首先判断当前worker的执行线程是否已经interrupt，然后判断worker是否能成功获取锁，如果返回true则说明当前worker没有执行任务（），最后执行interrupt()方法并释放work锁和mainLock同步锁\n\n- onShutdown()\n\n- tryTerminate()\n\n  interruptIdleWorkers方法只能保证将空闲的worker线程置为interruptted，但正在工作的worker还是会继续执行任务，这时候需要启用`tryTerminate`方法，进行终止操作：\n\n  ```JAVA\n      final void tryTerminate() {\n          for (;;) {\n              int c = ctl.get();\n              // 以下三种状态不进行终止操作：\n              // 1.RUNNING状态 2.TIDING或者TERMINATED状态 3.SHUTDOWN状态并且workQueue不为空\n              if (isRunning(c) ||\n                  runStateAtLeast(c, TIDYING) ||\n                  (runStateOf(c) == SHUTDOWN && ! workQueue.isEmpty()))\n                  return;\n              // 尝试interrupt空闲的worker，只中断一个\n              if (workerCountOf(c) != 0) { // Eligible to terminate\n                  interruptIdleWorkers(ONLY_ONE);\n                  return;\n              }\n  \t\t\t\n              final ReentrantLock mainLock = this.mainLock;\n              mainLock.lock();\n              try {\n                  // 真正开始终止操作，通过CAS将当前状态置为TIDYNG，即高于STOP的一种状态\n                  if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) {\n                      try {\n                          //终止操作，在ThreadPoolExecutor中啥也没做，留给子类用作额外的资源回收\n                          terminated();\n                      } finally {\n                          //将状态置为TERMINATED\n                          ctl.set(ctlOf(TERMINATED, 0));\n                          //将调用了awaitTermination()的线程唤醒\n                          termination.signalAll();\n                      }\n                      return;\n                  }\n              } finally {\n                  mainLock.unlock();\n              }\n              // else retry on failed CAS\n          }\n      }\n  ```","slug":"java线程池源码分析--shutdown, shutdownNow, awaitTermination","published":1,"updated":"2019-03-19T13:30:58.437Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3u0010g0qrj7s0ni1z","content":"<p>谈到jdk线程池的生命周期就不得不说shutdown，shutdownNow和awaitTermination这三个方法，一般用来进行线程池的停止和资源的释放，以下例子主要讨论<code>ThreadPoolExecutor</code>的实现</p>\n<a id=\"more\"></a>\n<h3 id=\"程序演示\"><a href=\"#程序演示\" class=\"headerlink\" title=\"程序演示\"></a>程序演示</h3><ul>\n<li><p>shutdown</p>\n<p>该方法用于在结束线程池的任务提交后关闭线程池，让线程池拒绝后续的任务提交，以下提交5个任务到线程池，并调用<code>shutdown</code>方法，接着尝试再次提交任务：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Before</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setup</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">5</span>);</span><br><span class=\"line\">\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test2</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"comment\">// here, jump out RejectedExecutionException</span></span><br><span class=\"line\">\texecutorService.submit(runnable);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><img src=\"/blog/images/1547708979007.png\" alt=\"1547708979007\"></p>\n<p>可以看出，调用<code>shutdown</code>后是无法再次提交任务的，资源会被回收，只能重新创建一个新的线程池重新提交任务</p>\n</li>\n<li><p>shutdown配合awaitTermination</p>\n<p>通常情况下<code>shutdown</code>方法不会独立使用，因为调用这个方法后无法得知立即线程池是否已经停止了，可能还有任务未执行完，官方注释如下：</p>\n<p><img src=\"/blog/images/1547709651604.png\" alt=\"1547709651604\"></p>\n<blockquote>\n<p>该方法并不会等待之前提交的任务完成执行，如果想达到这个目的请使用awaitTermination</p>\n</blockquote>\n<p>这个解释我认为有一点歧义，因为<code>shutdown</code>方法是会等待之前提交的任务完成执行的，只不过调用了该方法后并不会留时间给用户一个反馈，而<code>awaitTermination</code>会等待一段时间并查看线程池是否已经停止了:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test1</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// wait for the tasks which is in execution to finish their work</span></span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (!executorService.awaitTermination(<span class=\"number\">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"threadPool is not terminated!\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"threadPool is terminated!\"</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<p><img src=\"/blog/images/1547710015291.png\" alt=\"1547710015291\"></p>\n<p>提交5个任务后马上调用<code>shutdown</code>, 并不会中止任务，让<code>awaitTermination</code>每隔一秒检查一下是否停止，以此达到确认线程池成功停止的目的</p>\n</li>\n</ul>\n<ul>\n<li><p>shutdownNow</p>\n<p><code>shutdown</code>会等到提交过的任务完成执行， 但<code>shutdownNow</code>会尝试停止当前所有正在执行的任务</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Before</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setup</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">5</span>);</span><br><span class=\"line\">\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">boolean</span> isInterrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\tisInterrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (!isInterrupted) &#123;</span><br><span class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test3</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdownNow threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// if the Runnable or Callable can jump out InterruptedException</span></span><br><span class=\"line\">\t<span class=\"comment\">// the thread in ThreadPool can be terminated by this method</span></span><br><span class=\"line\">\texecutorService.shutdownNow();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>程序对比之前的例子，做了一些修改，当出现<code>InterruptedException</code>则中断线程，结果如下：</p>\n<p><img src=\"/blog/images/1547712156480.png\" alt=\"1547712156480\"></p>\n<p>官方文档指出，当调用<code>shutdownNow</code>后，每个正在执行任务的线程调用了<code>interrupt()</code>方法，但这种方式只有对能正常反馈<code>InterruptedException</code>的线程使用，否则正在运行的线程依然无法停止：</p>\n<p><img src=\"/blog/images/1547712312524.png\" alt=\"1547712312524\"></p>\n</li>\n</ul>\n<p>  <code>shutdownNow</code>最终会返回还未执行的任务集合，比如如下例子：</p>\n  <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test1</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">1</span>);</span><br><span class=\"line\">\t\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">boolean</span> isInterrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\t\tisInterrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isInterrupted) &#123;</span><br><span class=\"line\">\t\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\tList&lt;Runnable&gt; noExecutedTasks = executorService.shutdownNow();</span><br><span class=\"line\">\t\tSystem.out.println(noExecutedTasks.size());</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n<p>  结果如下：</p>\n<p>  <img src=\"/blog/images/1547712873168.png\" alt=\"1547712873168\"></p>\n<p>  线程池最大线程数只有1个，提交5个任务后马上调用<code>shutdownNow</code>，正在执行的任务被终止，并导致剩下4个任务没有机会执行，则将这四个任务集合返还给用户</p>\n<ul>\n<li><p>isShutdown和isTerminated</p>\n<p>线程池生命周期方法中的<code>isShutdown</code>和<code>isTerminated</code>也比较重要，前者在线程池调用<code>shutdown</code>后返回true，后者在线程池中任务全部执行结束后才返回true：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test4</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdownNow threadPool!\"</span>);</span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"comment\">// still RejectedExecutionException</span></span><br><span class=\"line\">\tSystem.out.println(executorService.isShutdown());</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"/blog/images/1547713322805.png\" alt=\"1547713322805\"></p>\n<p>调用<code>isShutdown</code>后，<code>isShutdown</code>为true，但<code>isTerminated</code>为false</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test5</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// wait for the tasks which is in execution to finish their work</span></span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\tSystem.out.println(executorService.isShutdown());</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (!executorService.awaitTermination(<span class=\"number\">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"threadPool is not terminated!\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"threadPool is terminated!\"</span>);</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"/blog/images/1547713402558.png\" alt=\"1547713402558\"></p>\n<p>第一次调用<code>isTerminated</code>，返回false，最后等所有任务都执行完了，该方法便返回true</p>\n</li>\n</ul>\n<h3 id=\"源码浅析\"><a href=\"#源码浅析\" class=\"headerlink\" title=\"源码浅析\"></a>源码浅析</h3><p>调用上述几种线程池生命周期方法后，线程池内部做的怎样的实现呢？jdk线程池内部实现机理还是挺复杂的，现在从<code>isShutdown</code>入手来一探究竟</p>\n<ul>\n<li><p>shutdown()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">shutdown</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        checkShutdownAccess();</span><br><span class=\"line\">        advanceRunState(SHUTDOWN);</span><br><span class=\"line\">        interruptIdleWorkers();</span><br><span class=\"line\">        onShutdown(); <span class=\"comment\">// hook for ScheduledThreadPoolExecutor</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    tryTerminate();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>mainLock</code>用于将线程的停止操作进行同步处理，然后调用<code>checkShutdownAccess</code>对用户是否有线程访问权限</p>\n</li>\n<li><p>advanceRunState(SHUTDOWN)</p>\n<p><code>advanceRunState</code>方法用于切换线程池状态，调用该方法将线程池状态设置为<code>SHUTDOWN</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">advanceRunState</span><span class=\"params\">(<span class=\"keyword\">int</span> targetState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (runStateAtLeast(c, targetState) ||</span><br><span class=\"line\">            ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c))))</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>内部使用了CAS原子操作来进行状态切换</p>\n</li>\n<li><p>interruptIdleWorkers()</p>\n<p>interruptIdleWorkers()方法用来给每个空闲的<code>worker</code>线程打上interrupt标记:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">interruptIdleWorkers</span><span class=\"params\">(<span class=\"keyword\">boolean</span> onlyOne)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Worker w : workers) &#123;</span><br><span class=\"line\">            Thread t = w.thread;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    t.interrupt();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (SecurityException ignore) &#123;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    w.unlock();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (onlyOne)</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>!t.isInterrupted() &amp;&amp; w.tryLock()</code>这个判断很重要，首先判断当前worker的执行线程是否已经interrupt，然后判断worker是否能成功获取锁，如果返回true则说明当前worker没有执行任务（），最后执行interrupt()方法并释放work锁和mainLock同步锁</p>\n</li>\n<li><p>onShutdown()</p>\n</li>\n<li><p>tryTerminate()</p>\n<p>interruptIdleWorkers方法只能保证将空闲的worker线程置为interruptted，但正在工作的worker还是会继续执行任务，这时候需要启用<code>tryTerminate</code>方法，进行终止操作：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">tryTerminate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"comment\">// 以下三种状态不进行终止操作：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1.RUNNING状态 2.TIDING或者TERMINATED状态 3.SHUTDOWN状态并且workQueue不为空</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (isRunning(c) ||</span><br><span class=\"line\">            runStateAtLeast(c, TIDYING) ||</span><br><span class=\"line\">            (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty()))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 尝试interrupt空闲的worker，只中断一个</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (workerCountOf(c) != <span class=\"number\">0</span>) &#123; <span class=\"comment\">// Eligible to terminate</span></span><br><span class=\"line\">            interruptIdleWorkers(ONLY_ONE);</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">  \t\t\t</span><br><span class=\"line\">        <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">        mainLock.lock();</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 真正开始终止操作，通过CAS将当前状态置为TIDYNG，即高于STOP的一种状态</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (ctl.compareAndSet(c, ctlOf(TIDYING, <span class=\"number\">0</span>))) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//终止操作，在ThreadPoolExecutor中啥也没做，留给子类用作额外的资源回收</span></span><br><span class=\"line\">                    terminated();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//将状态置为TERMINATED</span></span><br><span class=\"line\">                    ctl.set(ctlOf(TERMINATED, <span class=\"number\">0</span>));</span><br><span class=\"line\">                    <span class=\"comment\">//将调用了awaitTermination()的线程唤醒</span></span><br><span class=\"line\">                    termination.signalAll();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">return</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            mainLock.unlock();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// else retry on failed CAS</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>谈到jdk线程池的生命周期就不得不说shutdown，shutdownNow和awaitTermination这三个方法，一般用来进行线程池的停止和资源的释放，以下例子主要讨论<code>ThreadPoolExecutor</code>的实现</p>","more":"<h3 id=\"程序演示\"><a href=\"#程序演示\" class=\"headerlink\" title=\"程序演示\"></a>程序演示</h3><ul>\n<li><p>shutdown</p>\n<p>该方法用于在结束线程池的任务提交后关闭线程池，让线程池拒绝后续的任务提交，以下提交5个任务到线程池，并调用<code>shutdown</code>方法，接着尝试再次提交任务：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Before</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setup</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">5</span>);</span><br><span class=\"line\">\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test2</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"comment\">// here, jump out RejectedExecutionException</span></span><br><span class=\"line\">\texecutorService.submit(runnable);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><img src=\"/blog/images/1547708979007.png\" alt=\"1547708979007\"></p>\n<p>可以看出，调用<code>shutdown</code>后是无法再次提交任务的，资源会被回收，只能重新创建一个新的线程池重新提交任务</p>\n</li>\n<li><p>shutdown配合awaitTermination</p>\n<p>通常情况下<code>shutdown</code>方法不会独立使用，因为调用这个方法后无法得知立即线程池是否已经停止了，可能还有任务未执行完，官方注释如下：</p>\n<p><img src=\"/blog/images/1547709651604.png\" alt=\"1547709651604\"></p>\n<blockquote>\n<p>该方法并不会等待之前提交的任务完成执行，如果想达到这个目的请使用awaitTermination</p>\n</blockquote>\n<p>这个解释我认为有一点歧义，因为<code>shutdown</code>方法是会等待之前提交的任务完成执行的，只不过调用了该方法后并不会留时间给用户一个反馈，而<code>awaitTermination</code>会等待一段时间并查看线程池是否已经停止了:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test1</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// wait for the tasks which is in execution to finish their work</span></span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (!executorService.awaitTermination(<span class=\"number\">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"threadPool is not terminated!\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"threadPool is terminated!\"</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<p><img src=\"/blog/images/1547710015291.png\" alt=\"1547710015291\"></p>\n<p>提交5个任务后马上调用<code>shutdown</code>, 并不会中止任务，让<code>awaitTermination</code>每隔一秒检查一下是否停止，以此达到确认线程池成功停止的目的</p>\n</li>\n</ul>\n<ul>\n<li><p>shutdownNow</p>\n<p><code>shutdown</code>会等到提交过的任务完成执行， 但<code>shutdownNow</code>会尝试停止当前所有正在执行的任务</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Before</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setup</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">5</span>);</span><br><span class=\"line\">\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">boolean</span> isInterrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\tisInterrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (!isInterrupted) &#123;</span><br><span class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test3</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdownNow threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// if the Runnable or Callable can jump out InterruptedException</span></span><br><span class=\"line\">\t<span class=\"comment\">// the thread in ThreadPool can be terminated by this method</span></span><br><span class=\"line\">\texecutorService.shutdownNow();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>程序对比之前的例子，做了一些修改，当出现<code>InterruptedException</code>则中断线程，结果如下：</p>\n<p><img src=\"/blog/images/1547712156480.png\" alt=\"1547712156480\"></p>\n<p>官方文档指出，当调用<code>shutdownNow</code>后，每个正在执行任务的线程调用了<code>interrupt()</code>方法，但这种方式只有对能正常反馈<code>InterruptedException</code>的线程使用，否则正在运行的线程依然无法停止：</p>\n<p><img src=\"/blog/images/1547712312524.png\" alt=\"1547712312524\"></p>\n</li>\n</ul>\n<p>  <code>shutdownNow</code>最终会返回还未执行的任务集合，比如如下例子：</p>\n  <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test1</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\texecutorService = Executors.newFixedThreadPool(<span class=\"number\">1</span>);</span><br><span class=\"line\">\t\trunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; start!\"</span>);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">boolean</span> isInterrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tThread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\t\tisInterrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isInterrupted) &#123;</span><br><span class=\"line\">\t\t\t\t\tSystem.out.println(<span class=\"string\">\"Thread: &#123;\"</span> + Thread.currentThread().getName() + <span class=\"string\">\"&#125; stop!\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\texecutorService.submit(runnable);</span><br><span class=\"line\">\t\tList&lt;Runnable&gt; noExecutedTasks = executorService.shutdownNow();</span><br><span class=\"line\">\t\tSystem.out.println(noExecutedTasks.size());</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n<p>  结果如下：</p>\n<p>  <img src=\"/blog/images/1547712873168.png\" alt=\"1547712873168\"></p>\n<p>  线程池最大线程数只有1个，提交5个任务后马上调用<code>shutdownNow</code>，正在执行的任务被终止，并导致剩下4个任务没有机会执行，则将这四个任务集合返还给用户</p>\n<ul>\n<li><p>isShutdown和isTerminated</p>\n<p>线程池生命周期方法中的<code>isShutdown</code>和<code>isTerminated</code>也比较重要，前者在线程池调用<code>shutdown</code>后返回true，后者在线程池中任务全部执行结束后才返回true：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test4</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdownNow threadPool!\"</span>);</span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\t<span class=\"comment\">// still RejectedExecutionException</span></span><br><span class=\"line\">\tSystem.out.println(executorService.isShutdown());</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"/blog/images/1547713322805.png\" alt=\"1547713322805\"></p>\n<p>调用<code>isShutdown</code>后，<code>isShutdown</code>为true，但<code>isTerminated</code>为false</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test5</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"try to shutdown threadPool!\"</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// wait for the tasks which is in execution to finish their work</span></span><br><span class=\"line\">\texecutorService.shutdown();</span><br><span class=\"line\">\tSystem.out.println(executorService.isShutdown());</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (!executorService.awaitTermination(<span class=\"number\">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"threadPool is not terminated!\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tSystem.out.println(<span class=\"string\">\"threadPool is terminated!\"</span>);</span><br><span class=\"line\">\tSystem.out.println(executorService.isTerminated());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"/blog/images/1547713402558.png\" alt=\"1547713402558\"></p>\n<p>第一次调用<code>isTerminated</code>，返回false，最后等所有任务都执行完了，该方法便返回true</p>\n</li>\n</ul>\n<h3 id=\"源码浅析\"><a href=\"#源码浅析\" class=\"headerlink\" title=\"源码浅析\"></a>源码浅析</h3><p>调用上述几种线程池生命周期方法后，线程池内部做的怎样的实现呢？jdk线程池内部实现机理还是挺复杂的，现在从<code>isShutdown</code>入手来一探究竟</p>\n<ul>\n<li><p>shutdown()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">shutdown</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        checkShutdownAccess();</span><br><span class=\"line\">        advanceRunState(SHUTDOWN);</span><br><span class=\"line\">        interruptIdleWorkers();</span><br><span class=\"line\">        onShutdown(); <span class=\"comment\">// hook for ScheduledThreadPoolExecutor</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    tryTerminate();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>mainLock</code>用于将线程的停止操作进行同步处理，然后调用<code>checkShutdownAccess</code>对用户是否有线程访问权限</p>\n</li>\n<li><p>advanceRunState(SHUTDOWN)</p>\n<p><code>advanceRunState</code>方法用于切换线程池状态，调用该方法将线程池状态设置为<code>SHUTDOWN</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">advanceRunState</span><span class=\"params\">(<span class=\"keyword\">int</span> targetState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (runStateAtLeast(c, targetState) ||</span><br><span class=\"line\">            ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c))))</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>内部使用了CAS原子操作来进行状态切换</p>\n</li>\n<li><p>interruptIdleWorkers()</p>\n<p>interruptIdleWorkers()方法用来给每个空闲的<code>worker</code>线程打上interrupt标记:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">interruptIdleWorkers</span><span class=\"params\">(<span class=\"keyword\">boolean</span> onlyOne)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">    mainLock.lock();</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Worker w : workers) &#123;</span><br><span class=\"line\">            Thread t = w.thread;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    t.interrupt();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (SecurityException ignore) &#123;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    w.unlock();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (onlyOne)</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        mainLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>!t.isInterrupted() &amp;&amp; w.tryLock()</code>这个判断很重要，首先判断当前worker的执行线程是否已经interrupt，然后判断worker是否能成功获取锁，如果返回true则说明当前worker没有执行任务（），最后执行interrupt()方法并释放work锁和mainLock同步锁</p>\n</li>\n<li><p>onShutdown()</p>\n</li>\n<li><p>tryTerminate()</p>\n<p>interruptIdleWorkers方法只能保证将空闲的worker线程置为interruptted，但正在工作的worker还是会继续执行任务，这时候需要启用<code>tryTerminate</code>方法，进行终止操作：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">tryTerminate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = ctl.get();</span><br><span class=\"line\">        <span class=\"comment\">// 以下三种状态不进行终止操作：</span></span><br><span class=\"line\">        <span class=\"comment\">// 1.RUNNING状态 2.TIDING或者TERMINATED状态 3.SHUTDOWN状态并且workQueue不为空</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (isRunning(c) ||</span><br><span class=\"line\">            runStateAtLeast(c, TIDYING) ||</span><br><span class=\"line\">            (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty()))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 尝试interrupt空闲的worker，只中断一个</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (workerCountOf(c) != <span class=\"number\">0</span>) &#123; <span class=\"comment\">// Eligible to terminate</span></span><br><span class=\"line\">            interruptIdleWorkers(ONLY_ONE);</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">  \t\t\t</span><br><span class=\"line\">        <span class=\"keyword\">final</span> ReentrantLock mainLock = <span class=\"keyword\">this</span>.mainLock;</span><br><span class=\"line\">        mainLock.lock();</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 真正开始终止操作，通过CAS将当前状态置为TIDYNG，即高于STOP的一种状态</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (ctl.compareAndSet(c, ctlOf(TIDYING, <span class=\"number\">0</span>))) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//终止操作，在ThreadPoolExecutor中啥也没做，留给子类用作额外的资源回收</span></span><br><span class=\"line\">                    terminated();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//将状态置为TERMINATED</span></span><br><span class=\"line\">                    ctl.set(ctlOf(TERMINATED, <span class=\"number\">0</span>));</span><br><span class=\"line\">                    <span class=\"comment\">//将调用了awaitTermination()的线程唤醒</span></span><br><span class=\"line\">                    termination.signalAll();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">return</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            mainLock.unlock();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// else retry on failed CAS</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"kafka学习笔记（3）—— 消费者 consumer","author":"天渊","date":"2019-03-18T05:21:00.000Z","_content":"kafka-consumer作为kafka消息队列的消费者端，通过接收topic的消息进行处理然后输出到下游数据源（数据库，文件系统，或者是另外的消息队列）<!--more-->\n\nkafka-consumer最大的特点是，遵循发布/订阅模型，多个consumer共同订阅同一个topic，通过offset对消息的消费位置进行标记：\n\n![upload successful](\\blog\\images\\pasted-7.png)\n\n**consumer的offset保存在哪里？**：老版本的kafka，offset保存在zookeeper中，但由于consumer的offset数据太过庞大，不适合存放在zookeeper，因此新版本的kafka单独维护了一个保存offset的topic：`__consumer_offsets`，以group-id，topic以及partition做为组合Key对每个consumer的offset进行检索\n\n### consumer消费数据\n\nconsumer以consumer-group（消费者组）为单位向kafka订阅topic并消费数据：\n\n![upload successful](\\blog\\images\\pasted-8.png)\n\nkafka consumer抓取数据基本流程图：\n\n![upload successful](\\blog\\images\\pasted-9.png)\n\n几点特性：\n\n- **partition主从热备**：消息的读/写工作全部交给leader partition来完成，slave partition主动与leader同步，通过LEO和leader epoch等信息同步数据，leader通过ISR列表保存当前存活的slave状态\n- **可靠性**：只有当ISR列表中所有副本都成功收到的消息才能提供给consumer，可以提供给consumer的这部分消息由`high water`来进行控制 (设置`replica.lag.time.max.ms`参数可保证副本同步消息的时间上限)\n- **零拷贝**：kafka通过零拷贝的方式将数据返回给consumer\n\n#### consumer的java api\n\n使用kafka-consumer高级api进行消费操作：\n\n1. 创建单个consumer进行消费：\n\n   ```java\n   public class ConsumerGroupTest {\n       public static void main(String[] args) {\n           Map<String, Object> consumerConfig = new HashMap<>();\n           consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\n           consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n           consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n           consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, \"test-group-1\");\n           runConsumer(consumerConfig);\n       }\n       private static void runConsumer(Map<String, Object> consumerConfig) {\n           Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n           consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n           try {\n               while (true) {\n                   // poll拉取消息，最长等待时间10秒\n                   ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n                   records.forEach(record -> {\n                       printRecord(record);\n                       print(\"当前consumer实例：\", consumer.toString());\n                       System.out.println();\n                   });\n               }\n           } finally {\n               consumer.close();\n           }\n       }\n       private static void printRecord(ConsumerRecord<String, String> record) {\n           System.out.println(\"当前record：====> \"\n                              + \" topic:\" + record.topic()\n                              + \" partition:\" + record.partition()\n                              + \" offset:\" + record.offset()\n                              + \" value:\" + record.value()\n                             );\n       }\n       private static void print(String name, Object value) {\n           System.out.println(\"KafkaConsumer ====> \" + name + \" is \" + \"{ \" + value + \" }\");\n       }\n   }\n   ```\n\n   单个consumer实例可以订阅多个topic，也可以通过正则表达式订阅topic：\n\n   ```java\n   // 订阅多个topic\n   consumer.subscribe(Arrays.asList(\"test-topic-1\", \"test-topic-2\"));\n   // 通过正则表达式订阅多个topic\n   consumer.subscribe(Pattern.compile(\"test.*\"));\n   ```\n\n2. 创建消费者组订阅包含多个partition的topic：\n\n   创建一个叫做“test-group-1”的消费组，订阅“test_multi_par_topic_1” topic， 该topic包含三个partition：\n\n   ```java\n   public static void main(String[] args) {\n       Map<String, Object> consumerConfig = new HashMap<>();\n       consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\n       consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n       consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n       consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, \"test-group-1\");\n       runConsumerManualCommit(consumerConfig);\n   }\n   \n   private static void runConsumer(Map<String, Object> consumerConfig) {\n       Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n       consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n       try {\n           while (true) {\n               // poll拉取消息，最长等待时间10秒\n               ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n               records.forEach(record -> {\n                   printRecord(record);\n                   print(\"当前consumer实例：\", consumer.toString());\n                   System.out.println();\n               });\n           }\n       } finally {\n           consumer.close();\n       }\n   }\n   ```\n   \n   首先只启动一个实例，可以看到，当前consumer实例消费了全部的三个partition的数据：\n   \n![upload successful](\\blog\\images\\pasted-10.png)\n\n    \n   向该组加入一个consumer实例，可以看到，之前`@48322fe3`这个consumer实例只分到了partition2，而新加入的`@365ae362`这个实例分到了0和1：：\n   \n![upload successful](\\blog\\images\\pasted-11.png) \n\n![upload successful](\\blog\\images\\pasted-12.png)\n\n继续加consumer实例进来，最早的元老实例1，分到了partition2：\n \n![upload successful](\\blog\\images\\pasted-13.png)\n\t\n之后加入的实例2分到了partition0：\n\n![upload successful](\\blog\\images\\pasted-14.png)\n\n最新加入的实例3 `@345dd14f`，分到了parition1：\n\n![upload successful](\\blog\\images\\pasted-15.png)\n\t\n接下来将元老实例1`@48322fe3`停掉，观察实例2和实例3 （注意观察实例1的最后消费offset），实例1最后消费情况，消费完offset=85就挂掉了：\n   \n![upload successful](\\blog\\images\\pasted-16.png)\n   \n观察实例2可以发现，实例2除了继续消费partition0，还认领了之前实例3消费的partition1的任务\n  \n![upload successful](\\blog\\images\\pasted-17.png)\n\n观察实例3，消费partition1直到offset=91的位置，发生了rebalance，之后partition1的任务分派给了上面实例2，自己分到了因实例1挂掉而无人认领的partition2，并且从实例1最后消费位置即offset=86开始消费\n   \n   \n![upload successful](\\blog\\images\\pasted-18.png)\n\n### consumer-group\n\n单个consumer-group即可以只消费一个topic的消息，也可以同时消费多个topic的消息\n\n#### consumer-group订阅topic\n\n单个consumer-group在订阅某个topic时，如果不特殊指定订阅某个partition，kafka将启用`Coordinator`对consumer-group内的consumer实例进行负载均衡，有以下特点：\n\n1. 组内的一个consumer实例消费全部的partition：\n\n\t![upload successful](\\blog\\images\\pasted-19.png)\n\n2. 组内继续加入consumer实例，一同消费多个partition，消费过程与另外的consumer完全隔离不受影响：\n\n\t![upload successful](\\blog\\images\\pasted-20.png)\n\n3. 组内继续加入consumer，每个consumer实例单独消费一个partition，此时consumer数目与partition数目一致：\n\n\t![upload successful](\\blog\\images\\pasted-21.png)\n\n4. 继续加入consumer，数目超过partition数目，此时多出来的consumer将分不到partition进行消费：\n\n\t![upload successful](\\blog\\images\\pasted-22.png)\n\n如上可以看出，当单个consumer-group订阅topic进行消费时，kafka的Coordinator保证topic的数据能够被均匀分派到组内的各个消费者上，并且topic中的**单个partition只能被单个consumer-group中的其中一个consumer消费**\n\n不过，如果有其他consumer-group的consumer参与消费这个topic，将与之前的consumer-group隔离，Coordinator将单独为这个新的组进行负载均衡：\n\n   ![upload successful](\\blog\\images\\pasted-23.png)\n    \n#### consumer分区再平衡\n\n- 每当一个consumer-group有新成员加入，并且和老成员一起消费同一个topic的时候，Coordinator都将进行分区再平衡（`rebalance`），为了平衡消费能力，老成员消费的partition有可能会被分派给新加入的consumer\n- 当有consumer实例挂掉时，之前分派给他的partition将会重新分派给剩余还活着的consumer（conusmer实例通过发送心跳让kafka集群知道他还活着）\n- 订阅的topic新加入partition也会触发rebalance\n- 通过正则表达式订阅某个类型的topic，当新加入该类型的topic时，也会发生rebalance\n\n#### consumer rebalance的大致流程：\n\n1. Topic/Partition的改变或者新Consumer的加入或者已有Consumer停止，将触发Coordinator注册在Zookeeper上的watch，Coordinator收到通知准备发起Rebalance操作。\n2. Coordinator通过在HeartbeatResponse中返回IllegalGeneration错误码通知各个consumer发起Rebalance操作。\n3. 存活的Consumer向Coordinator发送JoinGroupRequest\n4. Coordinator在Zookeeper中增加Group的Generation ID并将新的Partition分配情况写入Zookeeper\n5. Coordinator向存活的consumer发送JoinGroupResponse\n6. 存活的consumer收到JoinGroupResponse后重新启动新一轮消费\n\n### consumer配置\n\n除了上述`bootstrap.servers`,`key.deserializer`,`value.deserializer`,`group.id`这几个配置项必须进行配置之外，其他的配置项均为可选项：\n\n```properties\n# 单次拉取消息的最小字节数，如果该值不为0，在consumer拉取消息时，如果可消费数据达不到该值，broker将等待一段时间(fetch.max.wait.ms)直到有足够数据或者等待超时，再返回给consumer\nfetch.min.bytes=1024\n# 单次拉取消息的最长等待时间\nfetch.max.wait.ms=500\n# 指定consumer单次poll()从分区中拉取的最大字节数，默认1MB\nmax.partition.fetch.bytes=1048576\n# kafka集群判断consumer连接中断的最长等待时间，默认10秒，如果consumer超过该时间没有发送心跳，则会判断为死亡进而触发rebalance\nsession.timeout.ms=10000\n# 心跳间隔时间，建议不超过session.timeout.ms的1/3\nheartbeat.interval.ms=3000\n# 是否开启自动提交offset，默认是开启的，如果开启，将每隔auto.commit.interval.ms的时间将消费完成但未提交offset的consumer统一向kafka集群提交一次（风险：当consumer消费完成但未提交offset就挂掉时，重启后将造成消息重复消费）\nenable.auto.commit=true\n# 自动统一提交offset的时间间隔\nauto.commit.interval.ms=5000\n# 开始消费的consumer-group（或者该consumer掉线已久，已经丢失有效的offset信息）初始化offset的策略，\n# latest：默认值，默认从最新的有效offset开始消费\n# earliest：从最早的有效offset开始消费\n# none：如果kafka没有保存该consumer-group的offset信息则直接抛异常\nauto.offset.reset=[latest, earliest, none]\n# 分区分派策略类，kafka-clients自带的分区策略有Range和RoundRobin\n# 必须是PartitionAssignor的实现类\npartition.assignment.strategy=class org.apache.kafka.clients.consumer.RangeAssignor\n# 单次poll()拉取的消息数量最大值，默认是500个\nmax.poll.records=500\n```\n\n### kafka consumer的offset提交策略\n\nconsumer每次消费数据完成后需要提交offset给集群以更新自己的消费状态，提交过程非常重要，直接关系到消费过程的稳定性，提交异常的话可能导致以下两种后果：\n\n- 重复消费：一般发生在还没来得及提交offset即被中断消费的情况\n  \n  ![upload successful](\\blog\\images\\pasted-24.png)\n\n- 丢失消费：一般发生在消费过程有耗时操作的情况，如果在此期间提交了当前还没处理完成的消息的offset，会造成消费丢失：\n\n  ![upload successful](\\blog\\images\\pasted-25.png)\n\nkafka提供了`手动commit`和`自动commit`两种提交策略：\n\n1. **自动提交**：将`enable.auto.commit`设置为true并且将`auto.commit.interval.ms`设置为大于0的时间，即可开启自动提交，每次调用`consumer.poll()`方法时都会检查是否达到提交间隔时间，并将上一次拉取消息中最大的一个offset信息发送给集群中的`__consumer_offsets`这个topic\n\n   缺点：自动提交不需要用户手动commit，因此提交过程不可控\n\n2. **手动提交**：手动提交又分为`commitSync()`和`commitAsync()`两种方式，该模式下`enable.auto.commit`必须为false，consumer api任何情况都不会自动帮用户提交offset，一切由用户主动控制：\n\n   `commitSync()`：同步提交，直接提交之前拉取的最新的offset，用户可自己保证在消息处理完毕后直接提交当前offset，缺点是整个提交过程是阻塞的，并且提交失败会进行重试\n\n   `commitAsync()`：异步提交，同样也是提交之前拉取的最新的offset，改善了同步提交过程会产生阻塞的缺点，支持回调函数，提交过程不进行重试\n\n比较好的手动提交方式是`同步和异步组合提交`：\n\n```java\ntry {\n    while (true) {\n        // poll拉取消息，最长等待时间10秒\n        try {\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                System.out.println();\n            });\n        } finally {\n            // 消费过程未报错则执行异步提交，速度更快，不阻塞consumer线程\n            consumer.commitAsync((offsets, exception) -> {\n                // offsets中的`offsetAndMetadata`包含本次提交后希望处理的下一个offset\n                print(\"本次提交后希望处理的下一个offset:\", offsets);\n                if (exception != null) {\n                    exception.printStackTrace();\n                }\n            });\n        }\n    }\n} catch (Exception e) {\n    e.printStackTrace();\n} finally {\n    try {\n        // 消费过程报错的话就尝试同步提交，防止未提交offset造成风险\n        consumer.commitSync();\n    } finally {\n        consumer.close();\n    }\n}\n```\n### kafka consumer消费时指定offset\n\n有些时候我们并不想按照默认情况读取最新offset位置的消息，kafka对此提供了查找特定offset的api，可以实现向后回退几个消息或者向前跳过几个消息：\n\n```java\nprivate static void runConsumerSeekOffset(Map<String, Object> consumerConfig, long specificOffset) {\n    Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n    consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n    try {\n        // 使用seek()方法将consumer的offset重置到指定位置\n        consumer.assignment().forEach(topicPartition -> {\n            consumer.seek(topicPartition, specificOffset);\n        });\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                System.out.println();\n            });\n        }\n    } finally {\n        consumer.close();\n    }\n}\n```\n\n### kafka consumer rebalance监听器\n\nconsumer和kafka的Coodinator交互的心跳心中包含是否需要rebalance信息，如果需要rebalance则停止拉取数据并直接提交offset，rebalance完成后consumer有可能失去对当前分配的partition的消费权\n\nkafka提供了rebalance监听器用于让用户设置在rebalance发生后需要做的一些资源回收的操作，并且可以在监听器中手动提交当前已经处理完成的消息offset，以下是相对比较保险不会造成消费丢失的一种提交策略：\n\n```java\nprivate static void runConsumerRebanlanceListener(Map<String, Object> consumerConfig) {\n    Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n    // 已处理的消息的offset暂存区\n    final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n    consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"), new ConsumerRebalanceListener() {\n        @Override\n        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            System.out.println(\"发生rebalance！\");\n            System.out.println(\"当前分配的partitions：\" + partitions);\n\t\t   // 将已经处理完成的offset进行同步提交\n            consumer.commitSync(currentOffsets);\n        }\n        @Override\n        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n            System.out.println(\"rebalance完成！\");\n            System.out.println(\"rebalance后分配的partitions：\" + partitions);\n        }\n    });\n    try {\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(100);\n            currentOffsets.clear();\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                // 当前处理完成，待提交的offset，存入暂存区\n                currentOffsets.put(\n                    new TopicPartition(record.topic(), record.partition()),\n                    new OffsetAndMetadata(record.offset() + 1, null));\n                System.out.println();\n            });\n            // 每隔批次消息处理完成后异步提交\n            consumer.commitAsync(currentOffsets, null);\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n    } finally {\n        try {\n            // 出错后同步提交\n            consumer.commitSync(currentOffsets);\n        } finally {\n            consumer.close();\n        }\n    }\n}\n```","source":"_posts/kafka学习笔记（3）——-消费者-consumer.md","raw":"title: kafka学习笔记（3）—— 消费者 consumer\nauthor: 天渊\ntags:\n  - Kafka\n  - 大数据\ncategories: []\ndate: 2019-03-18 13:21:00\n---\nkafka-consumer作为kafka消息队列的消费者端，通过接收topic的消息进行处理然后输出到下游数据源（数据库，文件系统，或者是另外的消息队列）<!--more-->\n\nkafka-consumer最大的特点是，遵循发布/订阅模型，多个consumer共同订阅同一个topic，通过offset对消息的消费位置进行标记：\n\n![upload successful](\\blog\\images\\pasted-7.png)\n\n**consumer的offset保存在哪里？**：老版本的kafka，offset保存在zookeeper中，但由于consumer的offset数据太过庞大，不适合存放在zookeeper，因此新版本的kafka单独维护了一个保存offset的topic：`__consumer_offsets`，以group-id，topic以及partition做为组合Key对每个consumer的offset进行检索\n\n### consumer消费数据\n\nconsumer以consumer-group（消费者组）为单位向kafka订阅topic并消费数据：\n\n![upload successful](\\blog\\images\\pasted-8.png)\n\nkafka consumer抓取数据基本流程图：\n\n![upload successful](\\blog\\images\\pasted-9.png)\n\n几点特性：\n\n- **partition主从热备**：消息的读/写工作全部交给leader partition来完成，slave partition主动与leader同步，通过LEO和leader epoch等信息同步数据，leader通过ISR列表保存当前存活的slave状态\n- **可靠性**：只有当ISR列表中所有副本都成功收到的消息才能提供给consumer，可以提供给consumer的这部分消息由`high water`来进行控制 (设置`replica.lag.time.max.ms`参数可保证副本同步消息的时间上限)\n- **零拷贝**：kafka通过零拷贝的方式将数据返回给consumer\n\n#### consumer的java api\n\n使用kafka-consumer高级api进行消费操作：\n\n1. 创建单个consumer进行消费：\n\n   ```java\n   public class ConsumerGroupTest {\n       public static void main(String[] args) {\n           Map<String, Object> consumerConfig = new HashMap<>();\n           consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\n           consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n           consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n           consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, \"test-group-1\");\n           runConsumer(consumerConfig);\n       }\n       private static void runConsumer(Map<String, Object> consumerConfig) {\n           Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n           consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n           try {\n               while (true) {\n                   // poll拉取消息，最长等待时间10秒\n                   ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n                   records.forEach(record -> {\n                       printRecord(record);\n                       print(\"当前consumer实例：\", consumer.toString());\n                       System.out.println();\n                   });\n               }\n           } finally {\n               consumer.close();\n           }\n       }\n       private static void printRecord(ConsumerRecord<String, String> record) {\n           System.out.println(\"当前record：====> \"\n                              + \" topic:\" + record.topic()\n                              + \" partition:\" + record.partition()\n                              + \" offset:\" + record.offset()\n                              + \" value:\" + record.value()\n                             );\n       }\n       private static void print(String name, Object value) {\n           System.out.println(\"KafkaConsumer ====> \" + name + \" is \" + \"{ \" + value + \" }\");\n       }\n   }\n   ```\n\n   单个consumer实例可以订阅多个topic，也可以通过正则表达式订阅topic：\n\n   ```java\n   // 订阅多个topic\n   consumer.subscribe(Arrays.asList(\"test-topic-1\", \"test-topic-2\"));\n   // 通过正则表达式订阅多个topic\n   consumer.subscribe(Pattern.compile(\"test.*\"));\n   ```\n\n2. 创建消费者组订阅包含多个partition的topic：\n\n   创建一个叫做“test-group-1”的消费组，订阅“test_multi_par_topic_1” topic， 该topic包含三个partition：\n\n   ```java\n   public static void main(String[] args) {\n       Map<String, Object> consumerConfig = new HashMap<>();\n       consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\n       consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n       consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n       consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, \"test-group-1\");\n       runConsumerManualCommit(consumerConfig);\n   }\n   \n   private static void runConsumer(Map<String, Object> consumerConfig) {\n       Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n       consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n       try {\n           while (true) {\n               // poll拉取消息，最长等待时间10秒\n               ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n               records.forEach(record -> {\n                   printRecord(record);\n                   print(\"当前consumer实例：\", consumer.toString());\n                   System.out.println();\n               });\n           }\n       } finally {\n           consumer.close();\n       }\n   }\n   ```\n   \n   首先只启动一个实例，可以看到，当前consumer实例消费了全部的三个partition的数据：\n   \n![upload successful](\\blog\\images\\pasted-10.png)\n\n    \n   向该组加入一个consumer实例，可以看到，之前`@48322fe3`这个consumer实例只分到了partition2，而新加入的`@365ae362`这个实例分到了0和1：：\n   \n![upload successful](\\blog\\images\\pasted-11.png) \n\n![upload successful](\\blog\\images\\pasted-12.png)\n\n继续加consumer实例进来，最早的元老实例1，分到了partition2：\n \n![upload successful](\\blog\\images\\pasted-13.png)\n\t\n之后加入的实例2分到了partition0：\n\n![upload successful](\\blog\\images\\pasted-14.png)\n\n最新加入的实例3 `@345dd14f`，分到了parition1：\n\n![upload successful](\\blog\\images\\pasted-15.png)\n\t\n接下来将元老实例1`@48322fe3`停掉，观察实例2和实例3 （注意观察实例1的最后消费offset），实例1最后消费情况，消费完offset=85就挂掉了：\n   \n![upload successful](\\blog\\images\\pasted-16.png)\n   \n观察实例2可以发现，实例2除了继续消费partition0，还认领了之前实例3消费的partition1的任务\n  \n![upload successful](\\blog\\images\\pasted-17.png)\n\n观察实例3，消费partition1直到offset=91的位置，发生了rebalance，之后partition1的任务分派给了上面实例2，自己分到了因实例1挂掉而无人认领的partition2，并且从实例1最后消费位置即offset=86开始消费\n   \n   \n![upload successful](\\blog\\images\\pasted-18.png)\n\n### consumer-group\n\n单个consumer-group即可以只消费一个topic的消息，也可以同时消费多个topic的消息\n\n#### consumer-group订阅topic\n\n单个consumer-group在订阅某个topic时，如果不特殊指定订阅某个partition，kafka将启用`Coordinator`对consumer-group内的consumer实例进行负载均衡，有以下特点：\n\n1. 组内的一个consumer实例消费全部的partition：\n\n\t![upload successful](\\blog\\images\\pasted-19.png)\n\n2. 组内继续加入consumer实例，一同消费多个partition，消费过程与另外的consumer完全隔离不受影响：\n\n\t![upload successful](\\blog\\images\\pasted-20.png)\n\n3. 组内继续加入consumer，每个consumer实例单独消费一个partition，此时consumer数目与partition数目一致：\n\n\t![upload successful](\\blog\\images\\pasted-21.png)\n\n4. 继续加入consumer，数目超过partition数目，此时多出来的consumer将分不到partition进行消费：\n\n\t![upload successful](\\blog\\images\\pasted-22.png)\n\n如上可以看出，当单个consumer-group订阅topic进行消费时，kafka的Coordinator保证topic的数据能够被均匀分派到组内的各个消费者上，并且topic中的**单个partition只能被单个consumer-group中的其中一个consumer消费**\n\n不过，如果有其他consumer-group的consumer参与消费这个topic，将与之前的consumer-group隔离，Coordinator将单独为这个新的组进行负载均衡：\n\n   ![upload successful](\\blog\\images\\pasted-23.png)\n    \n#### consumer分区再平衡\n\n- 每当一个consumer-group有新成员加入，并且和老成员一起消费同一个topic的时候，Coordinator都将进行分区再平衡（`rebalance`），为了平衡消费能力，老成员消费的partition有可能会被分派给新加入的consumer\n- 当有consumer实例挂掉时，之前分派给他的partition将会重新分派给剩余还活着的consumer（conusmer实例通过发送心跳让kafka集群知道他还活着）\n- 订阅的topic新加入partition也会触发rebalance\n- 通过正则表达式订阅某个类型的topic，当新加入该类型的topic时，也会发生rebalance\n\n#### consumer rebalance的大致流程：\n\n1. Topic/Partition的改变或者新Consumer的加入或者已有Consumer停止，将触发Coordinator注册在Zookeeper上的watch，Coordinator收到通知准备发起Rebalance操作。\n2. Coordinator通过在HeartbeatResponse中返回IllegalGeneration错误码通知各个consumer发起Rebalance操作。\n3. 存活的Consumer向Coordinator发送JoinGroupRequest\n4. Coordinator在Zookeeper中增加Group的Generation ID并将新的Partition分配情况写入Zookeeper\n5. Coordinator向存活的consumer发送JoinGroupResponse\n6. 存活的consumer收到JoinGroupResponse后重新启动新一轮消费\n\n### consumer配置\n\n除了上述`bootstrap.servers`,`key.deserializer`,`value.deserializer`,`group.id`这几个配置项必须进行配置之外，其他的配置项均为可选项：\n\n```properties\n# 单次拉取消息的最小字节数，如果该值不为0，在consumer拉取消息时，如果可消费数据达不到该值，broker将等待一段时间(fetch.max.wait.ms)直到有足够数据或者等待超时，再返回给consumer\nfetch.min.bytes=1024\n# 单次拉取消息的最长等待时间\nfetch.max.wait.ms=500\n# 指定consumer单次poll()从分区中拉取的最大字节数，默认1MB\nmax.partition.fetch.bytes=1048576\n# kafka集群判断consumer连接中断的最长等待时间，默认10秒，如果consumer超过该时间没有发送心跳，则会判断为死亡进而触发rebalance\nsession.timeout.ms=10000\n# 心跳间隔时间，建议不超过session.timeout.ms的1/3\nheartbeat.interval.ms=3000\n# 是否开启自动提交offset，默认是开启的，如果开启，将每隔auto.commit.interval.ms的时间将消费完成但未提交offset的consumer统一向kafka集群提交一次（风险：当consumer消费完成但未提交offset就挂掉时，重启后将造成消息重复消费）\nenable.auto.commit=true\n# 自动统一提交offset的时间间隔\nauto.commit.interval.ms=5000\n# 开始消费的consumer-group（或者该consumer掉线已久，已经丢失有效的offset信息）初始化offset的策略，\n# latest：默认值，默认从最新的有效offset开始消费\n# earliest：从最早的有效offset开始消费\n# none：如果kafka没有保存该consumer-group的offset信息则直接抛异常\nauto.offset.reset=[latest, earliest, none]\n# 分区分派策略类，kafka-clients自带的分区策略有Range和RoundRobin\n# 必须是PartitionAssignor的实现类\npartition.assignment.strategy=class org.apache.kafka.clients.consumer.RangeAssignor\n# 单次poll()拉取的消息数量最大值，默认是500个\nmax.poll.records=500\n```\n\n### kafka consumer的offset提交策略\n\nconsumer每次消费数据完成后需要提交offset给集群以更新自己的消费状态，提交过程非常重要，直接关系到消费过程的稳定性，提交异常的话可能导致以下两种后果：\n\n- 重复消费：一般发生在还没来得及提交offset即被中断消费的情况\n  \n  ![upload successful](\\blog\\images\\pasted-24.png)\n\n- 丢失消费：一般发生在消费过程有耗时操作的情况，如果在此期间提交了当前还没处理完成的消息的offset，会造成消费丢失：\n\n  ![upload successful](\\blog\\images\\pasted-25.png)\n\nkafka提供了`手动commit`和`自动commit`两种提交策略：\n\n1. **自动提交**：将`enable.auto.commit`设置为true并且将`auto.commit.interval.ms`设置为大于0的时间，即可开启自动提交，每次调用`consumer.poll()`方法时都会检查是否达到提交间隔时间，并将上一次拉取消息中最大的一个offset信息发送给集群中的`__consumer_offsets`这个topic\n\n   缺点：自动提交不需要用户手动commit，因此提交过程不可控\n\n2. **手动提交**：手动提交又分为`commitSync()`和`commitAsync()`两种方式，该模式下`enable.auto.commit`必须为false，consumer api任何情况都不会自动帮用户提交offset，一切由用户主动控制：\n\n   `commitSync()`：同步提交，直接提交之前拉取的最新的offset，用户可自己保证在消息处理完毕后直接提交当前offset，缺点是整个提交过程是阻塞的，并且提交失败会进行重试\n\n   `commitAsync()`：异步提交，同样也是提交之前拉取的最新的offset，改善了同步提交过程会产生阻塞的缺点，支持回调函数，提交过程不进行重试\n\n比较好的手动提交方式是`同步和异步组合提交`：\n\n```java\ntry {\n    while (true) {\n        // poll拉取消息，最长等待时间10秒\n        try {\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                System.out.println();\n            });\n        } finally {\n            // 消费过程未报错则执行异步提交，速度更快，不阻塞consumer线程\n            consumer.commitAsync((offsets, exception) -> {\n                // offsets中的`offsetAndMetadata`包含本次提交后希望处理的下一个offset\n                print(\"本次提交后希望处理的下一个offset:\", offsets);\n                if (exception != null) {\n                    exception.printStackTrace();\n                }\n            });\n        }\n    }\n} catch (Exception e) {\n    e.printStackTrace();\n} finally {\n    try {\n        // 消费过程报错的话就尝试同步提交，防止未提交offset造成风险\n        consumer.commitSync();\n    } finally {\n        consumer.close();\n    }\n}\n```\n### kafka consumer消费时指定offset\n\n有些时候我们并不想按照默认情况读取最新offset位置的消息，kafka对此提供了查找特定offset的api，可以实现向后回退几个消息或者向前跳过几个消息：\n\n```java\nprivate static void runConsumerSeekOffset(Map<String, Object> consumerConfig, long specificOffset) {\n    Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n    consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"));\n    try {\n        // 使用seek()方法将consumer的offset重置到指定位置\n        consumer.assignment().forEach(topicPartition -> {\n            consumer.seek(topicPartition, specificOffset);\n        });\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                System.out.println();\n            });\n        }\n    } finally {\n        consumer.close();\n    }\n}\n```\n\n### kafka consumer rebalance监听器\n\nconsumer和kafka的Coodinator交互的心跳心中包含是否需要rebalance信息，如果需要rebalance则停止拉取数据并直接提交offset，rebalance完成后consumer有可能失去对当前分配的partition的消费权\n\nkafka提供了rebalance监听器用于让用户设置在rebalance发生后需要做的一些资源回收的操作，并且可以在监听器中手动提交当前已经处理完成的消息offset，以下是相对比较保险不会造成消费丢失的一种提交策略：\n\n```java\nprivate static void runConsumerRebanlanceListener(Map<String, Object> consumerConfig) {\n    Consumer<String, String> consumer = new KafkaConsumer<>(consumerConfig);\n    // 已处理的消息的offset暂存区\n    final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n    consumer.subscribe(Collections.singleton(\"test_multi_par_topic_1\"), new ConsumerRebalanceListener() {\n        @Override\n        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            System.out.println(\"发生rebalance！\");\n            System.out.println(\"当前分配的partitions：\" + partitions);\n\t\t   // 将已经处理完成的offset进行同步提交\n            consumer.commitSync(currentOffsets);\n        }\n        @Override\n        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n            System.out.println(\"rebalance完成！\");\n            System.out.println(\"rebalance后分配的partitions：\" + partitions);\n        }\n    });\n    try {\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(100);\n            currentOffsets.clear();\n            records.forEach(record -> {\n                printRecord(record);\n                print(\"当前consumer实例：\", consumer.toString());\n                // 当前处理完成，待提交的offset，存入暂存区\n                currentOffsets.put(\n                    new TopicPartition(record.topic(), record.partition()),\n                    new OffsetAndMetadata(record.offset() + 1, null));\n                System.out.println();\n            });\n            // 每隔批次消息处理完成后异步提交\n            consumer.commitAsync(currentOffsets, null);\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n    } finally {\n        try {\n            // 出错后同步提交\n            consumer.commitSync(currentOffsets);\n        } finally {\n            consumer.close();\n        }\n    }\n}\n```","slug":"kafka学习笔记（3）——-消费者-consumer","published":1,"updated":"2019-03-19T13:30:58.461Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3w0011g0qrdf4483db","content":"<p>kafka-consumer作为kafka消息队列的消费者端，通过接收topic的消息进行处理然后输出到下游数据源（数据库，文件系统，或者是另外的消息队列）<a id=\"more\"></a></p>\n<p>kafka-consumer最大的特点是，遵循发布/订阅模型，多个consumer共同订阅同一个topic，通过offset对消息的消费位置进行标记：</p>\n<p><img src=\"\\blog\\images\\pasted-7.png\" alt=\"upload successful\"></p>\n<p><strong>consumer的offset保存在哪里？</strong>：老版本的kafka，offset保存在zookeeper中，但由于consumer的offset数据太过庞大，不适合存放在zookeeper，因此新版本的kafka单独维护了一个保存offset的topic：<code>__consumer_offsets</code>，以group-id，topic以及partition做为组合Key对每个consumer的offset进行检索</p>\n<h3 id=\"consumer消费数据\"><a href=\"#consumer消费数据\" class=\"headerlink\" title=\"consumer消费数据\"></a>consumer消费数据</h3><p>consumer以consumer-group（消费者组）为单位向kafka订阅topic并消费数据：</p>\n<p><img src=\"\\blog\\images\\pasted-8.png\" alt=\"upload successful\"></p>\n<p>kafka consumer抓取数据基本流程图：</p>\n<p><img src=\"\\blog\\images\\pasted-9.png\" alt=\"upload successful\"></p>\n<p>几点特性：</p>\n<ul>\n<li><strong>partition主从热备</strong>：消息的读/写工作全部交给leader partition来完成，slave partition主动与leader同步，通过LEO和leader epoch等信息同步数据，leader通过ISR列表保存当前存活的slave状态</li>\n<li><strong>可靠性</strong>：只有当ISR列表中所有副本都成功收到的消息才能提供给consumer，可以提供给consumer的这部分消息由<code>high water</code>来进行控制 (设置<code>replica.lag.time.max.ms</code>参数可保证副本同步消息的时间上限)</li>\n<li><strong>零拷贝</strong>：kafka通过零拷贝的方式将数据返回给consumer</li>\n</ul>\n<h4 id=\"consumer的java-api\"><a href=\"#consumer的java-api\" class=\"headerlink\" title=\"consumer的java api\"></a>consumer的java api</h4><p>使用kafka-consumer高级api进行消费操作：</p>\n<ol>\n<li><p>创建单个consumer进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ConsumerGroupTest</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; consumerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, <span class=\"string\">\"test-group-1\"</span>);</span><br><span class=\"line\">        runConsumer(consumerConfig);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumer</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">        Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">        consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">                records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                    printRecord(record);</span><br><span class=\"line\">                    print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                    System.out.println();</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            consumer.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">printRecord</span><span class=\"params\">(ConsumerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"当前record：====&gt; \"</span></span><br><span class=\"line\">                           + <span class=\"string\">\" topic:\"</span> + record.topic()</span><br><span class=\"line\">                           + <span class=\"string\">\" partition:\"</span> + record.partition()</span><br><span class=\"line\">                           + <span class=\"string\">\" offset:\"</span> + record.offset()</span><br><span class=\"line\">                           + <span class=\"string\">\" value:\"</span> + record.value()</span><br><span class=\"line\">                          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">print</span><span class=\"params\">(String name, Object value)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"KafkaConsumer ====&gt; \"</span> + name + <span class=\"string\">\" is \"</span> + <span class=\"string\">\"&#123; \"</span> + value + <span class=\"string\">\" &#125;\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>单个consumer实例可以订阅多个topic，也可以通过正则表达式订阅topic：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 订阅多个topic</span></span><br><span class=\"line\">consumer.subscribe(Arrays.asList(<span class=\"string\">\"test-topic-1\"</span>, <span class=\"string\">\"test-topic-2\"</span>));</span><br><span class=\"line\"><span class=\"comment\">// 通过正则表达式订阅多个topic</span></span><br><span class=\"line\">consumer.subscribe(Pattern.compile(<span class=\"string\">\"test.*\"</span>));</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建消费者组订阅包含多个partition的topic：</p>\n<p>创建一个叫做“test-group-1”的消费组，订阅“test_multi_par_topic_1” topic， 该topic包含三个partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Map&lt;String, Object&gt; consumerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, <span class=\"string\">\"test-group-1\"</span>);</span><br><span class=\"line\">    runConsumerManualCommit(consumerConfig);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumer</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>   首先只启动一个实例，可以看到，当前consumer实例消费了全部的三个partition的数据：</p>\n<p><img src=\"\\blog\\images\\pasted-10.png\" alt=\"upload successful\"></p>\n<p>   向该组加入一个consumer实例，可以看到，之前<code>@48322fe3</code>这个consumer实例只分到了partition2，而新加入的<code>@365ae362</code>这个实例分到了0和1：：</p>\n<p><img src=\"\\blog\\images\\pasted-11.png\" alt=\"upload successful\"> </p>\n<p><img src=\"\\blog\\images\\pasted-12.png\" alt=\"upload successful\"></p>\n<p>继续加consumer实例进来，最早的元老实例1，分到了partition2：</p>\n<p><img src=\"\\blog\\images\\pasted-13.png\" alt=\"upload successful\"></p>\n<p>之后加入的实例2分到了partition0：</p>\n<p><img src=\"\\blog\\images\\pasted-14.png\" alt=\"upload successful\"></p>\n<p>最新加入的实例3 <code>@345dd14f</code>，分到了parition1：</p>\n<p><img src=\"\\blog\\images\\pasted-15.png\" alt=\"upload successful\"></p>\n<p>接下来将元老实例1<code>@48322fe3</code>停掉，观察实例2和实例3 （注意观察实例1的最后消费offset），实例1最后消费情况，消费完offset=85就挂掉了：</p>\n<p><img src=\"\\blog\\images\\pasted-16.png\" alt=\"upload successful\"></p>\n<p>观察实例2可以发现，实例2除了继续消费partition0，还认领了之前实例3消费的partition1的任务</p>\n<p><img src=\"\\blog\\images\\pasted-17.png\" alt=\"upload successful\"></p>\n<p>观察实例3，消费partition1直到offset=91的位置，发生了rebalance，之后partition1的任务分派给了上面实例2，自己分到了因实例1挂掉而无人认领的partition2，并且从实例1最后消费位置即offset=86开始消费</p>\n<p><img src=\"\\blog\\images\\pasted-18.png\" alt=\"upload successful\"></p>\n<h3 id=\"consumer-group\"><a href=\"#consumer-group\" class=\"headerlink\" title=\"consumer-group\"></a>consumer-group</h3><p>单个consumer-group即可以只消费一个topic的消息，也可以同时消费多个topic的消息</p>\n<h4 id=\"consumer-group订阅topic\"><a href=\"#consumer-group订阅topic\" class=\"headerlink\" title=\"consumer-group订阅topic\"></a>consumer-group订阅topic</h4><p>单个consumer-group在订阅某个topic时，如果不特殊指定订阅某个partition，kafka将启用<code>Coordinator</code>对consumer-group内的consumer实例进行负载均衡，有以下特点：</p>\n<ol>\n<li><p>组内的一个consumer实例消费全部的partition：</p>\n<p> <img src=\"\\blog\\images\\pasted-19.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>组内继续加入consumer实例，一同消费多个partition，消费过程与另外的consumer完全隔离不受影响：</p>\n<p> <img src=\"\\blog\\images\\pasted-20.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>组内继续加入consumer，每个consumer实例单独消费一个partition，此时consumer数目与partition数目一致：</p>\n<p> <img src=\"\\blog\\images\\pasted-21.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>继续加入consumer，数目超过partition数目，此时多出来的consumer将分不到partition进行消费：</p>\n<p> <img src=\"\\blog\\images\\pasted-22.png\" alt=\"upload successful\"></p>\n</li>\n</ol>\n<p>如上可以看出，当单个consumer-group订阅topic进行消费时，kafka的Coordinator保证topic的数据能够被均匀分派到组内的各个消费者上，并且topic中的<strong>单个partition只能被单个consumer-group中的其中一个consumer消费</strong></p>\n<p>不过，如果有其他consumer-group的consumer参与消费这个topic，将与之前的consumer-group隔离，Coordinator将单独为这个新的组进行负载均衡：</p>\n<p>   <img src=\"\\blog\\images\\pasted-23.png\" alt=\"upload successful\"></p>\n<h4 id=\"consumer分区再平衡\"><a href=\"#consumer分区再平衡\" class=\"headerlink\" title=\"consumer分区再平衡\"></a>consumer分区再平衡</h4><ul>\n<li>每当一个consumer-group有新成员加入，并且和老成员一起消费同一个topic的时候，Coordinator都将进行分区再平衡（<code>rebalance</code>），为了平衡消费能力，老成员消费的partition有可能会被分派给新加入的consumer</li>\n<li>当有consumer实例挂掉时，之前分派给他的partition将会重新分派给剩余还活着的consumer（conusmer实例通过发送心跳让kafka集群知道他还活着）</li>\n<li>订阅的topic新加入partition也会触发rebalance</li>\n<li>通过正则表达式订阅某个类型的topic，当新加入该类型的topic时，也会发生rebalance</li>\n</ul>\n<h4 id=\"consumer-rebalance的大致流程：\"><a href=\"#consumer-rebalance的大致流程：\" class=\"headerlink\" title=\"consumer rebalance的大致流程：\"></a>consumer rebalance的大致流程：</h4><ol>\n<li>Topic/Partition的改变或者新Consumer的加入或者已有Consumer停止，将触发Coordinator注册在Zookeeper上的watch，Coordinator收到通知准备发起Rebalance操作。</li>\n<li>Coordinator通过在HeartbeatResponse中返回IllegalGeneration错误码通知各个consumer发起Rebalance操作。</li>\n<li>存活的Consumer向Coordinator发送JoinGroupRequest</li>\n<li>Coordinator在Zookeeper中增加Group的Generation ID并将新的Partition分配情况写入Zookeeper</li>\n<li>Coordinator向存活的consumer发送JoinGroupResponse</li>\n<li>存活的consumer收到JoinGroupResponse后重新启动新一轮消费</li>\n</ol>\n<h3 id=\"consumer配置\"><a href=\"#consumer配置\" class=\"headerlink\" title=\"consumer配置\"></a>consumer配置</h3><p>除了上述<code>bootstrap.servers</code>,<code>key.deserializer</code>,<code>value.deserializer</code>,<code>group.id</code>这几个配置项必须进行配置之外，其他的配置项均为可选项：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 单次拉取消息的最小字节数，如果该值不为0，在consumer拉取消息时，如果可消费数据达不到该值，broker将等待一段时间(fetch.max.wait.ms)直到有足够数据或者等待超时，再返回给consumer</span><br><span class=\"line\">fetch.min.bytes=1024</span><br><span class=\"line\"># 单次拉取消息的最长等待时间</span><br><span class=\"line\">fetch.max.wait.ms=500</span><br><span class=\"line\"># 指定consumer单次poll()从分区中拉取的最大字节数，默认1MB</span><br><span class=\"line\">max.partition.fetch.bytes=1048576</span><br><span class=\"line\"># kafka集群判断consumer连接中断的最长等待时间，默认10秒，如果consumer超过该时间没有发送心跳，则会判断为死亡进而触发rebalance</span><br><span class=\"line\">session.timeout.ms=10000</span><br><span class=\"line\"># 心跳间隔时间，建议不超过session.timeout.ms的1/3</span><br><span class=\"line\">heartbeat.interval.ms=3000</span><br><span class=\"line\"># 是否开启自动提交offset，默认是开启的，如果开启，将每隔auto.commit.interval.ms的时间将消费完成但未提交offset的consumer统一向kafka集群提交一次（风险：当consumer消费完成但未提交offset就挂掉时，重启后将造成消息重复消费）</span><br><span class=\"line\">enable.auto.commit=true</span><br><span class=\"line\"># 自动统一提交offset的时间间隔</span><br><span class=\"line\">auto.commit.interval.ms=5000</span><br><span class=\"line\"># 开始消费的consumer-group（或者该consumer掉线已久，已经丢失有效的offset信息）初始化offset的策略，</span><br><span class=\"line\"># latest：默认值，默认从最新的有效offset开始消费</span><br><span class=\"line\"># earliest：从最早的有效offset开始消费</span><br><span class=\"line\"># none：如果kafka没有保存该consumer-group的offset信息则直接抛异常</span><br><span class=\"line\">auto.offset.reset=[latest, earliest, none]</span><br><span class=\"line\"># 分区分派策略类，kafka-clients自带的分区策略有Range和RoundRobin</span><br><span class=\"line\"># 必须是PartitionAssignor的实现类</span><br><span class=\"line\">partition.assignment.strategy=class org.apache.kafka.clients.consumer.RangeAssignor</span><br><span class=\"line\"># 单次poll()拉取的消息数量最大值，默认是500个</span><br><span class=\"line\">max.poll.records=500</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer的offset提交策略\"><a href=\"#kafka-consumer的offset提交策略\" class=\"headerlink\" title=\"kafka consumer的offset提交策略\"></a>kafka consumer的offset提交策略</h3><p>consumer每次消费数据完成后需要提交offset给集群以更新自己的消费状态，提交过程非常重要，直接关系到消费过程的稳定性，提交异常的话可能导致以下两种后果：</p>\n<ul>\n<li><p>重复消费：一般发生在还没来得及提交offset即被中断消费的情况</p>\n<p><img src=\"\\blog\\images\\pasted-24.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>丢失消费：一般发生在消费过程有耗时操作的情况，如果在此期间提交了当前还没处理完成的消息的offset，会造成消费丢失：</p>\n<p><img src=\"\\blog\\images\\pasted-25.png\" alt=\"upload successful\"></p>\n</li>\n</ul>\n<p>kafka提供了<code>手动commit</code>和<code>自动commit</code>两种提交策略：</p>\n<ol>\n<li><p><strong>自动提交</strong>：将<code>enable.auto.commit</code>设置为true并且将<code>auto.commit.interval.ms</code>设置为大于0的时间，即可开启自动提交，每次调用<code>consumer.poll()</code>方法时都会检查是否达到提交间隔时间，并将上一次拉取消息中最大的一个offset信息发送给集群中的<code>__consumer_offsets</code>这个topic</p>\n<p>缺点：自动提交不需要用户手动commit，因此提交过程不可控</p>\n</li>\n<li><p><strong>手动提交</strong>：手动提交又分为<code>commitSync()</code>和<code>commitAsync()</code>两种方式，该模式下<code>enable.auto.commit</code>必须为false，consumer api任何情况都不会自动帮用户提交offset，一切由用户主动控制：</p>\n<p><code>commitSync()</code>：同步提交，直接提交之前拉取的最新的offset，用户可自己保证在消息处理完毕后直接提交当前offset，缺点是整个提交过程是阻塞的，并且提交失败会进行重试</p>\n<p><code>commitAsync()</code>：异步提交，同样也是提交之前拉取的最新的offset，改善了同步提交过程会产生阻塞的缺点，支持回调函数，提交过程不进行重试</p>\n</li>\n</ol>\n<p>比较好的手动提交方式是<code>同步和异步组合提交</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 消费过程未报错则执行异步提交，速度更快，不阻塞consumer线程</span></span><br><span class=\"line\">            consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class=\"line\">                <span class=\"comment\">// offsets中的`offsetAndMetadata`包含本次提交后希望处理的下一个offset</span></span><br><span class=\"line\">                print(<span class=\"string\">\"本次提交后希望处理的下一个offset:\"</span>, offsets);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (exception != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    exception.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">    e.printStackTrace();</span><br><span class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 消费过程报错的话就尝试同步提交，防止未提交offset造成风险</span></span><br><span class=\"line\">        consumer.commitSync();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer消费时指定offset\"><a href=\"#kafka-consumer消费时指定offset\" class=\"headerlink\" title=\"kafka consumer消费时指定offset\"></a>kafka consumer消费时指定offset</h3><p>有些时候我们并不想按照默认情况读取最新offset位置的消息，kafka对此提供了查找特定offset的api，可以实现向后回退几个消息或者向前跳过几个消息：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumerSeekOffset</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig, <span class=\"keyword\">long</span> specificOffset)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 使用seek()方法将consumer的offset重置到指定位置</span></span><br><span class=\"line\">        consumer.assignment().forEach(topicPartition -&gt; &#123;</span><br><span class=\"line\">            consumer.seek(topicPartition, specificOffset);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer-rebalance监听器\"><a href=\"#kafka-consumer-rebalance监听器\" class=\"headerlink\" title=\"kafka consumer rebalance监听器\"></a>kafka consumer rebalance监听器</h3><p>consumer和kafka的Coodinator交互的心跳心中包含是否需要rebalance信息，如果需要rebalance则停止拉取数据并直接提交offset，rebalance完成后consumer有可能失去对当前分配的partition的消费权</p>\n<p>kafka提供了rebalance监听器用于让用户设置在rebalance发生后需要做的一些资源回收的操作，并且可以在监听器中手动提交当前已经处理完成的消息offset，以下是相对比较保险不会造成消费丢失的一种提交策略：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumerRebanlanceListener</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    <span class=\"comment\">// 已处理的消息的offset暂存区</span></span><br><span class=\"line\">    <span class=\"keyword\">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>), <span class=\"keyword\">new</span> ConsumerRebalanceListener() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPartitionsRevoked</span><span class=\"params\">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"发生rebalance！\"</span>);</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"当前分配的partitions：\"</span> + partitions);</span><br><span class=\"line\">\t\t   <span class=\"comment\">// 将已经处理完成的offset进行同步提交</span></span><br><span class=\"line\">            consumer.commitSync(currentOffsets);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPartitionsAssigned</span><span class=\"params\">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"rebalance完成！\"</span>);</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"rebalance后分配的partitions：\"</span> + partitions);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class=\"number\">100</span>);</span><br><span class=\"line\">            currentOffsets.clear();</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                <span class=\"comment\">// 当前处理完成，待提交的offset，存入暂存区</span></span><br><span class=\"line\">                currentOffsets.put(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> OffsetAndMetadata(record.offset() + <span class=\"number\">1</span>, <span class=\"keyword\">null</span>));</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">            <span class=\"comment\">// 每隔批次消息处理完成后异步提交</span></span><br><span class=\"line\">            consumer.commitAsync(currentOffsets, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 出错后同步提交</span></span><br><span class=\"line\">            consumer.commitSync(currentOffsets);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            consumer.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>kafka-consumer作为kafka消息队列的消费者端，通过接收topic的消息进行处理然后输出到下游数据源（数据库，文件系统，或者是另外的消息队列）","more":"</p>\n<p>kafka-consumer最大的特点是，遵循发布/订阅模型，多个consumer共同订阅同一个topic，通过offset对消息的消费位置进行标记：</p>\n<p><img src=\"\\blog\\images\\pasted-7.png\" alt=\"upload successful\"></p>\n<p><strong>consumer的offset保存在哪里？</strong>：老版本的kafka，offset保存在zookeeper中，但由于consumer的offset数据太过庞大，不适合存放在zookeeper，因此新版本的kafka单独维护了一个保存offset的topic：<code>__consumer_offsets</code>，以group-id，topic以及partition做为组合Key对每个consumer的offset进行检索</p>\n<h3 id=\"consumer消费数据\"><a href=\"#consumer消费数据\" class=\"headerlink\" title=\"consumer消费数据\"></a>consumer消费数据</h3><p>consumer以consumer-group（消费者组）为单位向kafka订阅topic并消费数据：</p>\n<p><img src=\"\\blog\\images\\pasted-8.png\" alt=\"upload successful\"></p>\n<p>kafka consumer抓取数据基本流程图：</p>\n<p><img src=\"\\blog\\images\\pasted-9.png\" alt=\"upload successful\"></p>\n<p>几点特性：</p>\n<ul>\n<li><strong>partition主从热备</strong>：消息的读/写工作全部交给leader partition来完成，slave partition主动与leader同步，通过LEO和leader epoch等信息同步数据，leader通过ISR列表保存当前存活的slave状态</li>\n<li><strong>可靠性</strong>：只有当ISR列表中所有副本都成功收到的消息才能提供给consumer，可以提供给consumer的这部分消息由<code>high water</code>来进行控制 (设置<code>replica.lag.time.max.ms</code>参数可保证副本同步消息的时间上限)</li>\n<li><strong>零拷贝</strong>：kafka通过零拷贝的方式将数据返回给consumer</li>\n</ul>\n<h4 id=\"consumer的java-api\"><a href=\"#consumer的java-api\" class=\"headerlink\" title=\"consumer的java api\"></a>consumer的java api</h4><p>使用kafka-consumer高级api进行消费操作：</p>\n<ol>\n<li><p>创建单个consumer进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ConsumerGroupTest</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; consumerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, <span class=\"string\">\"test-group-1\"</span>);</span><br><span class=\"line\">        runConsumer(consumerConfig);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumer</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">        Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">        consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">                records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                    printRecord(record);</span><br><span class=\"line\">                    print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                    System.out.println();</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            consumer.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">printRecord</span><span class=\"params\">(ConsumerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"当前record：====&gt; \"</span></span><br><span class=\"line\">                           + <span class=\"string\">\" topic:\"</span> + record.topic()</span><br><span class=\"line\">                           + <span class=\"string\">\" partition:\"</span> + record.partition()</span><br><span class=\"line\">                           + <span class=\"string\">\" offset:\"</span> + record.offset()</span><br><span class=\"line\">                           + <span class=\"string\">\" value:\"</span> + record.value()</span><br><span class=\"line\">                          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">print</span><span class=\"params\">(String name, Object value)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"KafkaConsumer ====&gt; \"</span> + name + <span class=\"string\">\" is \"</span> + <span class=\"string\">\"&#123; \"</span> + value + <span class=\"string\">\" &#125;\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>单个consumer实例可以订阅多个topic，也可以通过正则表达式订阅topic：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 订阅多个topic</span></span><br><span class=\"line\">consumer.subscribe(Arrays.asList(<span class=\"string\">\"test-topic-1\"</span>, <span class=\"string\">\"test-topic-2\"</span>));</span><br><span class=\"line\"><span class=\"comment\">// 通过正则表达式订阅多个topic</span></span><br><span class=\"line\">consumer.subscribe(Pattern.compile(<span class=\"string\">\"test.*\"</span>));</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建消费者组订阅包含多个partition的topic：</p>\n<p>创建一个叫做“test-group-1”的消费组，订阅“test_multi_par_topic_1” topic， 该topic包含三个partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Map&lt;String, Object&gt; consumerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, <span class=\"string\">\"test-group-1\"</span>);</span><br><span class=\"line\">    runConsumerManualCommit(consumerConfig);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumer</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>   首先只启动一个实例，可以看到，当前consumer实例消费了全部的三个partition的数据：</p>\n<p><img src=\"\\blog\\images\\pasted-10.png\" alt=\"upload successful\"></p>\n<p>   向该组加入一个consumer实例，可以看到，之前<code>@48322fe3</code>这个consumer实例只分到了partition2，而新加入的<code>@365ae362</code>这个实例分到了0和1：：</p>\n<p><img src=\"\\blog\\images\\pasted-11.png\" alt=\"upload successful\"> </p>\n<p><img src=\"\\blog\\images\\pasted-12.png\" alt=\"upload successful\"></p>\n<p>继续加consumer实例进来，最早的元老实例1，分到了partition2：</p>\n<p><img src=\"\\blog\\images\\pasted-13.png\" alt=\"upload successful\"></p>\n<p>之后加入的实例2分到了partition0：</p>\n<p><img src=\"\\blog\\images\\pasted-14.png\" alt=\"upload successful\"></p>\n<p>最新加入的实例3 <code>@345dd14f</code>，分到了parition1：</p>\n<p><img src=\"\\blog\\images\\pasted-15.png\" alt=\"upload successful\"></p>\n<p>接下来将元老实例1<code>@48322fe3</code>停掉，观察实例2和实例3 （注意观察实例1的最后消费offset），实例1最后消费情况，消费完offset=85就挂掉了：</p>\n<p><img src=\"\\blog\\images\\pasted-16.png\" alt=\"upload successful\"></p>\n<p>观察实例2可以发现，实例2除了继续消费partition0，还认领了之前实例3消费的partition1的任务</p>\n<p><img src=\"\\blog\\images\\pasted-17.png\" alt=\"upload successful\"></p>\n<p>观察实例3，消费partition1直到offset=91的位置，发生了rebalance，之后partition1的任务分派给了上面实例2，自己分到了因实例1挂掉而无人认领的partition2，并且从实例1最后消费位置即offset=86开始消费</p>\n<p><img src=\"\\blog\\images\\pasted-18.png\" alt=\"upload successful\"></p>\n<h3 id=\"consumer-group\"><a href=\"#consumer-group\" class=\"headerlink\" title=\"consumer-group\"></a>consumer-group</h3><p>单个consumer-group即可以只消费一个topic的消息，也可以同时消费多个topic的消息</p>\n<h4 id=\"consumer-group订阅topic\"><a href=\"#consumer-group订阅topic\" class=\"headerlink\" title=\"consumer-group订阅topic\"></a>consumer-group订阅topic</h4><p>单个consumer-group在订阅某个topic时，如果不特殊指定订阅某个partition，kafka将启用<code>Coordinator</code>对consumer-group内的consumer实例进行负载均衡，有以下特点：</p>\n<ol>\n<li><p>组内的一个consumer实例消费全部的partition：</p>\n<p> <img src=\"\\blog\\images\\pasted-19.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>组内继续加入consumer实例，一同消费多个partition，消费过程与另外的consumer完全隔离不受影响：</p>\n<p> <img src=\"\\blog\\images\\pasted-20.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>组内继续加入consumer，每个consumer实例单独消费一个partition，此时consumer数目与partition数目一致：</p>\n<p> <img src=\"\\blog\\images\\pasted-21.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>继续加入consumer，数目超过partition数目，此时多出来的consumer将分不到partition进行消费：</p>\n<p> <img src=\"\\blog\\images\\pasted-22.png\" alt=\"upload successful\"></p>\n</li>\n</ol>\n<p>如上可以看出，当单个consumer-group订阅topic进行消费时，kafka的Coordinator保证topic的数据能够被均匀分派到组内的各个消费者上，并且topic中的<strong>单个partition只能被单个consumer-group中的其中一个consumer消费</strong></p>\n<p>不过，如果有其他consumer-group的consumer参与消费这个topic，将与之前的consumer-group隔离，Coordinator将单独为这个新的组进行负载均衡：</p>\n<p>   <img src=\"\\blog\\images\\pasted-23.png\" alt=\"upload successful\"></p>\n<h4 id=\"consumer分区再平衡\"><a href=\"#consumer分区再平衡\" class=\"headerlink\" title=\"consumer分区再平衡\"></a>consumer分区再平衡</h4><ul>\n<li>每当一个consumer-group有新成员加入，并且和老成员一起消费同一个topic的时候，Coordinator都将进行分区再平衡（<code>rebalance</code>），为了平衡消费能力，老成员消费的partition有可能会被分派给新加入的consumer</li>\n<li>当有consumer实例挂掉时，之前分派给他的partition将会重新分派给剩余还活着的consumer（conusmer实例通过发送心跳让kafka集群知道他还活着）</li>\n<li>订阅的topic新加入partition也会触发rebalance</li>\n<li>通过正则表达式订阅某个类型的topic，当新加入该类型的topic时，也会发生rebalance</li>\n</ul>\n<h4 id=\"consumer-rebalance的大致流程：\"><a href=\"#consumer-rebalance的大致流程：\" class=\"headerlink\" title=\"consumer rebalance的大致流程：\"></a>consumer rebalance的大致流程：</h4><ol>\n<li>Topic/Partition的改变或者新Consumer的加入或者已有Consumer停止，将触发Coordinator注册在Zookeeper上的watch，Coordinator收到通知准备发起Rebalance操作。</li>\n<li>Coordinator通过在HeartbeatResponse中返回IllegalGeneration错误码通知各个consumer发起Rebalance操作。</li>\n<li>存活的Consumer向Coordinator发送JoinGroupRequest</li>\n<li>Coordinator在Zookeeper中增加Group的Generation ID并将新的Partition分配情况写入Zookeeper</li>\n<li>Coordinator向存活的consumer发送JoinGroupResponse</li>\n<li>存活的consumer收到JoinGroupResponse后重新启动新一轮消费</li>\n</ol>\n<h3 id=\"consumer配置\"><a href=\"#consumer配置\" class=\"headerlink\" title=\"consumer配置\"></a>consumer配置</h3><p>除了上述<code>bootstrap.servers</code>,<code>key.deserializer</code>,<code>value.deserializer</code>,<code>group.id</code>这几个配置项必须进行配置之外，其他的配置项均为可选项：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 单次拉取消息的最小字节数，如果该值不为0，在consumer拉取消息时，如果可消费数据达不到该值，broker将等待一段时间(fetch.max.wait.ms)直到有足够数据或者等待超时，再返回给consumer</span><br><span class=\"line\">fetch.min.bytes=1024</span><br><span class=\"line\"># 单次拉取消息的最长等待时间</span><br><span class=\"line\">fetch.max.wait.ms=500</span><br><span class=\"line\"># 指定consumer单次poll()从分区中拉取的最大字节数，默认1MB</span><br><span class=\"line\">max.partition.fetch.bytes=1048576</span><br><span class=\"line\"># kafka集群判断consumer连接中断的最长等待时间，默认10秒，如果consumer超过该时间没有发送心跳，则会判断为死亡进而触发rebalance</span><br><span class=\"line\">session.timeout.ms=10000</span><br><span class=\"line\"># 心跳间隔时间，建议不超过session.timeout.ms的1/3</span><br><span class=\"line\">heartbeat.interval.ms=3000</span><br><span class=\"line\"># 是否开启自动提交offset，默认是开启的，如果开启，将每隔auto.commit.interval.ms的时间将消费完成但未提交offset的consumer统一向kafka集群提交一次（风险：当consumer消费完成但未提交offset就挂掉时，重启后将造成消息重复消费）</span><br><span class=\"line\">enable.auto.commit=true</span><br><span class=\"line\"># 自动统一提交offset的时间间隔</span><br><span class=\"line\">auto.commit.interval.ms=5000</span><br><span class=\"line\"># 开始消费的consumer-group（或者该consumer掉线已久，已经丢失有效的offset信息）初始化offset的策略，</span><br><span class=\"line\"># latest：默认值，默认从最新的有效offset开始消费</span><br><span class=\"line\"># earliest：从最早的有效offset开始消费</span><br><span class=\"line\"># none：如果kafka没有保存该consumer-group的offset信息则直接抛异常</span><br><span class=\"line\">auto.offset.reset=[latest, earliest, none]</span><br><span class=\"line\"># 分区分派策略类，kafka-clients自带的分区策略有Range和RoundRobin</span><br><span class=\"line\"># 必须是PartitionAssignor的实现类</span><br><span class=\"line\">partition.assignment.strategy=class org.apache.kafka.clients.consumer.RangeAssignor</span><br><span class=\"line\"># 单次poll()拉取的消息数量最大值，默认是500个</span><br><span class=\"line\">max.poll.records=500</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer的offset提交策略\"><a href=\"#kafka-consumer的offset提交策略\" class=\"headerlink\" title=\"kafka consumer的offset提交策略\"></a>kafka consumer的offset提交策略</h3><p>consumer每次消费数据完成后需要提交offset给集群以更新自己的消费状态，提交过程非常重要，直接关系到消费过程的稳定性，提交异常的话可能导致以下两种后果：</p>\n<ul>\n<li><p>重复消费：一般发生在还没来得及提交offset即被中断消费的情况</p>\n<p><img src=\"\\blog\\images\\pasted-24.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>丢失消费：一般发生在消费过程有耗时操作的情况，如果在此期间提交了当前还没处理完成的消息的offset，会造成消费丢失：</p>\n<p><img src=\"\\blog\\images\\pasted-25.png\" alt=\"upload successful\"></p>\n</li>\n</ul>\n<p>kafka提供了<code>手动commit</code>和<code>自动commit</code>两种提交策略：</p>\n<ol>\n<li><p><strong>自动提交</strong>：将<code>enable.auto.commit</code>设置为true并且将<code>auto.commit.interval.ms</code>设置为大于0的时间，即可开启自动提交，每次调用<code>consumer.poll()</code>方法时都会检查是否达到提交间隔时间，并将上一次拉取消息中最大的一个offset信息发送给集群中的<code>__consumer_offsets</code>这个topic</p>\n<p>缺点：自动提交不需要用户手动commit，因此提交过程不可控</p>\n</li>\n<li><p><strong>手动提交</strong>：手动提交又分为<code>commitSync()</code>和<code>commitAsync()</code>两种方式，该模式下<code>enable.auto.commit</code>必须为false，consumer api任何情况都不会自动帮用户提交offset，一切由用户主动控制：</p>\n<p><code>commitSync()</code>：同步提交，直接提交之前拉取的最新的offset，用户可自己保证在消息处理完毕后直接提交当前offset，缺点是整个提交过程是阻塞的，并且提交失败会进行重试</p>\n<p><code>commitAsync()</code>：异步提交，同样也是提交之前拉取的最新的offset，改善了同步提交过程会产生阻塞的缺点，支持回调函数，提交过程不进行重试</p>\n</li>\n</ol>\n<p>比较好的手动提交方式是<code>同步和异步组合提交</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// poll拉取消息，最长等待时间10秒</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 消费过程未报错则执行异步提交，速度更快，不阻塞consumer线程</span></span><br><span class=\"line\">            consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class=\"line\">                <span class=\"comment\">// offsets中的`offsetAndMetadata`包含本次提交后希望处理的下一个offset</span></span><br><span class=\"line\">                print(<span class=\"string\">\"本次提交后希望处理的下一个offset:\"</span>, offsets);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (exception != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    exception.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">    e.printStackTrace();</span><br><span class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 消费过程报错的话就尝试同步提交，防止未提交offset造成风险</span></span><br><span class=\"line\">        consumer.commitSync();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer消费时指定offset\"><a href=\"#kafka-consumer消费时指定offset\" class=\"headerlink\" title=\"kafka consumer消费时指定offset\"></a>kafka consumer消费时指定offset</h3><p>有些时候我们并不想按照默认情况读取最新offset位置的消息，kafka对此提供了查找特定offset的api，可以实现向后回退几个消息或者向前跳过几个消息：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumerSeekOffset</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig, <span class=\"keyword\">long</span> specificOffset)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 使用seek()方法将consumer的offset重置到指定位置</span></span><br><span class=\"line\">        consumer.assignment().forEach(topicPartition -&gt; &#123;</span><br><span class=\"line\">            consumer.seek(topicPartition, specificOffset);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class=\"number\">10</span>));</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        consumer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"kafka-consumer-rebalance监听器\"><a href=\"#kafka-consumer-rebalance监听器\" class=\"headerlink\" title=\"kafka consumer rebalance监听器\"></a>kafka consumer rebalance监听器</h3><p>consumer和kafka的Coodinator交互的心跳心中包含是否需要rebalance信息，如果需要rebalance则停止拉取数据并直接提交offset，rebalance完成后consumer有可能失去对当前分配的partition的消费权</p>\n<p>kafka提供了rebalance监听器用于让用户设置在rebalance发生后需要做的一些资源回收的操作，并且可以在监听器中手动提交当前已经处理完成的消息offset，以下是相对比较保险不会造成消费丢失的一种提交策略：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">runConsumerRebanlanceListener</span><span class=\"params\">(Map&lt;String, Object&gt; consumerConfig)</span> </span>&#123;</span><br><span class=\"line\">    Consumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;&gt;(consumerConfig);</span><br><span class=\"line\">    <span class=\"comment\">// 已处理的消息的offset暂存区</span></span><br><span class=\"line\">    <span class=\"keyword\">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    consumer.subscribe(Collections.singleton(<span class=\"string\">\"test_multi_par_topic_1\"</span>), <span class=\"keyword\">new</span> ConsumerRebalanceListener() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPartitionsRevoked</span><span class=\"params\">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"发生rebalance！\"</span>);</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"当前分配的partitions：\"</span> + partitions);</span><br><span class=\"line\">\t\t   <span class=\"comment\">// 将已经处理完成的offset进行同步提交</span></span><br><span class=\"line\">            consumer.commitSync(currentOffsets);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPartitionsAssigned</span><span class=\"params\">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"rebalance完成！\"</span>);</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"rebalance后分配的partitions：\"</span> + partitions);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class=\"number\">100</span>);</span><br><span class=\"line\">            currentOffsets.clear();</span><br><span class=\"line\">            records.forEach(record -&gt; &#123;</span><br><span class=\"line\">                printRecord(record);</span><br><span class=\"line\">                print(<span class=\"string\">\"当前consumer实例：\"</span>, consumer.toString());</span><br><span class=\"line\">                <span class=\"comment\">// 当前处理完成，待提交的offset，存入暂存区</span></span><br><span class=\"line\">                currentOffsets.put(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> OffsetAndMetadata(record.offset() + <span class=\"number\">1</span>, <span class=\"keyword\">null</span>));</span><br><span class=\"line\">                System.out.println();</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\">            <span class=\"comment\">// 每隔批次消息处理完成后异步提交</span></span><br><span class=\"line\">            consumer.commitAsync(currentOffsets, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 出错后同步提交</span></span><br><span class=\"line\">            consumer.commitSync(currentOffsets);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            consumer.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"kafka学习笔记（4）—— 深入集群","author":"天渊","date":"2019-03-18T05:33:00.000Z","_content":"## KafkaController\n\n`KafkaController`其实就是kafka集群中其中一个broker，他是由zookeeper在多个broker选举出来的`leader broker`，肩负`partition assign`，`consumer rebalance`，`partition election`等重任 <!--more-->\n\n![upload successful](\\blog\\images\\pasted-26.png)\n\n### KafkaController选举\n\n- 新加入集群的broker向zookeeper创建临时节点`/controller`，创建成功则为KafkaController，并在当前节点写入以下信息，其他broker节点会监听`/controller`节点的变化情况\n\n  ```json\n  {“version”:1,”brokerid”:1,”timestamp”:”1512018424988”}\n  ```\n\n- 新选出的Controller(leader broker)会在zookeeper的`/controller_epoch`节点上创建递增序列，用于区别不同代的leader，并向zookeeper同步元数据，其他follower broker则会监听当前的`controller_epoch`的值，如果在和某个自称为leader的broker通信时发现他的epoch不是最新值，则会选择忽略本次通信（防止controller脑裂）\n\n![upload successful](\\blog\\images\\pasted-27.png)\n\n### controller主导partition leader选举\n\nKafkaController有一个很重要的功能就是在某个partition的leader出现不可用时，主导这个partition各副本之间的新leader选举，单个partition分为 leader副本和follow副本：\n\n- leader副本：每隔partition都有一个leader和多个follower，所有producer和consumer的请求都得通过leader进行处理\n- follower副本：follower副本不处理客户端的读写请求，唯一任务就是从首领那里同步数据，如果leader发生崩溃（leader副本所在的broker发生down机或者该broker和zookeeper同步超时），controller则会启动该partition的leader选举，并将新选举产生的leader信息同步给zookeeper\n\n![upload successful](\\blog\\images\\pasted-28.png)\n\n## Partition Leader\n\n分区leader负责消息的读写并协调各副本的数据同步\n\n### hight water mark\n\nkafka为了保证消息数据的高可靠性，只有已经被所有副本完全同步的消息才能被consumer消费，这个所谓的“已经被所有副本完全同步的消息”由`high water mark`来标定，只有offset小于`high water mark`的消息才对consumer可见：\n\n![upload successful](\\blog\\images\\pasted-29.png)\n\n如上所示，只有offset < 3的消息才是可被消费的消息\n\n### LEO (log end offset)\n\n日志末端位移，即当前副本日志中下一条消息的offset，上图中Replaca 0的LEO为5，以此类推，Replica的LEO为4，Replica的LEO为3\n\npartition的leader和followers均保留一份自己的LEO值，同时leader保有所有follower的LEO值，follower向leader同步数据时，leader会根据follower当前的LEO值判断需要同步给他的消息范围，并根据follower的LEO值更新`high water mark`\n\n### ISR (insync replicas)\n\n处于同步状态的partition副本列表，partition的leader保留一份，并且在zookeeper也保留一份，这份列表记录了当前有哪些副本处于有效同步状态（包含leader自己）：\n\n- `replica.lag.time.max.ms`：超过这个时间还未与leader同步的follower将会被踢出ISR列表\n- 挂掉的follower重新与leader同步，在同步进度追上leader后重新加入ISR\n- `min.insync.replicas`：最小同步副本数，如果ISR当中的副本数目不足，当前partition则会变为不可用状态，拒绝任何produce和consume请求；该参数用于平衡kafka集群的可用性和一致性\n- ISR与高水位的关系：ISR中副本的最低LEO即为`high water mark`\n- `unclean.leader.election`：不完全的首领选举，如果设置为true，在进行leader选举时可以选举ISR列表以外的副本作为新leader，但这种情况下丢失消息的几率就比较高了\n\n### leader_epoch\n\n当前partition的leader分代标记，用于在某个副本崩溃重启后与当前leader同步消息时，判断当前leader_epoch是否与自己崩溃前保存的leader_epoch信息一致，并根据leader_epoch信息判断是否需要做日志truncate\n\n（关于老版本kafka使用高水位进行truncate风险及kafka丢消息的往事，细节比较复杂，详细请参考这篇文章：[confluence cwiki: use leader epoch rather than high water mark for truncation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation) )\n\nleader_epoch的数据结构是一个键值对：`LeaderEpoch -> StartOffset`，其中`LeaderEpoch `是一个单调递增序列号，每次进行leader选举后都会产生一个`LeaderEpoch `序列号，`StartOffset`是该次选举完成后新leader自己的LEO值\n\n## Partition副本同步\n\n为了保证消息一致性，kafka使用了`high water mark`，`ISR`，`LEO`和`leader_epoch`等方式来处理follower和leader间的消息同步，同步流程如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/kafka学习笔记（4）——-深入集群.md","raw":"title: kafka学习笔记（4）—— 深入集群\nauthor: 天渊\ntags:\n  - Kafka\n  - 大数据\ncategories:\n  - 基础知识\ndate: 2019-03-18 13:33:00\n---\n## KafkaController\n\n`KafkaController`其实就是kafka集群中其中一个broker，他是由zookeeper在多个broker选举出来的`leader broker`，肩负`partition assign`，`consumer rebalance`，`partition election`等重任 <!--more-->\n\n![upload successful](\\blog\\images\\pasted-26.png)\n\n### KafkaController选举\n\n- 新加入集群的broker向zookeeper创建临时节点`/controller`，创建成功则为KafkaController，并在当前节点写入以下信息，其他broker节点会监听`/controller`节点的变化情况\n\n  ```json\n  {“version”:1,”brokerid”:1,”timestamp”:”1512018424988”}\n  ```\n\n- 新选出的Controller(leader broker)会在zookeeper的`/controller_epoch`节点上创建递增序列，用于区别不同代的leader，并向zookeeper同步元数据，其他follower broker则会监听当前的`controller_epoch`的值，如果在和某个自称为leader的broker通信时发现他的epoch不是最新值，则会选择忽略本次通信（防止controller脑裂）\n\n![upload successful](\\blog\\images\\pasted-27.png)\n\n### controller主导partition leader选举\n\nKafkaController有一个很重要的功能就是在某个partition的leader出现不可用时，主导这个partition各副本之间的新leader选举，单个partition分为 leader副本和follow副本：\n\n- leader副本：每隔partition都有一个leader和多个follower，所有producer和consumer的请求都得通过leader进行处理\n- follower副本：follower副本不处理客户端的读写请求，唯一任务就是从首领那里同步数据，如果leader发生崩溃（leader副本所在的broker发生down机或者该broker和zookeeper同步超时），controller则会启动该partition的leader选举，并将新选举产生的leader信息同步给zookeeper\n\n![upload successful](\\blog\\images\\pasted-28.png)\n\n## Partition Leader\n\n分区leader负责消息的读写并协调各副本的数据同步\n\n### hight water mark\n\nkafka为了保证消息数据的高可靠性，只有已经被所有副本完全同步的消息才能被consumer消费，这个所谓的“已经被所有副本完全同步的消息”由`high water mark`来标定，只有offset小于`high water mark`的消息才对consumer可见：\n\n![upload successful](\\blog\\images\\pasted-29.png)\n\n如上所示，只有offset < 3的消息才是可被消费的消息\n\n### LEO (log end offset)\n\n日志末端位移，即当前副本日志中下一条消息的offset，上图中Replaca 0的LEO为5，以此类推，Replica的LEO为4，Replica的LEO为3\n\npartition的leader和followers均保留一份自己的LEO值，同时leader保有所有follower的LEO值，follower向leader同步数据时，leader会根据follower当前的LEO值判断需要同步给他的消息范围，并根据follower的LEO值更新`high water mark`\n\n### ISR (insync replicas)\n\n处于同步状态的partition副本列表，partition的leader保留一份，并且在zookeeper也保留一份，这份列表记录了当前有哪些副本处于有效同步状态（包含leader自己）：\n\n- `replica.lag.time.max.ms`：超过这个时间还未与leader同步的follower将会被踢出ISR列表\n- 挂掉的follower重新与leader同步，在同步进度追上leader后重新加入ISR\n- `min.insync.replicas`：最小同步副本数，如果ISR当中的副本数目不足，当前partition则会变为不可用状态，拒绝任何produce和consume请求；该参数用于平衡kafka集群的可用性和一致性\n- ISR与高水位的关系：ISR中副本的最低LEO即为`high water mark`\n- `unclean.leader.election`：不完全的首领选举，如果设置为true，在进行leader选举时可以选举ISR列表以外的副本作为新leader，但这种情况下丢失消息的几率就比较高了\n\n### leader_epoch\n\n当前partition的leader分代标记，用于在某个副本崩溃重启后与当前leader同步消息时，判断当前leader_epoch是否与自己崩溃前保存的leader_epoch信息一致，并根据leader_epoch信息判断是否需要做日志truncate\n\n（关于老版本kafka使用高水位进行truncate风险及kafka丢消息的往事，细节比较复杂，详细请参考这篇文章：[confluence cwiki: use leader epoch rather than high water mark for truncation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation) )\n\nleader_epoch的数据结构是一个键值对：`LeaderEpoch -> StartOffset`，其中`LeaderEpoch `是一个单调递增序列号，每次进行leader选举后都会产生一个`LeaderEpoch `序列号，`StartOffset`是该次选举完成后新leader自己的LEO值\n\n## Partition副本同步\n\n为了保证消息一致性，kafka使用了`high water mark`，`ISR`，`LEO`和`leader_epoch`等方式来处理follower和leader间的消息同步，同步流程如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"kafka学习笔记（4）——-深入集群","published":1,"updated":"2019-03-19T13:30:58.465Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js3y0013g0qr5onkb6r6","content":"<h2 id=\"KafkaController\"><a href=\"#KafkaController\" class=\"headerlink\" title=\"KafkaController\"></a>KafkaController</h2><p><code>KafkaController</code>其实就是kafka集群中其中一个broker，他是由zookeeper在多个broker选举出来的<code>leader broker</code>，肩负<code>partition assign</code>，<code>consumer rebalance</code>，<code>partition election</code>等重任 <a id=\"more\"></a></p>\n<p><img src=\"\\blog\\images\\pasted-26.png\" alt=\"upload successful\"></p>\n<h3 id=\"KafkaController选举\"><a href=\"#KafkaController选举\" class=\"headerlink\" title=\"KafkaController选举\"></a>KafkaController选举</h3><ul>\n<li><p>新加入集群的broker向zookeeper创建临时节点<code>/controller</code>，创建成功则为KafkaController，并在当前节点写入以下信息，其他broker节点会监听<code>/controller</code>节点的变化情况</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;“version”:1,”brokerid”:1,”timestamp”:”1512018424988”&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>新选出的Controller(leader broker)会在zookeeper的<code>/controller_epoch</code>节点上创建递增序列，用于区别不同代的leader，并向zookeeper同步元数据，其他follower broker则会监听当前的<code>controller_epoch</code>的值，如果在和某个自称为leader的broker通信时发现他的epoch不是最新值，则会选择忽略本次通信（防止controller脑裂）</p>\n</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-27.png\" alt=\"upload successful\"></p>\n<h3 id=\"controller主导partition-leader选举\"><a href=\"#controller主导partition-leader选举\" class=\"headerlink\" title=\"controller主导partition leader选举\"></a>controller主导partition leader选举</h3><p>KafkaController有一个很重要的功能就是在某个partition的leader出现不可用时，主导这个partition各副本之间的新leader选举，单个partition分为 leader副本和follow副本：</p>\n<ul>\n<li>leader副本：每隔partition都有一个leader和多个follower，所有producer和consumer的请求都得通过leader进行处理</li>\n<li>follower副本：follower副本不处理客户端的读写请求，唯一任务就是从首领那里同步数据，如果leader发生崩溃（leader副本所在的broker发生down机或者该broker和zookeeper同步超时），controller则会启动该partition的leader选举，并将新选举产生的leader信息同步给zookeeper</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-28.png\" alt=\"upload successful\"></p>\n<h2 id=\"Partition-Leader\"><a href=\"#Partition-Leader\" class=\"headerlink\" title=\"Partition Leader\"></a>Partition Leader</h2><p>分区leader负责消息的读写并协调各副本的数据同步</p>\n<h3 id=\"hight-water-mark\"><a href=\"#hight-water-mark\" class=\"headerlink\" title=\"hight water mark\"></a>hight water mark</h3><p>kafka为了保证消息数据的高可靠性，只有已经被所有副本完全同步的消息才能被consumer消费，这个所谓的“已经被所有副本完全同步的消息”由<code>high water mark</code>来标定，只有offset小于<code>high water mark</code>的消息才对consumer可见：</p>\n<p><img src=\"\\blog\\images\\pasted-29.png\" alt=\"upload successful\"></p>\n<p>如上所示，只有offset &lt; 3的消息才是可被消费的消息</p>\n<h3 id=\"LEO-log-end-offset\"><a href=\"#LEO-log-end-offset\" class=\"headerlink\" title=\"LEO (log end offset)\"></a>LEO (log end offset)</h3><p>日志末端位移，即当前副本日志中下一条消息的offset，上图中Replaca 0的LEO为5，以此类推，Replica的LEO为4，Replica的LEO为3</p>\n<p>partition的leader和followers均保留一份自己的LEO值，同时leader保有所有follower的LEO值，follower向leader同步数据时，leader会根据follower当前的LEO值判断需要同步给他的消息范围，并根据follower的LEO值更新<code>high water mark</code></p>\n<h3 id=\"ISR-insync-replicas\"><a href=\"#ISR-insync-replicas\" class=\"headerlink\" title=\"ISR (insync replicas)\"></a>ISR (insync replicas)</h3><p>处于同步状态的partition副本列表，partition的leader保留一份，并且在zookeeper也保留一份，这份列表记录了当前有哪些副本处于有效同步状态（包含leader自己）：</p>\n<ul>\n<li><code>replica.lag.time.max.ms</code>：超过这个时间还未与leader同步的follower将会被踢出ISR列表</li>\n<li>挂掉的follower重新与leader同步，在同步进度追上leader后重新加入ISR</li>\n<li><code>min.insync.replicas</code>：最小同步副本数，如果ISR当中的副本数目不足，当前partition则会变为不可用状态，拒绝任何produce和consume请求；该参数用于平衡kafka集群的可用性和一致性</li>\n<li>ISR与高水位的关系：ISR中副本的最低LEO即为<code>high water mark</code></li>\n<li><code>unclean.leader.election</code>：不完全的首领选举，如果设置为true，在进行leader选举时可以选举ISR列表以外的副本作为新leader，但这种情况下丢失消息的几率就比较高了</li>\n</ul>\n<h3 id=\"leader-epoch\"><a href=\"#leader-epoch\" class=\"headerlink\" title=\"leader_epoch\"></a>leader_epoch</h3><p>当前partition的leader分代标记，用于在某个副本崩溃重启后与当前leader同步消息时，判断当前leader_epoch是否与自己崩溃前保存的leader_epoch信息一致，并根据leader_epoch信息判断是否需要做日志truncate</p>\n<p>（关于老版本kafka使用高水位进行truncate风险及kafka丢消息的往事，细节比较复杂，详细请参考这篇文章：<a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation\" target=\"_blank\" rel=\"noopener\">confluence cwiki: use leader epoch rather than high water mark for truncation</a> )</p>\n<p>leader_epoch的数据结构是一个键值对：<code>LeaderEpoch -&gt; StartOffset</code>，其中<code>LeaderEpoch</code>是一个单调递增序列号，每次进行leader选举后都会产生一个<code>LeaderEpoch</code>序列号，<code>StartOffset</code>是该次选举完成后新leader自己的LEO值</p>\n<h2 id=\"Partition副本同步\"><a href=\"#Partition副本同步\" class=\"headerlink\" title=\"Partition副本同步\"></a>Partition副本同步</h2><p>为了保证消息一致性，kafka使用了<code>high water mark</code>，<code>ISR</code>，<code>LEO</code>和<code>leader_epoch</code>等方式来处理follower和leader间的消息同步，同步流程如下：</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"KafkaController\"><a href=\"#KafkaController\" class=\"headerlink\" title=\"KafkaController\"></a>KafkaController</h2><p><code>KafkaController</code>其实就是kafka集群中其中一个broker，他是由zookeeper在多个broker选举出来的<code>leader broker</code>，肩负<code>partition assign</code>，<code>consumer rebalance</code>，<code>partition election</code>等重任","more":"</p>\n<p><img src=\"\\blog\\images\\pasted-26.png\" alt=\"upload successful\"></p>\n<h3 id=\"KafkaController选举\"><a href=\"#KafkaController选举\" class=\"headerlink\" title=\"KafkaController选举\"></a>KafkaController选举</h3><ul>\n<li><p>新加入集群的broker向zookeeper创建临时节点<code>/controller</code>，创建成功则为KafkaController，并在当前节点写入以下信息，其他broker节点会监听<code>/controller</code>节点的变化情况</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;“version”:1,”brokerid”:1,”timestamp”:”1512018424988”&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>新选出的Controller(leader broker)会在zookeeper的<code>/controller_epoch</code>节点上创建递增序列，用于区别不同代的leader，并向zookeeper同步元数据，其他follower broker则会监听当前的<code>controller_epoch</code>的值，如果在和某个自称为leader的broker通信时发现他的epoch不是最新值，则会选择忽略本次通信（防止controller脑裂）</p>\n</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-27.png\" alt=\"upload successful\"></p>\n<h3 id=\"controller主导partition-leader选举\"><a href=\"#controller主导partition-leader选举\" class=\"headerlink\" title=\"controller主导partition leader选举\"></a>controller主导partition leader选举</h3><p>KafkaController有一个很重要的功能就是在某个partition的leader出现不可用时，主导这个partition各副本之间的新leader选举，单个partition分为 leader副本和follow副本：</p>\n<ul>\n<li>leader副本：每隔partition都有一个leader和多个follower，所有producer和consumer的请求都得通过leader进行处理</li>\n<li>follower副本：follower副本不处理客户端的读写请求，唯一任务就是从首领那里同步数据，如果leader发生崩溃（leader副本所在的broker发生down机或者该broker和zookeeper同步超时），controller则会启动该partition的leader选举，并将新选举产生的leader信息同步给zookeeper</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-28.png\" alt=\"upload successful\"></p>\n<h2 id=\"Partition-Leader\"><a href=\"#Partition-Leader\" class=\"headerlink\" title=\"Partition Leader\"></a>Partition Leader</h2><p>分区leader负责消息的读写并协调各副本的数据同步</p>\n<h3 id=\"hight-water-mark\"><a href=\"#hight-water-mark\" class=\"headerlink\" title=\"hight water mark\"></a>hight water mark</h3><p>kafka为了保证消息数据的高可靠性，只有已经被所有副本完全同步的消息才能被consumer消费，这个所谓的“已经被所有副本完全同步的消息”由<code>high water mark</code>来标定，只有offset小于<code>high water mark</code>的消息才对consumer可见：</p>\n<p><img src=\"\\blog\\images\\pasted-29.png\" alt=\"upload successful\"></p>\n<p>如上所示，只有offset &lt; 3的消息才是可被消费的消息</p>\n<h3 id=\"LEO-log-end-offset\"><a href=\"#LEO-log-end-offset\" class=\"headerlink\" title=\"LEO (log end offset)\"></a>LEO (log end offset)</h3><p>日志末端位移，即当前副本日志中下一条消息的offset，上图中Replaca 0的LEO为5，以此类推，Replica的LEO为4，Replica的LEO为3</p>\n<p>partition的leader和followers均保留一份自己的LEO值，同时leader保有所有follower的LEO值，follower向leader同步数据时，leader会根据follower当前的LEO值判断需要同步给他的消息范围，并根据follower的LEO值更新<code>high water mark</code></p>\n<h3 id=\"ISR-insync-replicas\"><a href=\"#ISR-insync-replicas\" class=\"headerlink\" title=\"ISR (insync replicas)\"></a>ISR (insync replicas)</h3><p>处于同步状态的partition副本列表，partition的leader保留一份，并且在zookeeper也保留一份，这份列表记录了当前有哪些副本处于有效同步状态（包含leader自己）：</p>\n<ul>\n<li><code>replica.lag.time.max.ms</code>：超过这个时间还未与leader同步的follower将会被踢出ISR列表</li>\n<li>挂掉的follower重新与leader同步，在同步进度追上leader后重新加入ISR</li>\n<li><code>min.insync.replicas</code>：最小同步副本数，如果ISR当中的副本数目不足，当前partition则会变为不可用状态，拒绝任何produce和consume请求；该参数用于平衡kafka集群的可用性和一致性</li>\n<li>ISR与高水位的关系：ISR中副本的最低LEO即为<code>high water mark</code></li>\n<li><code>unclean.leader.election</code>：不完全的首领选举，如果设置为true，在进行leader选举时可以选举ISR列表以外的副本作为新leader，但这种情况下丢失消息的几率就比较高了</li>\n</ul>\n<h3 id=\"leader-epoch\"><a href=\"#leader-epoch\" class=\"headerlink\" title=\"leader_epoch\"></a>leader_epoch</h3><p>当前partition的leader分代标记，用于在某个副本崩溃重启后与当前leader同步消息时，判断当前leader_epoch是否与自己崩溃前保存的leader_epoch信息一致，并根据leader_epoch信息判断是否需要做日志truncate</p>\n<p>（关于老版本kafka使用高水位进行truncate风险及kafka丢消息的往事，细节比较复杂，详细请参考这篇文章：<a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation\" target=\"_blank\" rel=\"noopener\">confluence cwiki: use leader epoch rather than high water mark for truncation</a> )</p>\n<p>leader_epoch的数据结构是一个键值对：<code>LeaderEpoch -&gt; StartOffset</code>，其中<code>LeaderEpoch</code>是一个单调递增序列号，每次进行leader选举后都会产生一个<code>LeaderEpoch</code>序列号，<code>StartOffset</code>是该次选举完成后新leader自己的LEO值</p>\n<h2 id=\"Partition副本同步\"><a href=\"#Partition副本同步\" class=\"headerlink\" title=\"Partition副本同步\"></a>Partition副本同步</h2><p>为了保证消息一致性，kafka使用了<code>high water mark</code>，<code>ISR</code>，<code>LEO</code>和<code>leader_epoch</code>等方式来处理follower和leader间的消息同步，同步流程如下：</p>"},{"title":"Mongodb索引浅析","author":"","date":"2019-01-21T03:16:00.000Z","_content":"mongodb索引数据结果是`b-树`，该数据结构作为索引具有高性能，磁盘io少等特点\n\n### 何为b-树\n\nb-树，又称b树，跟mysql的b+树索引不同，mongodb的b-树索引节点直接与数据绑定，找到索引后直接返回索引对应的document，这种索引适合mongodb这种聚合型nosql数据库：\n\n<!-- more -->\n\n![1547792452619](\\blog\\images\\1547792452619.png)\n\n- b树节点元素个数为k，子节点个数为k+1\n- 如果b树的阶树为m，则`m/2 ≤ k ≤ m`\n- 查询的时间复杂度为O(log n)，单次查询的磁盘io次数为数的高度\n\n### mongodb索引种类\n\nmongodb索引分为如下\n\n- single index: 单个索引，索引字段只有一个：\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1})\n  ```\n\n  score是索引字段，1表示升序索引，-1表示降序索引：\n\n- compound indexex: 混合索引，跟mysql类似，也是由多个字段组成索引：\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1,gender:1})\n  ```\n\n  使用混合索引时，有一些地方需要注意\n\n  - 排序查找的顺序：\n\n    官方文档提到：\n\n    > However, for [compound indexes], sort order can matter in determining whether the index can support a sort operation\n    >\n    > 对于复合索引，排序字段的顺序对于排序操作是否启用索引非常重要\n\n    对于如下索引：\n\n    ```javascript\n    db.getCollection('user').createIndex( { name : 1, gender : -1 } )\n    ```\n\n    对姓名作升序索引，对性别做降序索引，那么在进行以下两种排序时，都会启用索引\n\n    ```javascript\n    db.getCollection('user').find({}).sort({ name : -1, gender : 1 })\n    db.getCollection('user').find({}).sort({ name : 1, gender : -1 })\n    ```\n\n    用`explain(\"allPlansExecution\")`查看执行计划发现，totalKeysExamined是所需结果的数目，说明全部查询均走的索引，执行阶段首先执行IXSCAN即扫描索引确定key的位置，然后执行FETCH去检索指定的document。\n\n    但对以下排序，是不会启用索引的：\n\n    ```javascript\n    db.getCollection('user').find({}).sort({ name : 1, gender : 1 })\n    db.getCollection('user').find({}).sort({ name : -1, gender : -1 })\n    ```\n\n    用`explain(\"allPlansExecution\")`查看执行计划发现，totalKeysExamined是0，执行阶段首先执行COLLSCAN即全表扫描，然后进行排序；说明如果排序查找如果跟索引的排序情况不匹配的话，将放弃启用索引。\n\n  - 左子前缀子集：\n\n    这样的索引：`{ name : 1, gender : 1 }`，包含以下的前缀子集：\n\n    ```javascript\n    { name : 1}\n    { name : 1, gender : 1 }\n    ```\n\n    但不包含以下形式:\n\n    ```javascript\n    { gender : 1}\n    { gender : 1, name : 1 }\n    ```\n\n    使用的时候要特别注意能否匹配上索引的前缀子集\n\n- multikey indexes: 多key型索引，索引字段通常是数组类型\n\n- geospatial indexes: 地理位置索引，包括2dsphere和2d还有geoHaystack三种类型\n\n- text indexex: 全文索引，一个document最多只能有一个全文索引，用于在文档中搜索文本，但创建索引的开销比较大，需要后台或离线创建，综合来说不如elasticsearch等搜索引擎\n\n  在文本中subject和comments两个字段建立了全文索引“text”：\n\n  ```javascript\n  db.reviews.createIndex(\n     {\n       subject: \"text\",\n       comments: \"text\"\n     }\n   )\n  ```\n\n- hashed index：用于分布式collection分片的场景，将不同的document按照某个字段取hash的形式分布到不同的sharding上去：\n\n  > Hashed indexes support [sharding] using hashed shard keys. [Hashed based sharding] uses a hashed index of a field as the shard key to partition data across your sharded cluster.\n\n  对name字段创建一个hashed索引：\n\n  ```javascript\n  db.collection.createIndex( { name: \"hashed\" } )\n  ```\n\n\n\n### mongodb索引属性\n\n建立索引时可以对索引指定属性，有以下几种属性：\n\n- TTL: 对索引设置过期时间shu，有两种方式\n\n  - 延迟时间：在索引创建后延迟特定秒数然后删除该document：\n\n    ```javascript\n    db.collection.createIndex( { \"createAt\": 1 }, { expireAfterSeconds: 3600 } )\n    ```\n\n    createAt字段保存了创建时间，该document会在这个时间的3600秒后被删除\n\n  - 固定时间：在索引创建后延迟特定秒数然后删除该document:\n\n    ```javascript\n    db.collection.createIndex( { \"expireTime\": 1 }, { expireAfterSeconds: 0 } )\n    ```\n\n    该document将在到达expireTime上设置的时间时被删除\n\n- unique index：唯一索引，索引字段唯一，若插入包含相同内容字段的document则会报错\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1}, {unique: true})\n  ```\n\n- spares indexes：稀疏索引，如果设置了稀疏索引，将只对有该索引字段的document启用索引\n\n- partial indexes： 部分索引，跟稀疏索引一样也是非完全索引，在创建索引时可以对索引字段设置一个范围，在这个范围内的才启用索引：\n\n  ```javascript\n  db.restaurants.createIndex(\n     { cuisine: 1, name: 1 },\n     { partialFilterExpression: { rating: { $gt: 5 } } }\n  )\n  ```\n\n  partialFilterExpression表示范围取值，rating是部分索引的索引字段，需要注意的是，进行查询时，查询范围不能超出部分索引的设定范围，不然无法启用索引\n\n- case insensitive indexes\n\n\n\n### mongodb执行计划\n\n通过`explain()`执行计划进行mongodb的性能分析","source":"_posts/mongodb索引类型.md","raw":"title: Mongodb索引浅析\ntags:\n  - mongodb\ncategories: []\nauthor: ''\ndate: 2019-01-21 11:16:00\n---\nmongodb索引数据结果是`b-树`，该数据结构作为索引具有高性能，磁盘io少等特点\n\n### 何为b-树\n\nb-树，又称b树，跟mysql的b+树索引不同，mongodb的b-树索引节点直接与数据绑定，找到索引后直接返回索引对应的document，这种索引适合mongodb这种聚合型nosql数据库：\n\n<!-- more -->\n\n![1547792452619](\\blog\\images\\1547792452619.png)\n\n- b树节点元素个数为k，子节点个数为k+1\n- 如果b树的阶树为m，则`m/2 ≤ k ≤ m`\n- 查询的时间复杂度为O(log n)，单次查询的磁盘io次数为数的高度\n\n### mongodb索引种类\n\nmongodb索引分为如下\n\n- single index: 单个索引，索引字段只有一个：\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1})\n  ```\n\n  score是索引字段，1表示升序索引，-1表示降序索引：\n\n- compound indexex: 混合索引，跟mysql类似，也是由多个字段组成索引：\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1,gender:1})\n  ```\n\n  使用混合索引时，有一些地方需要注意\n\n  - 排序查找的顺序：\n\n    官方文档提到：\n\n    > However, for [compound indexes], sort order can matter in determining whether the index can support a sort operation\n    >\n    > 对于复合索引，排序字段的顺序对于排序操作是否启用索引非常重要\n\n    对于如下索引：\n\n    ```javascript\n    db.getCollection('user').createIndex( { name : 1, gender : -1 } )\n    ```\n\n    对姓名作升序索引，对性别做降序索引，那么在进行以下两种排序时，都会启用索引\n\n    ```javascript\n    db.getCollection('user').find({}).sort({ name : -1, gender : 1 })\n    db.getCollection('user').find({}).sort({ name : 1, gender : -1 })\n    ```\n\n    用`explain(\"allPlansExecution\")`查看执行计划发现，totalKeysExamined是所需结果的数目，说明全部查询均走的索引，执行阶段首先执行IXSCAN即扫描索引确定key的位置，然后执行FETCH去检索指定的document。\n\n    但对以下排序，是不会启用索引的：\n\n    ```javascript\n    db.getCollection('user').find({}).sort({ name : 1, gender : 1 })\n    db.getCollection('user').find({}).sort({ name : -1, gender : -1 })\n    ```\n\n    用`explain(\"allPlansExecution\")`查看执行计划发现，totalKeysExamined是0，执行阶段首先执行COLLSCAN即全表扫描，然后进行排序；说明如果排序查找如果跟索引的排序情况不匹配的话，将放弃启用索引。\n\n  - 左子前缀子集：\n\n    这样的索引：`{ name : 1, gender : 1 }`，包含以下的前缀子集：\n\n    ```javascript\n    { name : 1}\n    { name : 1, gender : 1 }\n    ```\n\n    但不包含以下形式:\n\n    ```javascript\n    { gender : 1}\n    { gender : 1, name : 1 }\n    ```\n\n    使用的时候要特别注意能否匹配上索引的前缀子集\n\n- multikey indexes: 多key型索引，索引字段通常是数组类型\n\n- geospatial indexes: 地理位置索引，包括2dsphere和2d还有geoHaystack三种类型\n\n- text indexex: 全文索引，一个document最多只能有一个全文索引，用于在文档中搜索文本，但创建索引的开销比较大，需要后台或离线创建，综合来说不如elasticsearch等搜索引擎\n\n  在文本中subject和comments两个字段建立了全文索引“text”：\n\n  ```javascript\n  db.reviews.createIndex(\n     {\n       subject: \"text\",\n       comments: \"text\"\n     }\n   )\n  ```\n\n- hashed index：用于分布式collection分片的场景，将不同的document按照某个字段取hash的形式分布到不同的sharding上去：\n\n  > Hashed indexes support [sharding] using hashed shard keys. [Hashed based sharding] uses a hashed index of a field as the shard key to partition data across your sharded cluster.\n\n  对name字段创建一个hashed索引：\n\n  ```javascript\n  db.collection.createIndex( { name: \"hashed\" } )\n  ```\n\n\n\n### mongodb索引属性\n\n建立索引时可以对索引指定属性，有以下几种属性：\n\n- TTL: 对索引设置过期时间shu，有两种方式\n\n  - 延迟时间：在索引创建后延迟特定秒数然后删除该document：\n\n    ```javascript\n    db.collection.createIndex( { \"createAt\": 1 }, { expireAfterSeconds: 3600 } )\n    ```\n\n    createAt字段保存了创建时间，该document会在这个时间的3600秒后被删除\n\n  - 固定时间：在索引创建后延迟特定秒数然后删除该document:\n\n    ```javascript\n    db.collection.createIndex( { \"expireTime\": 1 }, { expireAfterSeconds: 0 } )\n    ```\n\n    该document将在到达expireTime上设置的时间时被删除\n\n- unique index：唯一索引，索引字段唯一，若插入包含相同内容字段的document则会报错\n\n  ```javascript\n  db.getCollection('user').createIndex({name:1}, {unique: true})\n  ```\n\n- spares indexes：稀疏索引，如果设置了稀疏索引，将只对有该索引字段的document启用索引\n\n- partial indexes： 部分索引，跟稀疏索引一样也是非完全索引，在创建索引时可以对索引字段设置一个范围，在这个范围内的才启用索引：\n\n  ```javascript\n  db.restaurants.createIndex(\n     { cuisine: 1, name: 1 },\n     { partialFilterExpression: { rating: { $gt: 5 } } }\n  )\n  ```\n\n  partialFilterExpression表示范围取值，rating是部分索引的索引字段，需要注意的是，进行查询时，查询范围不能超出部分索引的设定范围，不然无法启用索引\n\n- case insensitive indexes\n\n\n\n### mongodb执行计划\n\n通过`explain()`执行计划进行mongodb的性能分析","slug":"mongodb索引类型","published":1,"updated":"2019-03-19T13:30:58.471Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js400016g0qr6goppgy4","content":"<p>mongodb索引数据结果是<code>b-树</code>，该数据结构作为索引具有高性能，磁盘io少等特点</p>\n<h3 id=\"何为b-树\"><a href=\"#何为b-树\" class=\"headerlink\" title=\"何为b-树\"></a>何为b-树</h3><p>b-树，又称b树，跟mysql的b+树索引不同，mongodb的b-树索引节点直接与数据绑定，找到索引后直接返回索引对应的document，这种索引适合mongodb这种聚合型nosql数据库：</p>\n<a id=\"more\"></a>\n<p><img src=\"\\blog\\images\\1547792452619.png\" alt=\"1547792452619\"></p>\n<ul>\n<li>b树节点元素个数为k，子节点个数为k+1</li>\n<li>如果b树的阶树为m，则<code>m/2 ≤ k ≤ m</code></li>\n<li>查询的时间复杂度为O(log n)，单次查询的磁盘io次数为数的高度</li>\n</ul>\n<h3 id=\"mongodb索引种类\"><a href=\"#mongodb索引种类\" class=\"headerlink\" title=\"mongodb索引种类\"></a>mongodb索引种类</h3><p>mongodb索引分为如下</p>\n<ul>\n<li><p>single index: 单个索引，索引字段只有一个：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>&#125;)</span><br></pre></td></tr></table></figure>\n<p>score是索引字段，1表示升序索引，-1表示降序索引：</p>\n</li>\n<li><p>compound indexex: 混合索引，跟mysql类似，也是由多个字段组成索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>,<span class=\"attr\">gender</span>:<span class=\"number\">1</span>&#125;)</span><br></pre></td></tr></table></figure>\n<p>使用混合索引时，有一些地方需要注意</p>\n<ul>\n<li><p>排序查找的顺序：</p>\n<p>官方文档提到：</p>\n<blockquote>\n<p>However, for [compound indexes], sort order can matter in determining whether the index can support a sort operation</p>\n<p>对于复合索引，排序字段的顺序对于排序操作是否启用索引非常重要</p>\n</blockquote>\n<p>对于如下索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex( &#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>对姓名作升序索引，对性别做降序索引，那么在进行以下两种排序时，都会启用索引</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">-1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;)</span><br><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125;)</span><br></pre></td></tr></table></figure>\n<p>用<code>explain(&quot;allPlansExecution&quot;)</code>查看执行计划发现，totalKeysExamined是所需结果的数目，说明全部查询均走的索引，执行阶段首先执行IXSCAN即扫描索引确定key的位置，然后执行FETCH去检索指定的document。</p>\n<p>但对以下排序，是不会启用索引的：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;)</span><br><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">-1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125;)</span><br></pre></td></tr></table></figure>\n<p>用<code>explain(&quot;allPlansExecution&quot;)</code>查看执行计划发现，totalKeysExamined是0，执行阶段首先执行COLLSCAN即全表扫描，然后进行排序；说明如果排序查找如果跟索引的排序情况不匹配的话，将放弃启用索引。</p>\n</li>\n<li><p>左子前缀子集：</p>\n<p>这样的索引：<code>{ name : 1, gender : 1 }</code>，包含以下的前缀子集：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>&#125;</span><br><span class=\"line\">&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>但不包含以下形式:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; <span class=\"attr\">gender</span> : <span class=\"number\">1</span>&#125;</span><br><span class=\"line\">&#123; <span class=\"attr\">gender</span> : <span class=\"number\">1</span>, <span class=\"attr\">name</span> : <span class=\"number\">1</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>使用的时候要特别注意能否匹配上索引的前缀子集</p>\n</li>\n</ul>\n</li>\n<li><p>multikey indexes: 多key型索引，索引字段通常是数组类型</p>\n</li>\n<li><p>geospatial indexes: 地理位置索引，包括2dsphere和2d还有geoHaystack三种类型</p>\n</li>\n<li><p>text indexex: 全文索引，一个document最多只能有一个全文索引，用于在文档中搜索文本，但创建索引的开销比较大，需要后台或离线创建，综合来说不如elasticsearch等搜索引擎</p>\n<p>在文本中subject和comments两个字段建立了全文索引“text”：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.reviews.createIndex(</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     subject: <span class=\"string\">\"text\"</span>,</span><br><span class=\"line\">     comments: <span class=\"string\">\"text\"</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> )</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hashed index：用于分布式collection分片的场景，将不同的document按照某个字段取hash的形式分布到不同的sharding上去：</p>\n<blockquote>\n<p>Hashed indexes support [sharding] using hashed shard keys. [Hashed based sharding] uses a hashed index of a field as the shard key to partition data across your sharded cluster.</p>\n</blockquote>\n<p>对name字段创建一个hashed索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"attr\">name</span>: <span class=\"string\">\"hashed\"</span> &#125; )</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"mongodb索引属性\"><a href=\"#mongodb索引属性\" class=\"headerlink\" title=\"mongodb索引属性\"></a>mongodb索引属性</h3><p>建立索引时可以对索引指定属性，有以下几种属性：</p>\n<ul>\n<li><p>TTL: 对索引设置过期时间shu，有两种方式</p>\n<ul>\n<li><p>延迟时间：在索引创建后延迟特定秒数然后删除该document：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"string\">\"createAt\"</span>: <span class=\"number\">1</span> &#125;, &#123; <span class=\"attr\">expireAfterSeconds</span>: <span class=\"number\">3600</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>createAt字段保存了创建时间，该document会在这个时间的3600秒后被删除</p>\n</li>\n<li><p>固定时间：在索引创建后延迟特定秒数然后删除该document:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"string\">\"expireTime\"</span>: <span class=\"number\">1</span> &#125;, &#123; <span class=\"attr\">expireAfterSeconds</span>: <span class=\"number\">0</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>该document将在到达expireTime上设置的时间时被删除</p>\n</li>\n</ul>\n</li>\n<li><p>unique index：唯一索引，索引字段唯一，若插入包含相同内容字段的document则会报错</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>&#125;, &#123;<span class=\"attr\">unique</span>: <span class=\"literal\">true</span>&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>spares indexes：稀疏索引，如果设置了稀疏索引，将只对有该索引字段的document启用索引</p>\n</li>\n<li><p>partial indexes： 部分索引，跟稀疏索引一样也是非完全索引，在创建索引时可以对索引字段设置一个范围，在这个范围内的才启用索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.restaurants.createIndex(</span><br><span class=\"line\">   &#123; <span class=\"attr\">cuisine</span>: <span class=\"number\">1</span>, <span class=\"attr\">name</span>: <span class=\"number\">1</span> &#125;,</span><br><span class=\"line\">   &#123; <span class=\"attr\">partialFilterExpression</span>: &#123; <span class=\"attr\">rating</span>: &#123; <span class=\"attr\">$gt</span>: <span class=\"number\">5</span> &#125; &#125; &#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>partialFilterExpression表示范围取值，rating是部分索引的索引字段，需要注意的是，进行查询时，查询范围不能超出部分索引的设定范围，不然无法启用索引</p>\n</li>\n<li><p>case insensitive indexes</p>\n</li>\n</ul>\n<h3 id=\"mongodb执行计划\"><a href=\"#mongodb执行计划\" class=\"headerlink\" title=\"mongodb执行计划\"></a>mongodb执行计划</h3><p>通过<code>explain()</code>执行计划进行mongodb的性能分析</p>\n","site":{"data":{}},"excerpt":"<p>mongodb索引数据结果是<code>b-树</code>，该数据结构作为索引具有高性能，磁盘io少等特点</p>\n<h3 id=\"何为b-树\"><a href=\"#何为b-树\" class=\"headerlink\" title=\"何为b-树\"></a>何为b-树</h3><p>b-树，又称b树，跟mysql的b+树索引不同，mongodb的b-树索引节点直接与数据绑定，找到索引后直接返回索引对应的document，这种索引适合mongodb这种聚合型nosql数据库：</p>","more":"<p><img src=\"\\blog\\images\\1547792452619.png\" alt=\"1547792452619\"></p>\n<ul>\n<li>b树节点元素个数为k，子节点个数为k+1</li>\n<li>如果b树的阶树为m，则<code>m/2 ≤ k ≤ m</code></li>\n<li>查询的时间复杂度为O(log n)，单次查询的磁盘io次数为数的高度</li>\n</ul>\n<h3 id=\"mongodb索引种类\"><a href=\"#mongodb索引种类\" class=\"headerlink\" title=\"mongodb索引种类\"></a>mongodb索引种类</h3><p>mongodb索引分为如下</p>\n<ul>\n<li><p>single index: 单个索引，索引字段只有一个：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>&#125;)</span><br></pre></td></tr></table></figure>\n<p>score是索引字段，1表示升序索引，-1表示降序索引：</p>\n</li>\n<li><p>compound indexex: 混合索引，跟mysql类似，也是由多个字段组成索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>,<span class=\"attr\">gender</span>:<span class=\"number\">1</span>&#125;)</span><br></pre></td></tr></table></figure>\n<p>使用混合索引时，有一些地方需要注意</p>\n<ul>\n<li><p>排序查找的顺序：</p>\n<p>官方文档提到：</p>\n<blockquote>\n<p>However, for [compound indexes], sort order can matter in determining whether the index can support a sort operation</p>\n<p>对于复合索引，排序字段的顺序对于排序操作是否启用索引非常重要</p>\n</blockquote>\n<p>对于如下索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex( &#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>对姓名作升序索引，对性别做降序索引，那么在进行以下两种排序时，都会启用索引</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">-1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;)</span><br><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125;)</span><br></pre></td></tr></table></figure>\n<p>用<code>explain(&quot;allPlansExecution&quot;)</code>查看执行计划发现，totalKeysExamined是所需结果的数目，说明全部查询均走的索引，执行阶段首先执行IXSCAN即扫描索引确定key的位置，然后执行FETCH去检索指定的document。</p>\n<p>但对以下排序，是不会启用索引的：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;)</span><br><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).find(&#123;&#125;).sort(&#123; <span class=\"attr\">name</span> : <span class=\"number\">-1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">-1</span> &#125;)</span><br></pre></td></tr></table></figure>\n<p>用<code>explain(&quot;allPlansExecution&quot;)</code>查看执行计划发现，totalKeysExamined是0，执行阶段首先执行COLLSCAN即全表扫描，然后进行排序；说明如果排序查找如果跟索引的排序情况不匹配的话，将放弃启用索引。</p>\n</li>\n<li><p>左子前缀子集：</p>\n<p>这样的索引：<code>{ name : 1, gender : 1 }</code>，包含以下的前缀子集：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>&#125;</span><br><span class=\"line\">&#123; <span class=\"attr\">name</span> : <span class=\"number\">1</span>, <span class=\"attr\">gender</span> : <span class=\"number\">1</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>但不包含以下形式:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; <span class=\"attr\">gender</span> : <span class=\"number\">1</span>&#125;</span><br><span class=\"line\">&#123; <span class=\"attr\">gender</span> : <span class=\"number\">1</span>, <span class=\"attr\">name</span> : <span class=\"number\">1</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>使用的时候要特别注意能否匹配上索引的前缀子集</p>\n</li>\n</ul>\n</li>\n<li><p>multikey indexes: 多key型索引，索引字段通常是数组类型</p>\n</li>\n<li><p>geospatial indexes: 地理位置索引，包括2dsphere和2d还有geoHaystack三种类型</p>\n</li>\n<li><p>text indexex: 全文索引，一个document最多只能有一个全文索引，用于在文档中搜索文本，但创建索引的开销比较大，需要后台或离线创建，综合来说不如elasticsearch等搜索引擎</p>\n<p>在文本中subject和comments两个字段建立了全文索引“text”：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.reviews.createIndex(</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     subject: <span class=\"string\">\"text\"</span>,</span><br><span class=\"line\">     comments: <span class=\"string\">\"text\"</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> )</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hashed index：用于分布式collection分片的场景，将不同的document按照某个字段取hash的形式分布到不同的sharding上去：</p>\n<blockquote>\n<p>Hashed indexes support [sharding] using hashed shard keys. [Hashed based sharding] uses a hashed index of a field as the shard key to partition data across your sharded cluster.</p>\n</blockquote>\n<p>对name字段创建一个hashed索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"attr\">name</span>: <span class=\"string\">\"hashed\"</span> &#125; )</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"mongodb索引属性\"><a href=\"#mongodb索引属性\" class=\"headerlink\" title=\"mongodb索引属性\"></a>mongodb索引属性</h3><p>建立索引时可以对索引指定属性，有以下几种属性：</p>\n<ul>\n<li><p>TTL: 对索引设置过期时间shu，有两种方式</p>\n<ul>\n<li><p>延迟时间：在索引创建后延迟特定秒数然后删除该document：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"string\">\"createAt\"</span>: <span class=\"number\">1</span> &#125;, &#123; <span class=\"attr\">expireAfterSeconds</span>: <span class=\"number\">3600</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>createAt字段保存了创建时间，该document会在这个时间的3600秒后被删除</p>\n</li>\n<li><p>固定时间：在索引创建后延迟特定秒数然后删除该document:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.collection.createIndex( &#123; <span class=\"string\">\"expireTime\"</span>: <span class=\"number\">1</span> &#125;, &#123; <span class=\"attr\">expireAfterSeconds</span>: <span class=\"number\">0</span> &#125; )</span><br></pre></td></tr></table></figure>\n<p>该document将在到达expireTime上设置的时间时被删除</p>\n</li>\n</ul>\n</li>\n<li><p>unique index：唯一索引，索引字段唯一，若插入包含相同内容字段的document则会报错</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.getCollection(<span class=\"string\">'user'</span>).createIndex(&#123;<span class=\"attr\">name</span>:<span class=\"number\">1</span>&#125;, &#123;<span class=\"attr\">unique</span>: <span class=\"literal\">true</span>&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>spares indexes：稀疏索引，如果设置了稀疏索引，将只对有该索引字段的document启用索引</p>\n</li>\n<li><p>partial indexes： 部分索引，跟稀疏索引一样也是非完全索引，在创建索引时可以对索引字段设置一个范围，在这个范围内的才启用索引：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.restaurants.createIndex(</span><br><span class=\"line\">   &#123; <span class=\"attr\">cuisine</span>: <span class=\"number\">1</span>, <span class=\"attr\">name</span>: <span class=\"number\">1</span> &#125;,</span><br><span class=\"line\">   &#123; <span class=\"attr\">partialFilterExpression</span>: &#123; <span class=\"attr\">rating</span>: &#123; <span class=\"attr\">$gt</span>: <span class=\"number\">5</span> &#125; &#125; &#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>partialFilterExpression表示范围取值，rating是部分索引的索引字段，需要注意的是，进行查询时，查询范围不能超出部分索引的设定范围，不然无法启用索引</p>\n</li>\n<li><p>case insensitive indexes</p>\n</li>\n</ul>\n<h3 id=\"mongodb执行计划\"><a href=\"#mongodb执行计划\" class=\"headerlink\" title=\"mongodb执行计划\"></a>mongodb执行计划</h3><p>通过<code>explain()</code>执行计划进行mongodb的性能分析</p>"},{"title":"netty-ByteBuf浅析","author":"天渊","date":"2019-02-12T09:59:00.000Z","_content":"netty使用`ByteBuf`来提代jdk自带的`ByteBuffer`作为nio的数据传输载体，相比于jdk原生实现，功能更加丰富，灵活性更强<!-- more -->，具有以下优点：\n\n- 扩展性好，用户可自定义所需要的缓冲区实现\n- 内置复合缓冲区实现了零拷贝功能\n- 容量按需增长\n- 读数据和写数据有独立的index，互相隔离，互不干扰\n- 支持引用计数和池化\n\n在netty中`ByteBuf`有三种实现：`heapBuffer`，`directBuffer`，`compositeBuffer`，通常情况下使用directBuffer：\n\n1. **heapBuffer**：即将数据存储通过java Byte数组的方式（称为支撑数组）存储在jvm heap中，使用以下方式快速创建一个heapBuffer，但java进行io读写时仍然需要将堆内内存的数据拷贝到堆外并传递给底层的C库:\n\n   ```java\n   ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer();\n   // 可以直接将所需Byte数组拿出来\n   if (buffer.hasArray()) {\n   \tbyte[] bufferArray = buffer.array();\n   \tint offset = buffer.arrayOffset() + buffer.readerIndex();\n   \tint length = buffer.readableBytes();\n       // 通过读指针和可读长度获取所需的数据\n   \tbyte[] neededData = Arrays.copyOfRange(bufferArray, offset, offset + length);\n   }\n   ```\n\n2. **directBuffer**：使用堆外内存存储数据，直接使用堆外内存进行io操作，好处是比`heapBuffer`少一次内存拷贝且在io操作频繁的时候大大降低了gc压力，缺点是需要手动释放内存空间：\n\n   ```java\n   ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer();\n   ```\n\n   `directBuffer`没有支撑数组，因此不能直接提取Byte数组，需要通过读写指针取数据\n\n3. **compositeBuffer**：复合buffer，其中可同时包含堆内数据和堆外数据，其实现是`ByteBuf`的子类：`CompositeByteBuf`，通过以下方式组装一个复合buffer，访问复合buffer的方式也类似于`directBuffer`，不能直接访问其支撑数组：\n\n   ```java\n   CompositeByteBuf compBuf = ByteBufAllocator.DEFAULT.compositeBuffer();\n   compBuf.addComponents(buffer, heapBuffer);\n   ```\n\n   复合buffer广泛运用于需要组合多种不同数据源的buffer，在对不同数据源的数据进行整合后提供统一的ByteBuf API供用户使用\n\n\n\n### ByteBuf字节操作\n\n以下操作均以`directBuffer`为例\n\n","source":"_posts/netty-ByteBuf浅析.md","raw":"title: netty-ByteBuf浅析\nauthor: 天渊\ntags:\n  - Netty\n  - Java\ncategories:\n  - 基础知识\ndate: 2019-02-12 17:59:00\n---\nnetty使用`ByteBuf`来提代jdk自带的`ByteBuffer`作为nio的数据传输载体，相比于jdk原生实现，功能更加丰富，灵活性更强<!-- more -->，具有以下优点：\n\n- 扩展性好，用户可自定义所需要的缓冲区实现\n- 内置复合缓冲区实现了零拷贝功能\n- 容量按需增长\n- 读数据和写数据有独立的index，互相隔离，互不干扰\n- 支持引用计数和池化\n\n在netty中`ByteBuf`有三种实现：`heapBuffer`，`directBuffer`，`compositeBuffer`，通常情况下使用directBuffer：\n\n1. **heapBuffer**：即将数据存储通过java Byte数组的方式（称为支撑数组）存储在jvm heap中，使用以下方式快速创建一个heapBuffer，但java进行io读写时仍然需要将堆内内存的数据拷贝到堆外并传递给底层的C库:\n\n   ```java\n   ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer();\n   // 可以直接将所需Byte数组拿出来\n   if (buffer.hasArray()) {\n   \tbyte[] bufferArray = buffer.array();\n   \tint offset = buffer.arrayOffset() + buffer.readerIndex();\n   \tint length = buffer.readableBytes();\n       // 通过读指针和可读长度获取所需的数据\n   \tbyte[] neededData = Arrays.copyOfRange(bufferArray, offset, offset + length);\n   }\n   ```\n\n2. **directBuffer**：使用堆外内存存储数据，直接使用堆外内存进行io操作，好处是比`heapBuffer`少一次内存拷贝且在io操作频繁的时候大大降低了gc压力，缺点是需要手动释放内存空间：\n\n   ```java\n   ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer();\n   ```\n\n   `directBuffer`没有支撑数组，因此不能直接提取Byte数组，需要通过读写指针取数据\n\n3. **compositeBuffer**：复合buffer，其中可同时包含堆内数据和堆外数据，其实现是`ByteBuf`的子类：`CompositeByteBuf`，通过以下方式组装一个复合buffer，访问复合buffer的方式也类似于`directBuffer`，不能直接访问其支撑数组：\n\n   ```java\n   CompositeByteBuf compBuf = ByteBufAllocator.DEFAULT.compositeBuffer();\n   compBuf.addComponents(buffer, heapBuffer);\n   ```\n\n   复合buffer广泛运用于需要组合多种不同数据源的buffer，在对不同数据源的数据进行整合后提供统一的ByteBuf API供用户使用\n\n\n\n### ByteBuf字节操作\n\n以下操作均以`directBuffer`为例\n\n","slug":"netty-ByteBuf浅析","published":1,"updated":"2019-03-19T13:30:58.475Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js410018g0qrf2bwtycy","content":"<p>netty使用<code>ByteBuf</code>来提代jdk自带的<code>ByteBuffer</code>作为nio的数据传输载体，相比于jdk原生实现，功能更加丰富，灵活性更强<a id=\"more\"></a>，具有以下优点：</p>\n<ul>\n<li>扩展性好，用户可自定义所需要的缓冲区实现</li>\n<li>内置复合缓冲区实现了零拷贝功能</li>\n<li>容量按需增长</li>\n<li>读数据和写数据有独立的index，互相隔离，互不干扰</li>\n<li>支持引用计数和池化</li>\n</ul>\n<p>在netty中<code>ByteBuf</code>有三种实现：<code>heapBuffer</code>，<code>directBuffer</code>，<code>compositeBuffer</code>，通常情况下使用directBuffer：</p>\n<ol>\n<li><p><strong>heapBuffer</strong>：即将数据存储通过java Byte数组的方式（称为支撑数组）存储在jvm heap中，使用以下方式快速创建一个heapBuffer，但java进行io读写时仍然需要将堆内内存的数据拷贝到堆外并传递给底层的C库:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer();</span><br><span class=\"line\"><span class=\"comment\">// 可以直接将所需Byte数组拿出来</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (buffer.hasArray()) &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] bufferArray = buffer.array();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> offset = buffer.arrayOffset() + buffer.readerIndex();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> length = buffer.readableBytes();</span><br><span class=\"line\">    <span class=\"comment\">// 通过读指针和可读长度获取所需的数据</span></span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] neededData = Arrays.copyOfRange(bufferArray, offset, offset + length);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>directBuffer</strong>：使用堆外内存存储数据，直接使用堆外内存进行io操作，好处是比<code>heapBuffer</code>少一次内存拷贝且在io操作频繁的时候大大降低了gc压力，缺点是需要手动释放内存空间：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer();</span><br></pre></td></tr></table></figure>\n<p><code>directBuffer</code>没有支撑数组，因此不能直接提取Byte数组，需要通过读写指针取数据</p>\n</li>\n<li><p><strong>compositeBuffer</strong>：复合buffer，其中可同时包含堆内数据和堆外数据，其实现是<code>ByteBuf</code>的子类：<code>CompositeByteBuf</code>，通过以下方式组装一个复合buffer，访问复合buffer的方式也类似于<code>directBuffer</code>，不能直接访问其支撑数组：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CompositeByteBuf compBuf = ByteBufAllocator.DEFAULT.compositeBuffer();</span><br><span class=\"line\">compBuf.addComponents(buffer, heapBuffer);</span><br></pre></td></tr></table></figure>\n<p>复合buffer广泛运用于需要组合多种不同数据源的buffer，在对不同数据源的数据进行整合后提供统一的ByteBuf API供用户使用</p>\n</li>\n</ol>\n<h3 id=\"ByteBuf字节操作\"><a href=\"#ByteBuf字节操作\" class=\"headerlink\" title=\"ByteBuf字节操作\"></a>ByteBuf字节操作</h3><p>以下操作均以<code>directBuffer</code>为例</p>\n","site":{"data":{}},"excerpt":"<p>netty使用<code>ByteBuf</code>来提代jdk自带的<code>ByteBuffer</code>作为nio的数据传输载体，相比于jdk原生实现，功能更加丰富，灵活性更强","more":"，具有以下优点：</p>\n<ul>\n<li>扩展性好，用户可自定义所需要的缓冲区实现</li>\n<li>内置复合缓冲区实现了零拷贝功能</li>\n<li>容量按需增长</li>\n<li>读数据和写数据有独立的index，互相隔离，互不干扰</li>\n<li>支持引用计数和池化</li>\n</ul>\n<p>在netty中<code>ByteBuf</code>有三种实现：<code>heapBuffer</code>，<code>directBuffer</code>，<code>compositeBuffer</code>，通常情况下使用directBuffer：</p>\n<ol>\n<li><p><strong>heapBuffer</strong>：即将数据存储通过java Byte数组的方式（称为支撑数组）存储在jvm heap中，使用以下方式快速创建一个heapBuffer，但java进行io读写时仍然需要将堆内内存的数据拷贝到堆外并传递给底层的C库:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer();</span><br><span class=\"line\"><span class=\"comment\">// 可以直接将所需Byte数组拿出来</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (buffer.hasArray()) &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] bufferArray = buffer.array();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> offset = buffer.arrayOffset() + buffer.readerIndex();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> length = buffer.readableBytes();</span><br><span class=\"line\">    <span class=\"comment\">// 通过读指针和可读长度获取所需的数据</span></span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] neededData = Arrays.copyOfRange(bufferArray, offset, offset + length);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>directBuffer</strong>：使用堆外内存存储数据，直接使用堆外内存进行io操作，好处是比<code>heapBuffer</code>少一次内存拷贝且在io操作频繁的时候大大降低了gc压力，缺点是需要手动释放内存空间：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer();</span><br></pre></td></tr></table></figure>\n<p><code>directBuffer</code>没有支撑数组，因此不能直接提取Byte数组，需要通过读写指针取数据</p>\n</li>\n<li><p><strong>compositeBuffer</strong>：复合buffer，其中可同时包含堆内数据和堆外数据，其实现是<code>ByteBuf</code>的子类：<code>CompositeByteBuf</code>，通过以下方式组装一个复合buffer，访问复合buffer的方式也类似于<code>directBuffer</code>，不能直接访问其支撑数组：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CompositeByteBuf compBuf = ByteBufAllocator.DEFAULT.compositeBuffer();</span><br><span class=\"line\">compBuf.addComponents(buffer, heapBuffer);</span><br></pre></td></tr></table></figure>\n<p>复合buffer广泛运用于需要组合多种不同数据源的buffer，在对不同数据源的数据进行整合后提供统一的ByteBuf API供用户使用</p>\n</li>\n</ol>\n<h3 id=\"ByteBuf字节操作\"><a href=\"#ByteBuf字节操作\" class=\"headerlink\" title=\"ByteBuf字节操作\"></a>ByteBuf字节操作</h3><p>以下操作均以<code>directBuffer</code>为例</p>"},{"title":"如何优雅地遍历并删除一个map中的元素","author":"","date":"2019-03-21T03:18:00.000Z","_content":"最近在实践基于netty造一个http服务器，需要实现`session`功能，需要有一个异步线程定期检查sessionMap中哪些session过期，过期的session需要删除，这个过程需要一边遍历Map一边删除元素，趁此机会探索一下如何优雅地实现这个功能\n\n<!--more-->\n\n## 单线程环境(HashMap)\n\n不考虑多线程的情况，单线程的情况下使用`HashMap`存储并遍历元素有以下方式：\n\n### 对Entry作foreach遍历\n\n对HashMap的Entry作foreach遍历，遍历时如果value为\"111\"则进行remove：\n\n```java\nfor (Map.Entry<String, Object> entry : hashMap.entrySet()) {\n    if (\"111\".equals(entry.getValue())) {\n        hashMap.remove(entry.getKey());\n    }\n}\n```\n\n显而易见这种情况会触发`ConcurrentModificationException`，因为foreach实际上是调用`EntrySet`的迭代器，也就是`HashMap`内部的实现：`EntryIterator`，在进行遍历时会调用这个迭代器的`next()`方法，最终调用的是`HashIterator`的`nextNode()`方法：\n\n```java\nfinal Node<K,V> nextNode() {\n    Node<K,V>[] t;\n    Node<K,V> e = next;\n    // 如果当前modCount与expectedModCount不匹配则抛异常\n    // hashmap的remove()方法会改变modCount，所以这个时候通过此方式遍历当然会报错了\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n\t// ...略\n    return e;\n}\n```\n\n`expectedModCount`即预期`modCount`，在初始化迭代器时一同初始化，如果通过迭代器进行遍历时改变了`modCount`就会出现经典的`ConcurrentModificationException`，这个异常在许多其他容器的遍历过程中都会存在，其用意就是让用户另外选择更为安全的遍历方式\n\n由于`hashMap.remove(entry.getKey())`这个操作会更新`modCount`，所以`ConcurrentModificationException`就免不了啦\n\n### 对Entry作foreach遍历（entrySet.forEach(lambda)方式）\n\n使用`EntrySet`自己实现的`forEach()`方法进行遍历，jdk8更新中与lambda表达式一起新加入的，看上去跟上一种方法差不多：\n\n```java\nhashMap.entrySet().forEach(entry -> {\n    if (\"111\".equals(entry.getValue())) {\n        hashMap.remove(entry.getKey());\n    }\n});\n```\n\n使用该方式遍历并删除元素，效果跟上一种一样，当然如预期一样抛出了`ConcurrentModificationException`异常，`EntrySet`自己的`forEach()`也是检测`modCount`在遍历时有没有改动：\n\n```java\npublic final void forEach(Consumer<? super Map.Entry<K,V>> action) {\n    Node<K,V>[] tab;\n    if (action == null)\n        throw new NullPointerException();\n    if (size > 0 && (tab = table) != null) {\n        int mc = modCount;\n        // 用for循环进行遍历，并执行lambda过程\n        for (int i = 0; i < tab.length; ++i) {\n            for (Node<K,V> e = tab[i]; e != null; e = e.next)\n                action.accept(e);\n        }\n        // 遍历完了看看modCount有没有改动，有改动就抛异常\n        if (modCount != mc)\n            throw new ConcurrentModificationException();\n    }\n}\n```\n\n这种实现方式倒是没有上一种曲折，不过没有达到我们的目的\n\n### 对HashMap作foreach遍历(hashMap.forEach(lambda)方式)\n\n这次直接调用`HashMap`自己的`foreach`方法:\n\n```java\nhashMap.forEach((key, value) -> {\n    if (\"111\".equals(value)) {\n        hashMap.remove(key);\n    }\n});\n```\n\n更简洁了，充分体验了lambda的好处，不过`ConcurrentModificationException`还是跑不了，继续探索吧\n\n### 显式的调用EntryIterator并EntryIterator.remove()\n\n翻翻`EntryIterator`的源码可以发现他的父类`HashIterator`实现了一个`remove()`方法，想到`List`的各个实现类都实现了自己的迭代器，以支持在遍历过程中对元素进行新增或者删除，举一反三可以想想现在这个`remove()`方法是不是可以达到我们的目的：\n\n```java\nHashMap<String, Object> hashMap = new HashMap<>();\nhashMap.put(\"hahah\", \"111\");\nhashMap.put(\"hehehe\", \"222\");\nhashMap.put(\"heiheihei\", \"333\");\nSystem.out.println(hashMap);\nIterator<Map.Entry<String, Object>> entryIterator = hashMap.entrySet().iterator();\n// 显示地执行迭代器的迭代方法\nwhile (entryIterator.hasNext()) {\n    Map.Entry<String, Object> next = entryIterator.next();\n    if (\"111\".equals(next.getValue())) {\n        // 使用HashIterator自己的remove，而不是HashMap的remove\n        entryIterator.remove();\n    }\n}\nSystem.out.println(hashMap);\n```\n\n结果如下，未曾抛出任何异常，说明这种方式是切实有效的：\n\n```java\n{hahah=111, heiheihei=333, hehehe=222}\n{heiheihei=333, hehehe=222}\n\nProcess finished with exit code 0\n```\n\n`HashIterator`的`remove()`为了支持遍历时删除，对`expectedModCount`做了一些手脚，源码如下：\n\n```java\npublic final void remove() {\n    Node<K,V> p = current;\n    if (p == null)\n        throw new IllegalStateException();\n    // 删除操作还未进行的时候发现modCount被修改了就抛异常\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n    current = null;\n    K key = p.key;\n    removeNode(hash(key), key, null, false, false);\n    // 删除操作完成，把expectedModCount修改为最新的modCount\n    expectedModCount = modCount;\n}\n```\n\n在调用完`removeNode`方法（这个过程会修改`modCount`）后，再把`expectedModCount`修改为`removeNode`，保证后续检查`expectedModCount`不会出问题\n\n> 为什么如此设计？\n>\n> 迭代过程中调用`HashIterator`自己的remove，删除当前迭代指针指向的元素，可以保证这个操作是不受外界影响的（比如其他线程的并发操作），hashMap.remove()就保证不了这个前提\n\n同理，`ListIterator`的设计思路也是基于相同的考虑\n\n### removeIf()\n\njdk8 引入了一个更为简练的方式：`removeIf`，是`Collection`的default方法，对元素进行遍历，满足条件就进行删除：\n\n```java\nhashMap.entrySet().removeIf(next -> \"111\".equals(next.getValue()));\n```\n\n进行该操作完全不需要考虑`ConcurrentModificationException`了，其内部也是调用的对应`Iterator`实现的`remove()`方法，原理跟上一种一致\n\n\n\n","source":"_posts/如何优雅地遍历并删除一个map中的元素.md","raw":"title: 如何优雅地遍历并删除一个map中的元素\ntags:\n  - Java\ncategories:\n  - 基础知识\nauthor: ''\ndate: 2019-03-21 11:18:00\n---\n最近在实践基于netty造一个http服务器，需要实现`session`功能，需要有一个异步线程定期检查sessionMap中哪些session过期，过期的session需要删除，这个过程需要一边遍历Map一边删除元素，趁此机会探索一下如何优雅地实现这个功能\n\n<!--more-->\n\n## 单线程环境(HashMap)\n\n不考虑多线程的情况，单线程的情况下使用`HashMap`存储并遍历元素有以下方式：\n\n### 对Entry作foreach遍历\n\n对HashMap的Entry作foreach遍历，遍历时如果value为\"111\"则进行remove：\n\n```java\nfor (Map.Entry<String, Object> entry : hashMap.entrySet()) {\n    if (\"111\".equals(entry.getValue())) {\n        hashMap.remove(entry.getKey());\n    }\n}\n```\n\n显而易见这种情况会触发`ConcurrentModificationException`，因为foreach实际上是调用`EntrySet`的迭代器，也就是`HashMap`内部的实现：`EntryIterator`，在进行遍历时会调用这个迭代器的`next()`方法，最终调用的是`HashIterator`的`nextNode()`方法：\n\n```java\nfinal Node<K,V> nextNode() {\n    Node<K,V>[] t;\n    Node<K,V> e = next;\n    // 如果当前modCount与expectedModCount不匹配则抛异常\n    // hashmap的remove()方法会改变modCount，所以这个时候通过此方式遍历当然会报错了\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n\t// ...略\n    return e;\n}\n```\n\n`expectedModCount`即预期`modCount`，在初始化迭代器时一同初始化，如果通过迭代器进行遍历时改变了`modCount`就会出现经典的`ConcurrentModificationException`，这个异常在许多其他容器的遍历过程中都会存在，其用意就是让用户另外选择更为安全的遍历方式\n\n由于`hashMap.remove(entry.getKey())`这个操作会更新`modCount`，所以`ConcurrentModificationException`就免不了啦\n\n### 对Entry作foreach遍历（entrySet.forEach(lambda)方式）\n\n使用`EntrySet`自己实现的`forEach()`方法进行遍历，jdk8更新中与lambda表达式一起新加入的，看上去跟上一种方法差不多：\n\n```java\nhashMap.entrySet().forEach(entry -> {\n    if (\"111\".equals(entry.getValue())) {\n        hashMap.remove(entry.getKey());\n    }\n});\n```\n\n使用该方式遍历并删除元素，效果跟上一种一样，当然如预期一样抛出了`ConcurrentModificationException`异常，`EntrySet`自己的`forEach()`也是检测`modCount`在遍历时有没有改动：\n\n```java\npublic final void forEach(Consumer<? super Map.Entry<K,V>> action) {\n    Node<K,V>[] tab;\n    if (action == null)\n        throw new NullPointerException();\n    if (size > 0 && (tab = table) != null) {\n        int mc = modCount;\n        // 用for循环进行遍历，并执行lambda过程\n        for (int i = 0; i < tab.length; ++i) {\n            for (Node<K,V> e = tab[i]; e != null; e = e.next)\n                action.accept(e);\n        }\n        // 遍历完了看看modCount有没有改动，有改动就抛异常\n        if (modCount != mc)\n            throw new ConcurrentModificationException();\n    }\n}\n```\n\n这种实现方式倒是没有上一种曲折，不过没有达到我们的目的\n\n### 对HashMap作foreach遍历(hashMap.forEach(lambda)方式)\n\n这次直接调用`HashMap`自己的`foreach`方法:\n\n```java\nhashMap.forEach((key, value) -> {\n    if (\"111\".equals(value)) {\n        hashMap.remove(key);\n    }\n});\n```\n\n更简洁了，充分体验了lambda的好处，不过`ConcurrentModificationException`还是跑不了，继续探索吧\n\n### 显式的调用EntryIterator并EntryIterator.remove()\n\n翻翻`EntryIterator`的源码可以发现他的父类`HashIterator`实现了一个`remove()`方法，想到`List`的各个实现类都实现了自己的迭代器，以支持在遍历过程中对元素进行新增或者删除，举一反三可以想想现在这个`remove()`方法是不是可以达到我们的目的：\n\n```java\nHashMap<String, Object> hashMap = new HashMap<>();\nhashMap.put(\"hahah\", \"111\");\nhashMap.put(\"hehehe\", \"222\");\nhashMap.put(\"heiheihei\", \"333\");\nSystem.out.println(hashMap);\nIterator<Map.Entry<String, Object>> entryIterator = hashMap.entrySet().iterator();\n// 显示地执行迭代器的迭代方法\nwhile (entryIterator.hasNext()) {\n    Map.Entry<String, Object> next = entryIterator.next();\n    if (\"111\".equals(next.getValue())) {\n        // 使用HashIterator自己的remove，而不是HashMap的remove\n        entryIterator.remove();\n    }\n}\nSystem.out.println(hashMap);\n```\n\n结果如下，未曾抛出任何异常，说明这种方式是切实有效的：\n\n```java\n{hahah=111, heiheihei=333, hehehe=222}\n{heiheihei=333, hehehe=222}\n\nProcess finished with exit code 0\n```\n\n`HashIterator`的`remove()`为了支持遍历时删除，对`expectedModCount`做了一些手脚，源码如下：\n\n```java\npublic final void remove() {\n    Node<K,V> p = current;\n    if (p == null)\n        throw new IllegalStateException();\n    // 删除操作还未进行的时候发现modCount被修改了就抛异常\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n    current = null;\n    K key = p.key;\n    removeNode(hash(key), key, null, false, false);\n    // 删除操作完成，把expectedModCount修改为最新的modCount\n    expectedModCount = modCount;\n}\n```\n\n在调用完`removeNode`方法（这个过程会修改`modCount`）后，再把`expectedModCount`修改为`removeNode`，保证后续检查`expectedModCount`不会出问题\n\n> 为什么如此设计？\n>\n> 迭代过程中调用`HashIterator`自己的remove，删除当前迭代指针指向的元素，可以保证这个操作是不受外界影响的（比如其他线程的并发操作），hashMap.remove()就保证不了这个前提\n\n同理，`ListIterator`的设计思路也是基于相同的考虑\n\n### removeIf()\n\njdk8 引入了一个更为简练的方式：`removeIf`，是`Collection`的default方法，对元素进行遍历，满足条件就进行删除：\n\n```java\nhashMap.entrySet().removeIf(next -> \"111\".equals(next.getValue()));\n```\n\n进行该操作完全不需要考虑`ConcurrentModificationException`了，其内部也是调用的对应`Iterator`实现的`remove()`方法，原理跟上一种一致\n\n\n\n","slug":"如何优雅地遍历并删除一个map中的元素","published":1,"updated":"2019-03-20T16:32:02.935Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js42001ag0qrsqmhoq95","content":"<p>最近在实践基于netty造一个http服务器，需要实现<code>session</code>功能，需要有一个异步线程定期检查sessionMap中哪些session过期，过期的session需要删除，这个过程需要一边遍历Map一边删除元素，趁此机会探索一下如何优雅地实现这个功能</p>\n<a id=\"more\"></a>\n<h2 id=\"单线程环境-HashMap\"><a href=\"#单线程环境-HashMap\" class=\"headerlink\" title=\"单线程环境(HashMap)\"></a>单线程环境(HashMap)</h2><p>不考虑多线程的情况，单线程的情况下使用<code>HashMap</code>存储并遍历元素有以下方式：</p>\n<h3 id=\"对Entry作foreach遍历\"><a href=\"#对Entry作foreach遍历\" class=\"headerlink\" title=\"对Entry作foreach遍历\"></a>对Entry作foreach遍历</h3><p>对HashMap的Entry作foreach遍历，遍历时如果value为”111”则进行remove：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (Map.Entry&lt;String, Object&gt; entry : hashMap.entrySet()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(entry.getValue())) &#123;</span><br><span class=\"line\">        hashMap.remove(entry.getKey());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>显而易见这种情况会触发<code>ConcurrentModificationException</code>，因为foreach实际上是调用<code>EntrySet</code>的迭代器，也就是<code>HashMap</code>内部的实现：<code>EntryIterator</code>，在进行遍历时会调用这个迭代器的<code>next()</code>方法，最终调用的是<code>HashIterator</code>的<code>nextNode()</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Node&lt;K,V&gt; <span class=\"title\">nextNode</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt;[] t;</span><br><span class=\"line\">    Node&lt;K,V&gt; e = next;</span><br><span class=\"line\">    <span class=\"comment\">// 如果当前modCount与expectedModCount不匹配则抛异常</span></span><br><span class=\"line\">    <span class=\"comment\">// hashmap的remove()方法会改变modCount，所以这个时候通过此方式遍历当然会报错了</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">\t<span class=\"comment\">// ...略</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> e;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>expectedModCount</code>即预期<code>modCount</code>，在初始化迭代器时一同初始化，如果通过迭代器进行遍历时改变了<code>modCount</code>就会出现经典的<code>ConcurrentModificationException</code>，这个异常在许多其他容器的遍历过程中都会存在，其用意就是让用户另外选择更为安全的遍历方式</p>\n<p>由于<code>hashMap.remove(entry.getKey())</code>这个操作会更新<code>modCount</code>，所以<code>ConcurrentModificationException</code>就免不了啦</p>\n<h3 id=\"对Entry作foreach遍历（entrySet-forEach-lambda-方式）\"><a href=\"#对Entry作foreach遍历（entrySet-forEach-lambda-方式）\" class=\"headerlink\" title=\"对Entry作foreach遍历（entrySet.forEach(lambda)方式）\"></a>对Entry作foreach遍历（entrySet.forEach(lambda)方式）</h3><p>使用<code>EntrySet</code>自己实现的<code>forEach()</code>方法进行遍历，jdk8更新中与lambda表达式一起新加入的，看上去跟上一种方法差不多：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.entrySet().forEach(entry -&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(entry.getValue())) &#123;</span><br><span class=\"line\">        hashMap.remove(entry.getKey());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>使用该方式遍历并删除元素，效果跟上一种一样，当然如预期一样抛出了<code>ConcurrentModificationException</code>异常，<code>EntrySet</code>自己的<code>forEach()</code>也是检测<code>modCount</code>在遍历时有没有改动：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">forEach</span><span class=\"params\">(Consumer&lt;? <span class=\"keyword\">super</span> Map.Entry&lt;K,V&gt;&gt; action)</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt;[] tab;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (action == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (size &gt; <span class=\"number\">0</span> &amp;&amp; (tab = table) != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mc = modCount;</span><br><span class=\"line\">        <span class=\"comment\">// 用for循环进行遍历，并执行lambda过程</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; tab.length; ++i) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Node&lt;K,V&gt; e = tab[i]; e != <span class=\"keyword\">null</span>; e = e.next)</span><br><span class=\"line\">                action.accept(e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 遍历完了看看modCount有没有改动，有改动就抛异常</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (modCount != mc)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这种实现方式倒是没有上一种曲折，不过没有达到我们的目的</p>\n<h3 id=\"对HashMap作foreach遍历-hashMap-forEach-lambda-方式\"><a href=\"#对HashMap作foreach遍历-hashMap-forEach-lambda-方式\" class=\"headerlink\" title=\"对HashMap作foreach遍历(hashMap.forEach(lambda)方式)\"></a>对HashMap作foreach遍历(hashMap.forEach(lambda)方式)</h3><p>这次直接调用<code>HashMap</code>自己的<code>foreach</code>方法:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.forEach((key, value) -&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(value)) &#123;</span><br><span class=\"line\">        hashMap.remove(key);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>更简洁了，充分体验了lambda的好处，不过<code>ConcurrentModificationException</code>还是跑不了，继续探索吧</p>\n<h3 id=\"显式的调用EntryIterator并EntryIterator-remove\"><a href=\"#显式的调用EntryIterator并EntryIterator-remove\" class=\"headerlink\" title=\"显式的调用EntryIterator并EntryIterator.remove()\"></a>显式的调用EntryIterator并EntryIterator.remove()</h3><p>翻翻<code>EntryIterator</code>的源码可以发现他的父类<code>HashIterator</code>实现了一个<code>remove()</code>方法，想到<code>List</code>的各个实现类都实现了自己的迭代器，以支持在遍历过程中对元素进行新增或者删除，举一反三可以想想现在这个<code>remove()</code>方法是不是可以达到我们的目的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HashMap&lt;String, Object&gt; hashMap = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"hahah\"</span>, <span class=\"string\">\"111\"</span>);</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"hehehe\"</span>, <span class=\"string\">\"222\"</span>);</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"heiheihei\"</span>, <span class=\"string\">\"333\"</span>);</span><br><span class=\"line\">System.out.println(hashMap);</span><br><span class=\"line\">Iterator&lt;Map.Entry&lt;String, Object&gt;&gt; entryIterator = hashMap.entrySet().iterator();</span><br><span class=\"line\"><span class=\"comment\">// 显示地执行迭代器的迭代方法</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (entryIterator.hasNext()) &#123;</span><br><span class=\"line\">    Map.Entry&lt;String, Object&gt; next = entryIterator.next();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(next.getValue())) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 使用HashIterator自己的remove，而不是HashMap的remove</span></span><br><span class=\"line\">        entryIterator.remove();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">System.out.println(hashMap);</span><br></pre></td></tr></table></figure>\n<p>结果如下，未曾抛出任何异常，说明这种方式是切实有效的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;hahah=<span class=\"number\">111</span>, heiheihei=<span class=\"number\">333</span>, hehehe=<span class=\"number\">222</span>&#125;</span><br><span class=\"line\">&#123;heiheihei=<span class=\"number\">333</span>, hehehe=<span class=\"number\">222</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p><code>HashIterator</code>的<code>remove()</code>为了支持遍历时删除，对<code>expectedModCount</code>做了一些手脚，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt; p = current;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException();</span><br><span class=\"line\">    <span class=\"comment\">// 删除操作还未进行的时候发现modCount被修改了就抛异常</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">    current = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    K key = p.key;</span><br><span class=\"line\">    removeNode(hash(key), key, <span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 删除操作完成，把expectedModCount修改为最新的modCount</span></span><br><span class=\"line\">    expectedModCount = modCount;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在调用完<code>removeNode</code>方法（这个过程会修改<code>modCount</code>）后，再把<code>expectedModCount</code>修改为<code>removeNode</code>，保证后续检查<code>expectedModCount</code>不会出问题</p>\n<blockquote>\n<p>为什么如此设计？</p>\n<p>迭代过程中调用<code>HashIterator</code>自己的remove，删除当前迭代指针指向的元素，可以保证这个操作是不受外界影响的（比如其他线程的并发操作），hashMap.remove()就保证不了这个前提</p>\n</blockquote>\n<p>同理，<code>ListIterator</code>的设计思路也是基于相同的考虑</p>\n<h3 id=\"removeIf\"><a href=\"#removeIf\" class=\"headerlink\" title=\"removeIf()\"></a>removeIf()</h3><p>jdk8 引入了一个更为简练的方式：<code>removeIf</code>，是<code>Collection</code>的default方法，对元素进行遍历，满足条件就进行删除：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.entrySet().removeIf(next -&gt; <span class=\"string\">\"111\"</span>.equals(next.getValue()));</span><br></pre></td></tr></table></figure>\n<p>进行该操作完全不需要考虑<code>ConcurrentModificationException</code>了，其内部也是调用的对应<code>Iterator</code>实现的<code>remove()</code>方法，原理跟上一种一致</p>\n","site":{"data":{}},"excerpt":"<p>最近在实践基于netty造一个http服务器，需要实现<code>session</code>功能，需要有一个异步线程定期检查sessionMap中哪些session过期，过期的session需要删除，这个过程需要一边遍历Map一边删除元素，趁此机会探索一下如何优雅地实现这个功能</p>","more":"<h2 id=\"单线程环境-HashMap\"><a href=\"#单线程环境-HashMap\" class=\"headerlink\" title=\"单线程环境(HashMap)\"></a>单线程环境(HashMap)</h2><p>不考虑多线程的情况，单线程的情况下使用<code>HashMap</code>存储并遍历元素有以下方式：</p>\n<h3 id=\"对Entry作foreach遍历\"><a href=\"#对Entry作foreach遍历\" class=\"headerlink\" title=\"对Entry作foreach遍历\"></a>对Entry作foreach遍历</h3><p>对HashMap的Entry作foreach遍历，遍历时如果value为”111”则进行remove：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (Map.Entry&lt;String, Object&gt; entry : hashMap.entrySet()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(entry.getValue())) &#123;</span><br><span class=\"line\">        hashMap.remove(entry.getKey());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>显而易见这种情况会触发<code>ConcurrentModificationException</code>，因为foreach实际上是调用<code>EntrySet</code>的迭代器，也就是<code>HashMap</code>内部的实现：<code>EntryIterator</code>，在进行遍历时会调用这个迭代器的<code>next()</code>方法，最终调用的是<code>HashIterator</code>的<code>nextNode()</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Node&lt;K,V&gt; <span class=\"title\">nextNode</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt;[] t;</span><br><span class=\"line\">    Node&lt;K,V&gt; e = next;</span><br><span class=\"line\">    <span class=\"comment\">// 如果当前modCount与expectedModCount不匹配则抛异常</span></span><br><span class=\"line\">    <span class=\"comment\">// hashmap的remove()方法会改变modCount，所以这个时候通过此方式遍历当然会报错了</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">\t<span class=\"comment\">// ...略</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> e;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>expectedModCount</code>即预期<code>modCount</code>，在初始化迭代器时一同初始化，如果通过迭代器进行遍历时改变了<code>modCount</code>就会出现经典的<code>ConcurrentModificationException</code>，这个异常在许多其他容器的遍历过程中都会存在，其用意就是让用户另外选择更为安全的遍历方式</p>\n<p>由于<code>hashMap.remove(entry.getKey())</code>这个操作会更新<code>modCount</code>，所以<code>ConcurrentModificationException</code>就免不了啦</p>\n<h3 id=\"对Entry作foreach遍历（entrySet-forEach-lambda-方式）\"><a href=\"#对Entry作foreach遍历（entrySet-forEach-lambda-方式）\" class=\"headerlink\" title=\"对Entry作foreach遍历（entrySet.forEach(lambda)方式）\"></a>对Entry作foreach遍历（entrySet.forEach(lambda)方式）</h3><p>使用<code>EntrySet</code>自己实现的<code>forEach()</code>方法进行遍历，jdk8更新中与lambda表达式一起新加入的，看上去跟上一种方法差不多：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.entrySet().forEach(entry -&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(entry.getValue())) &#123;</span><br><span class=\"line\">        hashMap.remove(entry.getKey());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>使用该方式遍历并删除元素，效果跟上一种一样，当然如预期一样抛出了<code>ConcurrentModificationException</code>异常，<code>EntrySet</code>自己的<code>forEach()</code>也是检测<code>modCount</code>在遍历时有没有改动：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">forEach</span><span class=\"params\">(Consumer&lt;? <span class=\"keyword\">super</span> Map.Entry&lt;K,V&gt;&gt; action)</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt;[] tab;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (action == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> NullPointerException();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (size &gt; <span class=\"number\">0</span> &amp;&amp; (tab = table) != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mc = modCount;</span><br><span class=\"line\">        <span class=\"comment\">// 用for循环进行遍历，并执行lambda过程</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; tab.length; ++i) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Node&lt;K,V&gt; e = tab[i]; e != <span class=\"keyword\">null</span>; e = e.next)</span><br><span class=\"line\">                action.accept(e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 遍历完了看看modCount有没有改动，有改动就抛异常</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (modCount != mc)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这种实现方式倒是没有上一种曲折，不过没有达到我们的目的</p>\n<h3 id=\"对HashMap作foreach遍历-hashMap-forEach-lambda-方式\"><a href=\"#对HashMap作foreach遍历-hashMap-forEach-lambda-方式\" class=\"headerlink\" title=\"对HashMap作foreach遍历(hashMap.forEach(lambda)方式)\"></a>对HashMap作foreach遍历(hashMap.forEach(lambda)方式)</h3><p>这次直接调用<code>HashMap</code>自己的<code>foreach</code>方法:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.forEach((key, value) -&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(value)) &#123;</span><br><span class=\"line\">        hashMap.remove(key);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>更简洁了，充分体验了lambda的好处，不过<code>ConcurrentModificationException</code>还是跑不了，继续探索吧</p>\n<h3 id=\"显式的调用EntryIterator并EntryIterator-remove\"><a href=\"#显式的调用EntryIterator并EntryIterator-remove\" class=\"headerlink\" title=\"显式的调用EntryIterator并EntryIterator.remove()\"></a>显式的调用EntryIterator并EntryIterator.remove()</h3><p>翻翻<code>EntryIterator</code>的源码可以发现他的父类<code>HashIterator</code>实现了一个<code>remove()</code>方法，想到<code>List</code>的各个实现类都实现了自己的迭代器，以支持在遍历过程中对元素进行新增或者删除，举一反三可以想想现在这个<code>remove()</code>方法是不是可以达到我们的目的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HashMap&lt;String, Object&gt; hashMap = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"hahah\"</span>, <span class=\"string\">\"111\"</span>);</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"hehehe\"</span>, <span class=\"string\">\"222\"</span>);</span><br><span class=\"line\">hashMap.put(<span class=\"string\">\"heiheihei\"</span>, <span class=\"string\">\"333\"</span>);</span><br><span class=\"line\">System.out.println(hashMap);</span><br><span class=\"line\">Iterator&lt;Map.Entry&lt;String, Object&gt;&gt; entryIterator = hashMap.entrySet().iterator();</span><br><span class=\"line\"><span class=\"comment\">// 显示地执行迭代器的迭代方法</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (entryIterator.hasNext()) &#123;</span><br><span class=\"line\">    Map.Entry&lt;String, Object&gt; next = entryIterator.next();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"string\">\"111\"</span>.equals(next.getValue())) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 使用HashIterator自己的remove，而不是HashMap的remove</span></span><br><span class=\"line\">        entryIterator.remove();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">System.out.println(hashMap);</span><br></pre></td></tr></table></figure>\n<p>结果如下，未曾抛出任何异常，说明这种方式是切实有效的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;hahah=<span class=\"number\">111</span>, heiheihei=<span class=\"number\">333</span>, hehehe=<span class=\"number\">222</span>&#125;</span><br><span class=\"line\">&#123;heiheihei=<span class=\"number\">333</span>, hehehe=<span class=\"number\">222</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p><code>HashIterator</code>的<code>remove()</code>为了支持遍历时删除，对<code>expectedModCount</code>做了一些手脚，源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node&lt;K,V&gt; p = current;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException();</span><br><span class=\"line\">    <span class=\"comment\">// 删除操作还未进行的时候发现modCount被修改了就抛异常</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</span><br><span class=\"line\">    current = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    K key = p.key;</span><br><span class=\"line\">    removeNode(hash(key), key, <span class=\"keyword\">null</span>, <span class=\"keyword\">false</span>, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 删除操作完成，把expectedModCount修改为最新的modCount</span></span><br><span class=\"line\">    expectedModCount = modCount;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在调用完<code>removeNode</code>方法（这个过程会修改<code>modCount</code>）后，再把<code>expectedModCount</code>修改为<code>removeNode</code>，保证后续检查<code>expectedModCount</code>不会出问题</p>\n<blockquote>\n<p>为什么如此设计？</p>\n<p>迭代过程中调用<code>HashIterator</code>自己的remove，删除当前迭代指针指向的元素，可以保证这个操作是不受外界影响的（比如其他线程的并发操作），hashMap.remove()就保证不了这个前提</p>\n</blockquote>\n<p>同理，<code>ListIterator</code>的设计思路也是基于相同的考虑</p>\n<h3 id=\"removeIf\"><a href=\"#removeIf\" class=\"headerlink\" title=\"removeIf()\"></a>removeIf()</h3><p>jdk8 引入了一个更为简练的方式：<code>removeIf</code>，是<code>Collection</code>的default方法，对元素进行遍历，满足条件就进行删除：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hashMap.entrySet().removeIf(next -&gt; <span class=\"string\">\"111\"</span>.equals(next.getValue()));</span><br></pre></td></tr></table></figure>\n<p>进行该操作完全不需要考虑<code>ConcurrentModificationException</code>了，其内部也是调用的对应<code>Iterator</code>实现的<code>remove()</code>方法，原理跟上一种一致</p>"},{"title":"RestTemplate学习","author":"天渊","date":"2019-01-21T03:18:00.000Z","_content":"`RestTemplate`是spring framework中对Http请求封装的一套方法，广泛运用于springboot和springcloud中的Http数据传输，官方文档描述如下：\n\n> `RestTemplate` is a synchronous client to perform HTTP requests. It is the original Spring REST client and exposes a simple, template-method API over underlying HTTP client libraries.\n\n<!-- more -->\n\n不过在spring 5.0发行版中推出了基于NIO的响应式Http客户端：`WebClient`，将在未来代替RestTemplate:\n\n> As of 5.0, the non-blocking, reactive `WebClient` offers a modern alternative to the `RestTemplate`, with efficient support for both synchronous and asynchronous, as well as streaming scenarios. The `RestTemplate` will be deprecated in a future version and will not have major new features added going forward.\n\n### RestTemplate的特点\n- 使用方便：可直接传递Java实体对象，无需人工配置ContentType和Charset，自动识别序列化方式和消息格式，无需用户自己encode url，并对多种http传输方法进行了相应封装\n\n- 可扩展性强：RestTemplate可配置多种底层通信框架如JDK HttpURLConnection、Apache HttpClient、OkHttp以及netty等\n\n- 可配置性强：用户可灵活多种第三方组件，除了底层通信框架，还可扩展配置消息转换器，异常处理器，uri模板解析器和请求拦截器等组件\n\n- RestTemplate的多种组件过于依赖spring-framework，如果脱离了spring环境，用起来就很不方便了\n\n\n\n# RestTemplate使用方法\n\n## 1. 初始化\nRestTemplate有两种初始化方式：`构造方法形式`，`RestTemplateBuilder形式`\n\n### 构造方法形式\n\n```java\n@Bean\npublic RestTemplate restTemplate(){\n\treturn new RestTemplate();\n}\n```\n\n\n\nRestTemplate有三种构造方法：\n\n- 无参数构造方法，也是使用得最普遍的一个：\n\n  ```java\n  public RestTemplate() {\n  \tthis.messageConverters.add(new ByteArrayHttpMessageConverter());\n  \tthis.messageConverters.add(new StringHttpMessageConverter());\n  \tthis.messageConverters.add(new ResourceHttpMessageConverter());\n  \tthis.messageConverters.add(new SourceHttpMessageConverter<Source>());\n  \tthis.messageConverters.add(new AllEncompassingFormHttpMessageConverter());\n  \tif (romePresent) {\n  \t\tthis.messageConverters.add(new AtomFeedHttpMessageConverter());\n  \t\tthis.messageConverters.add(new RssChannelHttpMessageConverter());\n  \t}\n  \tif (jaxb2Present) {\n  \t\tthis.messageConverters.add(new Jaxb2RootElementHttpMessageConverter());\n  \t}\n  \tif (jackson2Present) {\n  \t\tthis.messageConverters.add(new MappingJackson2HttpMessageConverter());\n  \t}\n  \telse if (jacksonPresent) {\n  \t\tthis.messageConverters.add(new MappingJacksonHttpMessageConverter());\n  \t}\n  }\n  ```\n\n  RestTemplate中内置了多种HttpMessageConverter，用于对不同场景下的输入输出流进行序列化和反序列化，无参数构造方法主要对HttpMessageConverter列表进行初始化\n\n- 有参数构造方法 — 初始化ClientHttpRequestFactory\n\n  ```java\n  public RestTemplate(ClientHttpRequestFactory requestFactory) {\n  \tthis();\n  \tsetRequestFactory(requestFactory);\n  }\n  ```\n\n  用户可以自己指定需要的ClientHttpRequestFactory，用于进行http连接和请求，默认是采用`SimpleClientHttpRequestFactory`，底层封装的是JDK的`HttpURLConnection`，用户可以指定其他种类的factory，比如以下几种：\n\n  - `BufferingClientHttpRequestFactory`：可在内存中建立输入数据的缓存\n\n  - `HttpComponentsClientHttpRequestFactory`：采用Apache的HttpClient进行远程调用，可以配置连接池和证书信息，不过需要在pom.xml中加入以下依赖：\n\n    ```xml\n    <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpclient</artifactId>\n        <version>4.5.2</version>\n    </dependency>\n    ```\n\n  - `InterceptingClientHttpRequestFactory`：可以配置ClientHttpRequestInterceptor拦截器对http请求进行拦截处理，springcloud中的Ribbon就用到了这个factory用于将server name url转换为实际调用的url\n  - 基于Netty4的`Netty4ClientHttpRequestFactory`\n  - 基于OkHttp2的`OkHttpClientHttpRequestFactory`\n\n- 有参数构造方法 — 添加多个HttpMessageConverter\n\n  ```java\n  public RestTemplate(List<HttpMessageConverter<?>> messageConverters) {\n  \tAssert.notEmpty(messageConverters, \"'messageConverters' must not be empty\");\n  \tthis.messageConverters.addAll(messageConverters);\n  }\n  ```\n\n  如果用户觉得RestTemplate默认的几个序列化API无法满足要求，可以自己指定MessageConverter\n\n### RestTemplateBuilder形式\n如果用户要对RestTemplate进行多种初始化配置的话，推荐使用RestTemplateBuilder建造器，属于高级用法：\n  ```java\n    @Bean\n\tpublic RestTemplate myRestTemplate() {\n\t\tRestTemplateBuilder builder = new RestTemplateBuilder();\n\t\tRestTemplate restTemplate = builder\n            \t\t\t//配置ClientHttpRequestFactory\n\t\t\t\t\t\t.requestFactory(HttpComponentsClientHttpRequestFactory.class)\n            \t\t\t//配置MessageConverter\n\t\t\t\t\t\t.messageConverters(new MappingJackson2HttpMessageConverter())\n            \t\t\t//配置ResponseErrorHandler\n\t\t\t\t\t\t.errorHandler(new DefaultResponseErrorHandler())\n            \t\t\t//配置UriTemplateHandler\n\t\t\t\t\t\t.uriTemplateHandler(new DefaultUriTemplateHandler())\n            \t\t\t//配置连接超时时间和连接过期时间\n\t\t\t\t\t\t.setConnectTimeout(10000)\n\t\t\t\t\t\t.setReadTimeout(5000)\n\t\t\t\t\t\t.build();\n        return restTemplate;\n\t}\n  ```\n## 2. 执行Http请求\n\nRestTemplate对多种http method的请求进行了封装，用户可以直接进行使用\n\n- RestTemplate中几种常用的方法：\n\n| 方法名          | 描述                                                         |\n| --------------- | ------------------------------------------------------------ |\n| getForObject    | 通过get方法获取资源，返回用户指定的Object对象类型            |\n| getForEntity    | 通过get方法获取资源，返回一个封装好的HttpEntiry对象          |\n| postForObject   | 通过post方法发送Object对象，返回用户指定的Object对象类型     |\n| postForEntity   | 通过post方法发送Object对象，返回封装好的HttpEntiry对象       |\n| put             | 通过put方法上载资源                                          |\n| delete          | 通过delete方法删除服务器数据                                 |\n| optionsForAllow | 获取目的资源支持的method                                     |\n| exchange        | 一种比较通用的方法，接受的参数分别为url，method，response数据类型，url参数，以及一个封装了http header数据和body数据的HttpEntity对象，统一返回一个封装了所有response数据的ResponseEntity对象 |\n| execute         | 该方法是RestTemplate中通用性最强的方法，以上所有方法最终都调用的是execute方法，接受的参数包括RequestCallback对象（用于选择合适的MessageConverter对requestBody数据进行解析并将结果封装到ClientHttpRequest中，进行最终的http请求），以及ResponseExtractor对象（用于将response数据解析为用户需要的数据类型） |\n\n- get方法\n\n  发起get请求，如果只是想获取ResponseBody数据的话直接采用getForObject()的一系列重载方法：\n\n  - `Object`方式\n\n    将返回数据封装到指定的实体类CommonResponse中，RestTemplate会自动进行序列化和反序列化\n\n  ```java\n  CommonResponse response = restTemplate.getForObject(url, CommonResponse.class);\n  CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);\n  ```\n\n\n  - `HttpEntity`方式\n\n    如果想获取更详细的数据（比如响应头和http状态码等信息）就使用getForEntity()\n\n  ```java\n  ResponseEntity<CommonResponse> responseEntity = restTemplate.getForEntity(url, CommonResponse.class, pathVariables);\n  CommonResponse responseBody = responseEntity.getBody();\n  HttpStatus status = responseEntity.getStatusCode();\n  HttpHeaders headers = responseEntity.getHeaders();\n  ```\n\n  ​\t跟Object方式类似，不过返回的结果数据封装到了一个ResponseEntity对象中\n\n  - 配置`queryParameters`和`pathVariables`\n\n    RestTemplate中没有为queryParameters设置对应的传参，需要用户自己将queryParameters写到url里面，不过RestTemplate为restful风格的pathVariables配置了专用的传参：\n\n    （看得出来RestTemplate专业服务于restful API调用）\n\n    ```java\n    url = \"http://127.0.0.1:8081/persons/{id}\"\n    Map<String, Object> urlVariables = new HashMap<>();\n    pathVariables.put(\"id\", \"0\");\n    CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);\n    ```\n\n  - `UriTemplateHandler`（高级用法）\n\n    虽然RestTemplate中没有为queryParameters设置对应的传参，但是用户可以自己实现一个UriTemplateHandler：\n\n    ```java\n    public class QueryParamsUrlTemplateHandler extends DefaultUriTemplateHandler {\n    \n    @Override\n    public URI expand(String uriTemplate, Map<String, ?> params) {\n    \t\tUriComponentsBuilder componentsBuilder = UriComponentsBuilder.fromHttpUrl(uriTemplate);\n    \t\tfor(Map.Entry<String, ?> varEntry : params.entrySet()){\n    \t\t\tcomponentsBuilder.queryParam(varEntry.getKey(), varEntry.getValue());\n    \t\t}\n    \t\turiTemplate = componentsBuilder.build().toUriString();\n    \t\treturn super.expand(uriTemplate, params);\n    \t}\n    }\n    ```\n\n    ```java\n    restTemplate.setUriTemplateHandler(urlTemplateHandler);\n    ```\n\n    通过配置该UriTemplateHandler，就可以以Map的形式配置queryParameters了：\n\n    ```java\n    Map<String, Object> params = new HashMap<>();\n    params.put(\"name\", \"张三\");\n    ResponseEntity<CommonResponse> responseEntity = restTemplate.getForEntity(url, CommonResponse.class, params);\n    ```\n\n- post方法\n\n  发起post请求跟get很类似，唯一不同的地方在于需要设置`RequestBody`参数：\n\n  - `Object`方式：\n\n    无需自己设置contentType和charset，只需要直接传递实体对象作为RequestBody，RestTemplate会进行自动判断并选择合适的MeesageConverter\n\n  ```java\n  CommonResponse response = restTemplate.postForObject(url, person, CommonResponse.class);\n  ```\n\n  使用实体类Person封装上传数据，也可以直接使用Map封装数据\n\n  - `HttpEntity`方式\n\n    跟Object方式类似，request数据可以直接传递实体，也可以将请求头和请求体封装到一个HttpEntity对象中进行发送，返回数据都是ResponseEntity对象，可以取HttpStatus和HttpHeaders\n\n    ```java\n    //设置请求头\n    HttpHeaders headers = new HttpHeaders();\n    MediaType type = MediaType.parseMediaType(\"application/json; charset=UTF-8\");\n    headers.setContentType(type);\n    headers.set(\"headerName\", \"headerValue\");\n    //设置HttpEntity\n    HttpEntity<List<Person>> request = new HttpEntity<>(personList, headers);\n    //返回ResponseEntity对象，对Object实体数据进行封装\n    ResponseEntity<CommonResponse> response = restTemplate.postForEntity(url, request, CommonResponse.class);\n    ```\n\n  - post方法上传文件\n\n    上传文件需要用`MultiValueMap`进行文件数据的封装：\n\n  ```java\n  MultipartBodyBuilder builder = new MultipartBodyBuilder();\n  File file = new File(\"D:\\\\xxx\\\\xxx.png\");\n  builder.part(\"file\", new FileSystemResource(file));\n  MultiValueMap<String, Object> request = builder.build();\n  //上传文件\n  CommonResponse response = restTemplate.postForObject(url, request, CommonResponse.class)\n  ```\n\n- put和delete实现方式和上述方法都类似，不过这两种请求没有返回数据，不太实用\n\n  - 在RestTemplate中，对GET, POST, PUT, DELETE, OPTIONS, HEAD 这几种http方法都有相应的封装，如果不想用它封装好的方法，可以选择exchange()方法自定义http请求\n\n- exchange方法：\n\n  可以自己指定请求头和http method，其他的细节跟前面几种方法差不多\n\n  对于某些不太常见的方法（比如HEAD或者TRACE），就需要使用exchange()方法了， exchange()方法也有多种重载\n\n  以发起POST请求为例：\n\n  ```java\n  //设置requestBody数据\n  Person person = new Person();\n  //设置queryParameters和pathVariables\n  Map<String, Object> params = new HashMap<>();\n  params.put(\"name\", \"value\");\n  //设置请求头\n  HttpHeaders headers = new HttpHeaders();\n  MediaType type = MediaType.parseMediaType(\"application/json; charset=UTF-8\");\n  headers.setContentType(type);\n  headers.set(\"headerName\", \"headerValue\");\n  //配置requestEntity\n  HttpEntity<MyData> requestEntity = new HttpEntity(person, headers);\n  //发起请求\n  ResponseEntity<Map> responseEntity = restTemplate.exchange(url, HttpMethod.POST, requestEntity, Map.class, params);\n  ```\n\n- execute方法（一般不用）：\n\n  可以定制request和response的序列化和反序列化方式：\n\n  ```java\n  public <T> T execute(URI url, HttpMethod method, RequestCallback requestCallback,\n  \tResponseExtractor<T> responseExtractor) throws RestClientException {\n  \treturn doExecute(url, method, requestCallback, responseExtractor);\n  }\n  ```\n\n  - RequestCallback用于封装request信息，并对这部分信息进行解析\n  - ResponseExtractor用于对response返回数据进行解析\n\n  用户可以自由定制序列化方式，并以回调函数的形式传入execute()中；RestTemplate默认实现是两个静态内部类，默认选取RestTemplate中已经初始化完成的那部分HttpMessageConverter实现，进行序列化和反序列化操作\n\n\n\n## 3. 异常捕捉\n\nRestTemplate内部已经把http请求过程中会出现的各种异常，例如404或者500等异常，都包装为了RestClientException抛出\n\n在RestTemplate中进行异常处理的组件是`ResponseErrorHandler`，默认是`DefaultResponseErrorHandler`\n\n用户可以自己实现ResponseErrorHandler来处理http异常：\n\n```java\npublic class MyselfResponseErrorHandler extends DefaultResponseErrorHandler {\n\n\tprivate final Logger logger = LoggerFactory.getLogger(MyselfResponseErrorHandler.class);\n\n\t@Override\n\tpublic void handleError(ClientHttpResponse response) throws IOException {\n\t\tHttpStatus statusCode = getHttpStatusCode(response);\n\t\tString code = statusCode.toString();\n\t\tString msg = statusCode.getReasonPhrase();\n\t\tswitch (statusCode.series()) {\n\t\t\tcase CLIENT_ERROR:\n\t\t\t\tlogger.error(\"客户端请求错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t\t\tbreak;\n\t\t\tcase SERVER_ERROR:\n\t\t\t\tlogger.error(\"服务器端错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tlogger.error(\"不知道什么错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t}\n\t}\n}\n```\n\n```java\nrestTemplate.setErrorHandler(responseErrorHandler);\n```\n\n\n\n# RestTemplate内部源码分析\n\n## doExecute方法\n\n\n以上所有方法都有一个最终方法，也是RestTemplate的核心方法：doExecute()\n\n```java\n\tprotected <T> T doExecute(URI url, HttpMethod method, RequestCallback requestCallback,\n\t\t\tResponseExtractor<T> responseExtractor) throws RestClientException {\n\t\t//url和method都不能为空\n\t\tAssert.notNull(url, \"'url' must not be null\");\n\t\tAssert.notNull(method, \"'method' must not be null\");\n\t\tClientHttpResponse response = null;\n\t\ttry {\n            //使用ClientHttpRequestFactory创建一个ClientHttpRequest对象\n\t\t\tClientHttpRequest request = createRequest(url, method);\n\t\t\tif (requestCallback != null) {\n                //调用RequestCallback对象对requestBody数据进行解析\n                //序列化后的数据封装到ClientHttpRequest对象中\n\t\t\t\trequestCallback.doWithRequest(request);\n\t\t\t}\n            //ClientHttpRequest对象执行http请求得到ClientHttpResponse对象\n\t\t\tresponse = request.execute();\n\t\t\tif (!getErrorHandler().hasError(response)) {\n\t\t\t\tlogResponseStatus(method, url, response);\n\t\t\t}\n\t\t\telse {\n\t\t\t\thandleResponseError(method, url, response);\n\t\t\t}\n\t\t\tif (responseExtractor != null) {\n                //将ClientHttpResponse对象反序列化为用户指定的数据类型\n\t\t\t\treturn responseExtractor.extractData(response);\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\tcatch (IOException ex) {\n\t\t\tthrow new ResourceAccessException(\"I/O error on \" + method.name() +\n\t\t\t\t\t\" request for \\\"\" + url + \"\\\": \" + ex.getMessage(), ex);\n\t\t}\n\t\tfinally {\n\t\t\tif (response != null) {\n\t\t\t\tresponse.close();\n\t\t\t}\n\t\t}\n\t}\n```\n\n可以看出，实际执行http请求的是ClientHttpRequest，用户可以通过设置不同的ClientHttpRequestFactory自己定制http连接方式，例如HttpComponentsClientHttpRequestFactory就是基于Apache HttpClient实现：\n\n```java\n//需要将HttpComponentsClientHttpRequestFactory暴露为spring bean，因为其实现了DisposableBean，可以在bean销毁后自动关闭连接池\n@Bean\npublic HttpComponentsClientHttpRequestFactory getFactory(){\n\tHttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory();\n    //设置socket请求连接超时时间\n\tfactory.setConnectTimeout(5000);\n    //设置socket读取数据阻塞超时时间\n\tfactory.setReadTimeout(5000);\n\treturn factory;\n}\n```\n\n\n\n## MessageConverter\n\n- 当调用不同的RestTemplate方法传输数据时，RestTemplate会自动检查ContentType并采用合适的MessageConverter进行序列化和反序列化，如果找不到合适的MessageConverter，将会报错\n\n- RestTemplate里面默认的几种MessageConverter已经能够满足大多数应用场景了\n\n| MessageConverter                    | 描述                                                         |\n| ----------------------------------- | ------------------------------------------------------------ |\n| StringHttpMessageConverter          | 支持文本类型的数据格式：text/plain，text/*                    |\n| FormHttpMessageConverter            | 支持表单类型的数据格式：application/x-www-form-urlencoded    |\n| MappingJackson2HttpMessageConverter | 支持json类型的数据格式：application/json                     |\n| ResourceHttpMessageConverter        | 可用于对Resource类型的文件io流进行序列化，并支持任意的MediaType |\n| BufferedImageHttpMessageConverter | 用于读取java.awt.image.BufferedImage格式的图片文件 |\n\n- 用户可以自己定义满足自己业务需求的MessageConverter\n\n\n\n# WebClient\n\nspring 5.0全面引入了reactive响应式编程模式，同时也就有了RestTemplate的reactive版：WebClient","source":"_posts/何为RestTemplate.md","raw":"title: RestTemplate学习\ntags:\n  - spring\n  - RestTemplate\n  - Java\ncategories:\n  - 基础知识\nauthor: 天渊\ndate: 2019-01-21 11:18:00\n---\n`RestTemplate`是spring framework中对Http请求封装的一套方法，广泛运用于springboot和springcloud中的Http数据传输，官方文档描述如下：\n\n> `RestTemplate` is a synchronous client to perform HTTP requests. It is the original Spring REST client and exposes a simple, template-method API over underlying HTTP client libraries.\n\n<!-- more -->\n\n不过在spring 5.0发行版中推出了基于NIO的响应式Http客户端：`WebClient`，将在未来代替RestTemplate:\n\n> As of 5.0, the non-blocking, reactive `WebClient` offers a modern alternative to the `RestTemplate`, with efficient support for both synchronous and asynchronous, as well as streaming scenarios. The `RestTemplate` will be deprecated in a future version and will not have major new features added going forward.\n\n### RestTemplate的特点\n- 使用方便：可直接传递Java实体对象，无需人工配置ContentType和Charset，自动识别序列化方式和消息格式，无需用户自己encode url，并对多种http传输方法进行了相应封装\n\n- 可扩展性强：RestTemplate可配置多种底层通信框架如JDK HttpURLConnection、Apache HttpClient、OkHttp以及netty等\n\n- 可配置性强：用户可灵活多种第三方组件，除了底层通信框架，还可扩展配置消息转换器，异常处理器，uri模板解析器和请求拦截器等组件\n\n- RestTemplate的多种组件过于依赖spring-framework，如果脱离了spring环境，用起来就很不方便了\n\n\n\n# RestTemplate使用方法\n\n## 1. 初始化\nRestTemplate有两种初始化方式：`构造方法形式`，`RestTemplateBuilder形式`\n\n### 构造方法形式\n\n```java\n@Bean\npublic RestTemplate restTemplate(){\n\treturn new RestTemplate();\n}\n```\n\n\n\nRestTemplate有三种构造方法：\n\n- 无参数构造方法，也是使用得最普遍的一个：\n\n  ```java\n  public RestTemplate() {\n  \tthis.messageConverters.add(new ByteArrayHttpMessageConverter());\n  \tthis.messageConverters.add(new StringHttpMessageConverter());\n  \tthis.messageConverters.add(new ResourceHttpMessageConverter());\n  \tthis.messageConverters.add(new SourceHttpMessageConverter<Source>());\n  \tthis.messageConverters.add(new AllEncompassingFormHttpMessageConverter());\n  \tif (romePresent) {\n  \t\tthis.messageConverters.add(new AtomFeedHttpMessageConverter());\n  \t\tthis.messageConverters.add(new RssChannelHttpMessageConverter());\n  \t}\n  \tif (jaxb2Present) {\n  \t\tthis.messageConverters.add(new Jaxb2RootElementHttpMessageConverter());\n  \t}\n  \tif (jackson2Present) {\n  \t\tthis.messageConverters.add(new MappingJackson2HttpMessageConverter());\n  \t}\n  \telse if (jacksonPresent) {\n  \t\tthis.messageConverters.add(new MappingJacksonHttpMessageConverter());\n  \t}\n  }\n  ```\n\n  RestTemplate中内置了多种HttpMessageConverter，用于对不同场景下的输入输出流进行序列化和反序列化，无参数构造方法主要对HttpMessageConverter列表进行初始化\n\n- 有参数构造方法 — 初始化ClientHttpRequestFactory\n\n  ```java\n  public RestTemplate(ClientHttpRequestFactory requestFactory) {\n  \tthis();\n  \tsetRequestFactory(requestFactory);\n  }\n  ```\n\n  用户可以自己指定需要的ClientHttpRequestFactory，用于进行http连接和请求，默认是采用`SimpleClientHttpRequestFactory`，底层封装的是JDK的`HttpURLConnection`，用户可以指定其他种类的factory，比如以下几种：\n\n  - `BufferingClientHttpRequestFactory`：可在内存中建立输入数据的缓存\n\n  - `HttpComponentsClientHttpRequestFactory`：采用Apache的HttpClient进行远程调用，可以配置连接池和证书信息，不过需要在pom.xml中加入以下依赖：\n\n    ```xml\n    <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpclient</artifactId>\n        <version>4.5.2</version>\n    </dependency>\n    ```\n\n  - `InterceptingClientHttpRequestFactory`：可以配置ClientHttpRequestInterceptor拦截器对http请求进行拦截处理，springcloud中的Ribbon就用到了这个factory用于将server name url转换为实际调用的url\n  - 基于Netty4的`Netty4ClientHttpRequestFactory`\n  - 基于OkHttp2的`OkHttpClientHttpRequestFactory`\n\n- 有参数构造方法 — 添加多个HttpMessageConverter\n\n  ```java\n  public RestTemplate(List<HttpMessageConverter<?>> messageConverters) {\n  \tAssert.notEmpty(messageConverters, \"'messageConverters' must not be empty\");\n  \tthis.messageConverters.addAll(messageConverters);\n  }\n  ```\n\n  如果用户觉得RestTemplate默认的几个序列化API无法满足要求，可以自己指定MessageConverter\n\n### RestTemplateBuilder形式\n如果用户要对RestTemplate进行多种初始化配置的话，推荐使用RestTemplateBuilder建造器，属于高级用法：\n  ```java\n    @Bean\n\tpublic RestTemplate myRestTemplate() {\n\t\tRestTemplateBuilder builder = new RestTemplateBuilder();\n\t\tRestTemplate restTemplate = builder\n            \t\t\t//配置ClientHttpRequestFactory\n\t\t\t\t\t\t.requestFactory(HttpComponentsClientHttpRequestFactory.class)\n            \t\t\t//配置MessageConverter\n\t\t\t\t\t\t.messageConverters(new MappingJackson2HttpMessageConverter())\n            \t\t\t//配置ResponseErrorHandler\n\t\t\t\t\t\t.errorHandler(new DefaultResponseErrorHandler())\n            \t\t\t//配置UriTemplateHandler\n\t\t\t\t\t\t.uriTemplateHandler(new DefaultUriTemplateHandler())\n            \t\t\t//配置连接超时时间和连接过期时间\n\t\t\t\t\t\t.setConnectTimeout(10000)\n\t\t\t\t\t\t.setReadTimeout(5000)\n\t\t\t\t\t\t.build();\n        return restTemplate;\n\t}\n  ```\n## 2. 执行Http请求\n\nRestTemplate对多种http method的请求进行了封装，用户可以直接进行使用\n\n- RestTemplate中几种常用的方法：\n\n| 方法名          | 描述                                                         |\n| --------------- | ------------------------------------------------------------ |\n| getForObject    | 通过get方法获取资源，返回用户指定的Object对象类型            |\n| getForEntity    | 通过get方法获取资源，返回一个封装好的HttpEntiry对象          |\n| postForObject   | 通过post方法发送Object对象，返回用户指定的Object对象类型     |\n| postForEntity   | 通过post方法发送Object对象，返回封装好的HttpEntiry对象       |\n| put             | 通过put方法上载资源                                          |\n| delete          | 通过delete方法删除服务器数据                                 |\n| optionsForAllow | 获取目的资源支持的method                                     |\n| exchange        | 一种比较通用的方法，接受的参数分别为url，method，response数据类型，url参数，以及一个封装了http header数据和body数据的HttpEntity对象，统一返回一个封装了所有response数据的ResponseEntity对象 |\n| execute         | 该方法是RestTemplate中通用性最强的方法，以上所有方法最终都调用的是execute方法，接受的参数包括RequestCallback对象（用于选择合适的MessageConverter对requestBody数据进行解析并将结果封装到ClientHttpRequest中，进行最终的http请求），以及ResponseExtractor对象（用于将response数据解析为用户需要的数据类型） |\n\n- get方法\n\n  发起get请求，如果只是想获取ResponseBody数据的话直接采用getForObject()的一系列重载方法：\n\n  - `Object`方式\n\n    将返回数据封装到指定的实体类CommonResponse中，RestTemplate会自动进行序列化和反序列化\n\n  ```java\n  CommonResponse response = restTemplate.getForObject(url, CommonResponse.class);\n  CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);\n  ```\n\n\n  - `HttpEntity`方式\n\n    如果想获取更详细的数据（比如响应头和http状态码等信息）就使用getForEntity()\n\n  ```java\n  ResponseEntity<CommonResponse> responseEntity = restTemplate.getForEntity(url, CommonResponse.class, pathVariables);\n  CommonResponse responseBody = responseEntity.getBody();\n  HttpStatus status = responseEntity.getStatusCode();\n  HttpHeaders headers = responseEntity.getHeaders();\n  ```\n\n  ​\t跟Object方式类似，不过返回的结果数据封装到了一个ResponseEntity对象中\n\n  - 配置`queryParameters`和`pathVariables`\n\n    RestTemplate中没有为queryParameters设置对应的传参，需要用户自己将queryParameters写到url里面，不过RestTemplate为restful风格的pathVariables配置了专用的传参：\n\n    （看得出来RestTemplate专业服务于restful API调用）\n\n    ```java\n    url = \"http://127.0.0.1:8081/persons/{id}\"\n    Map<String, Object> urlVariables = new HashMap<>();\n    pathVariables.put(\"id\", \"0\");\n    CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);\n    ```\n\n  - `UriTemplateHandler`（高级用法）\n\n    虽然RestTemplate中没有为queryParameters设置对应的传参，但是用户可以自己实现一个UriTemplateHandler：\n\n    ```java\n    public class QueryParamsUrlTemplateHandler extends DefaultUriTemplateHandler {\n    \n    @Override\n    public URI expand(String uriTemplate, Map<String, ?> params) {\n    \t\tUriComponentsBuilder componentsBuilder = UriComponentsBuilder.fromHttpUrl(uriTemplate);\n    \t\tfor(Map.Entry<String, ?> varEntry : params.entrySet()){\n    \t\t\tcomponentsBuilder.queryParam(varEntry.getKey(), varEntry.getValue());\n    \t\t}\n    \t\turiTemplate = componentsBuilder.build().toUriString();\n    \t\treturn super.expand(uriTemplate, params);\n    \t}\n    }\n    ```\n\n    ```java\n    restTemplate.setUriTemplateHandler(urlTemplateHandler);\n    ```\n\n    通过配置该UriTemplateHandler，就可以以Map的形式配置queryParameters了：\n\n    ```java\n    Map<String, Object> params = new HashMap<>();\n    params.put(\"name\", \"张三\");\n    ResponseEntity<CommonResponse> responseEntity = restTemplate.getForEntity(url, CommonResponse.class, params);\n    ```\n\n- post方法\n\n  发起post请求跟get很类似，唯一不同的地方在于需要设置`RequestBody`参数：\n\n  - `Object`方式：\n\n    无需自己设置contentType和charset，只需要直接传递实体对象作为RequestBody，RestTemplate会进行自动判断并选择合适的MeesageConverter\n\n  ```java\n  CommonResponse response = restTemplate.postForObject(url, person, CommonResponse.class);\n  ```\n\n  使用实体类Person封装上传数据，也可以直接使用Map封装数据\n\n  - `HttpEntity`方式\n\n    跟Object方式类似，request数据可以直接传递实体，也可以将请求头和请求体封装到一个HttpEntity对象中进行发送，返回数据都是ResponseEntity对象，可以取HttpStatus和HttpHeaders\n\n    ```java\n    //设置请求头\n    HttpHeaders headers = new HttpHeaders();\n    MediaType type = MediaType.parseMediaType(\"application/json; charset=UTF-8\");\n    headers.setContentType(type);\n    headers.set(\"headerName\", \"headerValue\");\n    //设置HttpEntity\n    HttpEntity<List<Person>> request = new HttpEntity<>(personList, headers);\n    //返回ResponseEntity对象，对Object实体数据进行封装\n    ResponseEntity<CommonResponse> response = restTemplate.postForEntity(url, request, CommonResponse.class);\n    ```\n\n  - post方法上传文件\n\n    上传文件需要用`MultiValueMap`进行文件数据的封装：\n\n  ```java\n  MultipartBodyBuilder builder = new MultipartBodyBuilder();\n  File file = new File(\"D:\\\\xxx\\\\xxx.png\");\n  builder.part(\"file\", new FileSystemResource(file));\n  MultiValueMap<String, Object> request = builder.build();\n  //上传文件\n  CommonResponse response = restTemplate.postForObject(url, request, CommonResponse.class)\n  ```\n\n- put和delete实现方式和上述方法都类似，不过这两种请求没有返回数据，不太实用\n\n  - 在RestTemplate中，对GET, POST, PUT, DELETE, OPTIONS, HEAD 这几种http方法都有相应的封装，如果不想用它封装好的方法，可以选择exchange()方法自定义http请求\n\n- exchange方法：\n\n  可以自己指定请求头和http method，其他的细节跟前面几种方法差不多\n\n  对于某些不太常见的方法（比如HEAD或者TRACE），就需要使用exchange()方法了， exchange()方法也有多种重载\n\n  以发起POST请求为例：\n\n  ```java\n  //设置requestBody数据\n  Person person = new Person();\n  //设置queryParameters和pathVariables\n  Map<String, Object> params = new HashMap<>();\n  params.put(\"name\", \"value\");\n  //设置请求头\n  HttpHeaders headers = new HttpHeaders();\n  MediaType type = MediaType.parseMediaType(\"application/json; charset=UTF-8\");\n  headers.setContentType(type);\n  headers.set(\"headerName\", \"headerValue\");\n  //配置requestEntity\n  HttpEntity<MyData> requestEntity = new HttpEntity(person, headers);\n  //发起请求\n  ResponseEntity<Map> responseEntity = restTemplate.exchange(url, HttpMethod.POST, requestEntity, Map.class, params);\n  ```\n\n- execute方法（一般不用）：\n\n  可以定制request和response的序列化和反序列化方式：\n\n  ```java\n  public <T> T execute(URI url, HttpMethod method, RequestCallback requestCallback,\n  \tResponseExtractor<T> responseExtractor) throws RestClientException {\n  \treturn doExecute(url, method, requestCallback, responseExtractor);\n  }\n  ```\n\n  - RequestCallback用于封装request信息，并对这部分信息进行解析\n  - ResponseExtractor用于对response返回数据进行解析\n\n  用户可以自由定制序列化方式，并以回调函数的形式传入execute()中；RestTemplate默认实现是两个静态内部类，默认选取RestTemplate中已经初始化完成的那部分HttpMessageConverter实现，进行序列化和反序列化操作\n\n\n\n## 3. 异常捕捉\n\nRestTemplate内部已经把http请求过程中会出现的各种异常，例如404或者500等异常，都包装为了RestClientException抛出\n\n在RestTemplate中进行异常处理的组件是`ResponseErrorHandler`，默认是`DefaultResponseErrorHandler`\n\n用户可以自己实现ResponseErrorHandler来处理http异常：\n\n```java\npublic class MyselfResponseErrorHandler extends DefaultResponseErrorHandler {\n\n\tprivate final Logger logger = LoggerFactory.getLogger(MyselfResponseErrorHandler.class);\n\n\t@Override\n\tpublic void handleError(ClientHttpResponse response) throws IOException {\n\t\tHttpStatus statusCode = getHttpStatusCode(response);\n\t\tString code = statusCode.toString();\n\t\tString msg = statusCode.getReasonPhrase();\n\t\tswitch (statusCode.series()) {\n\t\t\tcase CLIENT_ERROR:\n\t\t\t\tlogger.error(\"客户端请求错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t\t\tbreak;\n\t\t\tcase SERVER_ERROR:\n\t\t\t\tlogger.error(\"服务器端错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tlogger.error(\"不知道什么错误，错误码：\" + code + \", 错误信息：\" + msg);\n\t\t}\n\t}\n}\n```\n\n```java\nrestTemplate.setErrorHandler(responseErrorHandler);\n```\n\n\n\n# RestTemplate内部源码分析\n\n## doExecute方法\n\n\n以上所有方法都有一个最终方法，也是RestTemplate的核心方法：doExecute()\n\n```java\n\tprotected <T> T doExecute(URI url, HttpMethod method, RequestCallback requestCallback,\n\t\t\tResponseExtractor<T> responseExtractor) throws RestClientException {\n\t\t//url和method都不能为空\n\t\tAssert.notNull(url, \"'url' must not be null\");\n\t\tAssert.notNull(method, \"'method' must not be null\");\n\t\tClientHttpResponse response = null;\n\t\ttry {\n            //使用ClientHttpRequestFactory创建一个ClientHttpRequest对象\n\t\t\tClientHttpRequest request = createRequest(url, method);\n\t\t\tif (requestCallback != null) {\n                //调用RequestCallback对象对requestBody数据进行解析\n                //序列化后的数据封装到ClientHttpRequest对象中\n\t\t\t\trequestCallback.doWithRequest(request);\n\t\t\t}\n            //ClientHttpRequest对象执行http请求得到ClientHttpResponse对象\n\t\t\tresponse = request.execute();\n\t\t\tif (!getErrorHandler().hasError(response)) {\n\t\t\t\tlogResponseStatus(method, url, response);\n\t\t\t}\n\t\t\telse {\n\t\t\t\thandleResponseError(method, url, response);\n\t\t\t}\n\t\t\tif (responseExtractor != null) {\n                //将ClientHttpResponse对象反序列化为用户指定的数据类型\n\t\t\t\treturn responseExtractor.extractData(response);\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\tcatch (IOException ex) {\n\t\t\tthrow new ResourceAccessException(\"I/O error on \" + method.name() +\n\t\t\t\t\t\" request for \\\"\" + url + \"\\\": \" + ex.getMessage(), ex);\n\t\t}\n\t\tfinally {\n\t\t\tif (response != null) {\n\t\t\t\tresponse.close();\n\t\t\t}\n\t\t}\n\t}\n```\n\n可以看出，实际执行http请求的是ClientHttpRequest，用户可以通过设置不同的ClientHttpRequestFactory自己定制http连接方式，例如HttpComponentsClientHttpRequestFactory就是基于Apache HttpClient实现：\n\n```java\n//需要将HttpComponentsClientHttpRequestFactory暴露为spring bean，因为其实现了DisposableBean，可以在bean销毁后自动关闭连接池\n@Bean\npublic HttpComponentsClientHttpRequestFactory getFactory(){\n\tHttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory();\n    //设置socket请求连接超时时间\n\tfactory.setConnectTimeout(5000);\n    //设置socket读取数据阻塞超时时间\n\tfactory.setReadTimeout(5000);\n\treturn factory;\n}\n```\n\n\n\n## MessageConverter\n\n- 当调用不同的RestTemplate方法传输数据时，RestTemplate会自动检查ContentType并采用合适的MessageConverter进行序列化和反序列化，如果找不到合适的MessageConverter，将会报错\n\n- RestTemplate里面默认的几种MessageConverter已经能够满足大多数应用场景了\n\n| MessageConverter                    | 描述                                                         |\n| ----------------------------------- | ------------------------------------------------------------ |\n| StringHttpMessageConverter          | 支持文本类型的数据格式：text/plain，text/*                    |\n| FormHttpMessageConverter            | 支持表单类型的数据格式：application/x-www-form-urlencoded    |\n| MappingJackson2HttpMessageConverter | 支持json类型的数据格式：application/json                     |\n| ResourceHttpMessageConverter        | 可用于对Resource类型的文件io流进行序列化，并支持任意的MediaType |\n| BufferedImageHttpMessageConverter | 用于读取java.awt.image.BufferedImage格式的图片文件 |\n\n- 用户可以自己定义满足自己业务需求的MessageConverter\n\n\n\n# WebClient\n\nspring 5.0全面引入了reactive响应式编程模式，同时也就有了RestTemplate的reactive版：WebClient","slug":"何为RestTemplate","published":1,"updated":"2019-03-19T13:30:58.493Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js43001dg0qrap94t63n","content":"<p><code>RestTemplate</code>是spring framework中对Http请求封装的一套方法，广泛运用于springboot和springcloud中的Http数据传输，官方文档描述如下：</p>\n<blockquote>\n<p><code>RestTemplate</code> is a synchronous client to perform HTTP requests. It is the original Spring REST client and exposes a simple, template-method API over underlying HTTP client libraries.</p>\n</blockquote>\n<a id=\"more\"></a>\n<p>不过在spring 5.0发行版中推出了基于NIO的响应式Http客户端：<code>WebClient</code>，将在未来代替RestTemplate:</p>\n<blockquote>\n<p>As of 5.0, the non-blocking, reactive <code>WebClient</code> offers a modern alternative to the <code>RestTemplate</code>, with efficient support for both synchronous and asynchronous, as well as streaming scenarios. The <code>RestTemplate</code> will be deprecated in a future version and will not have major new features added going forward.</p>\n</blockquote>\n<h3 id=\"RestTemplate的特点\"><a href=\"#RestTemplate的特点\" class=\"headerlink\" title=\"RestTemplate的特点\"></a>RestTemplate的特点</h3><ul>\n<li><p>使用方便：可直接传递Java实体对象，无需人工配置ContentType和Charset，自动识别序列化方式和消息格式，无需用户自己encode url，并对多种http传输方法进行了相应封装</p>\n</li>\n<li><p>可扩展性强：RestTemplate可配置多种底层通信框架如JDK HttpURLConnection、Apache HttpClient、OkHttp以及netty等</p>\n</li>\n<li><p>可配置性强：用户可灵活多种第三方组件，除了底层通信框架，还可扩展配置消息转换器，异常处理器，uri模板解析器和请求拦截器等组件</p>\n</li>\n<li><p>RestTemplate的多种组件过于依赖spring-framework，如果脱离了spring环境，用起来就很不方便了</p>\n</li>\n</ul>\n<h1 id=\"RestTemplate使用方法\"><a href=\"#RestTemplate使用方法\" class=\"headerlink\" title=\"RestTemplate使用方法\"></a>RestTemplate使用方法</h1><h2 id=\"1-初始化\"><a href=\"#1-初始化\" class=\"headerlink\" title=\"1. 初始化\"></a>1. 初始化</h2><p>RestTemplate有两种初始化方式：<code>构造方法形式</code>，<code>RestTemplateBuilder形式</code></p>\n<h3 id=\"构造方法形式\"><a href=\"#构造方法形式\" class=\"headerlink\" title=\"构造方法形式\"></a>构造方法形式</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> RestTemplate <span class=\"title\">restTemplate</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> RestTemplate();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RestTemplate有三种构造方法：</p>\n<ul>\n<li><p>无参数构造方法，也是使用得最普遍的一个：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> ByteArrayHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> StringHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> ResourceHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> SourceHttpMessageConverter&lt;Source&gt;());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> AllEncompassingFormHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (romePresent) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> AtomFeedHttpMessageConverter());</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> RssChannelHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (jaxb2Present) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> Jaxb2RootElementHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (jackson2Present) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> MappingJackson2HttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (jacksonPresent) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> MappingJacksonHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RestTemplate中内置了多种HttpMessageConverter，用于对不同场景下的输入输出流进行序列化和反序列化，无参数构造方法主要对HttpMessageConverter列表进行初始化</p>\n</li>\n<li><p>有参数构造方法 — 初始化ClientHttpRequestFactory</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">(ClientHttpRequestFactory requestFactory)</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>();</span><br><span class=\"line\">\tsetRequestFactory(requestFactory);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>用户可以自己指定需要的ClientHttpRequestFactory，用于进行http连接和请求，默认是采用<code>SimpleClientHttpRequestFactory</code>，底层封装的是JDK的<code>HttpURLConnection</code>，用户可以指定其他种类的factory，比如以下几种：</p>\n<ul>\n<li><p><code>BufferingClientHttpRequestFactory</code>：可在内存中建立输入数据的缓存</p>\n</li>\n<li><p><code>HttpComponentsClientHttpRequestFactory</code>：采用Apache的HttpClient进行远程调用，可以配置连接池和证书信息，不过需要在pom.xml中加入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.httpcomponents<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>httpclient<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>4.5.2<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>InterceptingClientHttpRequestFactory</code>：可以配置ClientHttpRequestInterceptor拦截器对http请求进行拦截处理，springcloud中的Ribbon就用到了这个factory用于将server name url转换为实际调用的url</p>\n</li>\n<li>基于Netty4的<code>Netty4ClientHttpRequestFactory</code></li>\n<li>基于OkHttp2的<code>OkHttpClientHttpRequestFactory</code></li>\n</ul>\n</li>\n<li><p>有参数构造方法 — 添加多个HttpMessageConverter</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">(List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters)</span> </span>&#123;</span><br><span class=\"line\">\tAssert.notEmpty(messageConverters, <span class=\"string\">\"'messageConverters' must not be empty\"</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.addAll(messageConverters);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果用户觉得RestTemplate默认的几个序列化API无法满足要求，可以自己指定MessageConverter</p>\n</li>\n</ul>\n<h3 id=\"RestTemplateBuilder形式\"><a href=\"#RestTemplateBuilder形式\" class=\"headerlink\" title=\"RestTemplateBuilder形式\"></a>RestTemplateBuilder形式</h3><p>如果用户要对RestTemplate进行多种初始化配置的话，推荐使用RestTemplateBuilder建造器，属于高级用法：<br>  <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   <span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> RestTemplate <span class=\"title\">myRestTemplate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\tRestTemplateBuilder builder = <span class=\"keyword\">new</span> RestTemplateBuilder();</span><br><span class=\"line\">\tRestTemplate restTemplate = builder</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置ClientHttpRequestFactory</span></span><br><span class=\"line\">\t\t\t\t\t.requestFactory(HttpComponentsClientHttpRequestFactory.class)</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置MessageConverter</span></span><br><span class=\"line\">\t\t\t\t\t.messageConverters(<span class=\"keyword\">new</span> MappingJackson2HttpMessageConverter())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置ResponseErrorHandler</span></span><br><span class=\"line\">\t\t\t\t\t.errorHandler(<span class=\"keyword\">new</span> DefaultResponseErrorHandler())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置UriTemplateHandler</span></span><br><span class=\"line\">\t\t\t\t\t.uriTemplateHandler(<span class=\"keyword\">new</span> DefaultUriTemplateHandler())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置连接超时时间和连接过期时间</span></span><br><span class=\"line\">\t\t\t\t\t.setConnectTimeout(<span class=\"number\">10000</span>)</span><br><span class=\"line\">\t\t\t\t\t.setReadTimeout(<span class=\"number\">5000</span>)</span><br><span class=\"line\">\t\t\t\t\t.build();</span><br><span class=\"line\">       <span class=\"keyword\">return</span> restTemplate;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-执行Http请求\"><a href=\"#2-执行Http请求\" class=\"headerlink\" title=\"2. 执行Http请求\"></a>2. 执行Http请求</h2><p>RestTemplate对多种http method的请求进行了封装，用户可以直接进行使用</p>\n<ul>\n<li>RestTemplate中几种常用的方法：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法名</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>getForObject</td>\n<td>通过get方法获取资源，返回用户指定的Object对象类型</td>\n</tr>\n<tr>\n<td>getForEntity</td>\n<td>通过get方法获取资源，返回一个封装好的HttpEntiry对象</td>\n</tr>\n<tr>\n<td>postForObject</td>\n<td>通过post方法发送Object对象，返回用户指定的Object对象类型</td>\n</tr>\n<tr>\n<td>postForEntity</td>\n<td>通过post方法发送Object对象，返回封装好的HttpEntiry对象</td>\n</tr>\n<tr>\n<td>put</td>\n<td>通过put方法上载资源</td>\n</tr>\n<tr>\n<td>delete</td>\n<td>通过delete方法删除服务器数据</td>\n</tr>\n<tr>\n<td>optionsForAllow</td>\n<td>获取目的资源支持的method</td>\n</tr>\n<tr>\n<td>exchange</td>\n<td>一种比较通用的方法，接受的参数分别为url，method，response数据类型，url参数，以及一个封装了http header数据和body数据的HttpEntity对象，统一返回一个封装了所有response数据的ResponseEntity对象</td>\n</tr>\n<tr>\n<td>execute</td>\n<td>该方法是RestTemplate中通用性最强的方法，以上所有方法最终都调用的是execute方法，接受的参数包括RequestCallback对象（用于选择合适的MessageConverter对requestBody数据进行解析并将结果封装到ClientHttpRequest中，进行最终的http请求），以及ResponseExtractor对象（用于将response数据解析为用户需要的数据类型）</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><p>get方法</p>\n<p>发起get请求，如果只是想获取ResponseBody数据的话直接采用getForObject()的一系列重载方法：</p>\n<ul>\n<li><p><code>Object</code>方式</p>\n<p>将返回数据封装到指定的实体类CommonResponse中，RestTemplate会自动进行序列化和反序列化</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class);</span><br><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><code>HttpEntity</code>方式</p>\n<p>如果想获取更详细的数据（比如响应头和http状态码等信息）就使用getForEntity()</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; responseEntity = restTemplate.getForEntity(url, CommonResponse.class, pathVariables);</span><br><span class=\"line\">CommonResponse responseBody = responseEntity.getBody();</span><br><span class=\"line\">HttpStatus status = responseEntity.getStatusCode();</span><br><span class=\"line\">HttpHeaders headers = responseEntity.getHeaders();</span><br></pre></td></tr></table></figure>\n<p>​    跟Object方式类似，不过返回的结果数据封装到了一个ResponseEntity对象中</p>\n<ul>\n<li><p>配置<code>queryParameters</code>和<code>pathVariables</code></p>\n<p>RestTemplate中没有为queryParameters设置对应的传参，需要用户自己将queryParameters写到url里面，不过RestTemplate为restful风格的pathVariables配置了专用的传参：</p>\n<p>（看得出来RestTemplate专业服务于restful API调用）</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">url = <span class=\"string\">\"http://127.0.0.1:8081/persons/&#123;id&#125;\"</span></span><br><span class=\"line\">Map&lt;String, Object&gt; urlVariables = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">pathVariables.put(<span class=\"string\">\"id\"</span>, <span class=\"string\">\"0\"</span>);</span><br><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>UriTemplateHandler</code>（高级用法）</p>\n<p>虽然RestTemplate中没有为queryParameters设置对应的传参，但是用户可以自己实现一个UriTemplateHandler：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">QueryParamsUrlTemplateHandler</span> <span class=\"keyword\">extends</span> <span class=\"title\">DefaultUriTemplateHandler</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> URI <span class=\"title\">expand</span><span class=\"params\">(String uriTemplate, Map&lt;String, ?&gt; params)</span> </span>&#123;</span><br><span class=\"line\">\t\tUriComponentsBuilder componentsBuilder = UriComponentsBuilder.fromHttpUrl(uriTemplate);</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span>(Map.Entry&lt;String, ?&gt; varEntry : params.entrySet())&#123;</span><br><span class=\"line\">\t\t\tcomponentsBuilder.queryParam(varEntry.getKey(), varEntry.getValue());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\turiTemplate = componentsBuilder.build().toUriString();</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.expand(uriTemplate, params);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">restTemplate.setUriTemplateHandler(urlTemplateHandler);</span><br></pre></td></tr></table></figure>\n<p>通过配置该UriTemplateHandler，就可以以Map的形式配置queryParameters了：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; params = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">params.put(<span class=\"string\">\"name\"</span>, <span class=\"string\">\"张三\"</span>);</span><br><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; responseEntity = restTemplate.getForEntity(url, CommonResponse.class, params);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p>post方法</p>\n<p>发起post请求跟get很类似，唯一不同的地方在于需要设置<code>RequestBody</code>参数：</p>\n<ul>\n<li><p><code>Object</code>方式：</p>\n<p>无需自己设置contentType和charset，只需要直接传递实体对象作为RequestBody，RestTemplate会进行自动判断并选择合适的MeesageConverter</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CommonResponse response = restTemplate.postForObject(url, person, CommonResponse.class);</span><br></pre></td></tr></table></figure>\n<p>使用实体类Person封装上传数据，也可以直接使用Map封装数据</p>\n<ul>\n<li><p><code>HttpEntity</code>方式</p>\n<p>跟Object方式类似，request数据可以直接传递实体，也可以将请求头和请求体封装到一个HttpEntity对象中进行发送，返回数据都是ResponseEntity对象，可以取HttpStatus和HttpHeaders</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//设置请求头</span></span><br><span class=\"line\">HttpHeaders headers = <span class=\"keyword\">new</span> HttpHeaders();</span><br><span class=\"line\">MediaType type = MediaType.parseMediaType(<span class=\"string\">\"application/json; charset=UTF-8\"</span>);</span><br><span class=\"line\">headers.setContentType(type);</span><br><span class=\"line\">headers.set(<span class=\"string\">\"headerName\"</span>, <span class=\"string\">\"headerValue\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//设置HttpEntity</span></span><br><span class=\"line\">HttpEntity&lt;List&lt;Person&gt;&gt; request = <span class=\"keyword\">new</span> HttpEntity&lt;&gt;(personList, headers);</span><br><span class=\"line\"><span class=\"comment\">//返回ResponseEntity对象，对Object实体数据进行封装</span></span><br><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; response = restTemplate.postForEntity(url, request, CommonResponse.class);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>post方法上传文件</p>\n<p>上传文件需要用<code>MultiValueMap</code>进行文件数据的封装：</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultipartBodyBuilder builder = <span class=\"keyword\">new</span> MultipartBodyBuilder();</span><br><span class=\"line\">File file = <span class=\"keyword\">new</span> File(<span class=\"string\">\"D:\\\\xxx\\\\xxx.png\"</span>);</span><br><span class=\"line\">builder.part(<span class=\"string\">\"file\"</span>, <span class=\"keyword\">new</span> FileSystemResource(file));</span><br><span class=\"line\">MultiValueMap&lt;String, Object&gt; request = builder.build();</span><br><span class=\"line\"><span class=\"comment\">//上传文件</span></span><br><span class=\"line\">CommonResponse response = restTemplate.postForObject(url, request, CommonResponse.class)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>put和delete实现方式和上述方法都类似，不过这两种请求没有返回数据，不太实用</p>\n<ul>\n<li>在RestTemplate中，对GET, POST, PUT, DELETE, OPTIONS, HEAD 这几种http方法都有相应的封装，如果不想用它封装好的方法，可以选择exchange()方法自定义http请求</li>\n</ul>\n</li>\n<li><p>exchange方法：</p>\n<p>可以自己指定请求头和http method，其他的细节跟前面几种方法差不多</p>\n<p>对于某些不太常见的方法（比如HEAD或者TRACE），就需要使用exchange()方法了， exchange()方法也有多种重载</p>\n<p>以发起POST请求为例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//设置requestBody数据</span></span><br><span class=\"line\">Person person = <span class=\"keyword\">new</span> Person();</span><br><span class=\"line\"><span class=\"comment\">//设置queryParameters和pathVariables</span></span><br><span class=\"line\">Map&lt;String, Object&gt; params = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">params.put(<span class=\"string\">\"name\"</span>, <span class=\"string\">\"value\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//设置请求头</span></span><br><span class=\"line\">HttpHeaders headers = <span class=\"keyword\">new</span> HttpHeaders();</span><br><span class=\"line\">MediaType type = MediaType.parseMediaType(<span class=\"string\">\"application/json; charset=UTF-8\"</span>);</span><br><span class=\"line\">headers.setContentType(type);</span><br><span class=\"line\">headers.set(<span class=\"string\">\"headerName\"</span>, <span class=\"string\">\"headerValue\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//配置requestEntity</span></span><br><span class=\"line\">HttpEntity&lt;MyData&gt; requestEntity = <span class=\"keyword\">new</span> HttpEntity(person, headers);</span><br><span class=\"line\"><span class=\"comment\">//发起请求</span></span><br><span class=\"line\">ResponseEntity&lt;Map&gt; responseEntity = restTemplate.exchange(url, HttpMethod.POST, requestEntity, Map.class, params);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>execute方法（一般不用）：</p>\n<p>可以定制request和response的序列化和反序列化方式：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> &lt;T&gt; <span class=\"function\">T <span class=\"title\">execute</span><span class=\"params\">(URI url, HttpMethod method, RequestCallback requestCallback,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\tResponseExtractor&lt;T&gt; responseExtractor)</span> <span class=\"keyword\">throws</span> RestClientException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> doExecute(url, method, requestCallback, responseExtractor);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>RequestCallback用于封装request信息，并对这部分信息进行解析</li>\n<li>ResponseExtractor用于对response返回数据进行解析</li>\n</ul>\n<p>用户可以自由定制序列化方式，并以回调函数的形式传入execute()中；RestTemplate默认实现是两个静态内部类，默认选取RestTemplate中已经初始化完成的那部分HttpMessageConverter实现，进行序列化和反序列化操作</p>\n</li>\n</ul>\n<h2 id=\"3-异常捕捉\"><a href=\"#3-异常捕捉\" class=\"headerlink\" title=\"3. 异常捕捉\"></a>3. 异常捕捉</h2><p>RestTemplate内部已经把http请求过程中会出现的各种异常，例如404或者500等异常，都包装为了RestClientException抛出</p>\n<p>在RestTemplate中进行异常处理的组件是<code>ResponseErrorHandler</code>，默认是<code>DefaultResponseErrorHandler</code></p>\n<p>用户可以自己实现ResponseErrorHandler来处理http异常：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyselfResponseErrorHandler</span> <span class=\"keyword\">extends</span> <span class=\"title\">DefaultResponseErrorHandler</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Logger logger = LoggerFactory.getLogger(MyselfResponseErrorHandler.class);</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">handleError</span><span class=\"params\">(ClientHttpResponse response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t\tHttpStatus statusCode = getHttpStatusCode(response);</span><br><span class=\"line\">\t\tString code = statusCode.toString();</span><br><span class=\"line\">\t\tString msg = statusCode.getReasonPhrase();</span><br><span class=\"line\">\t\t<span class=\"keyword\">switch</span> (statusCode.series()) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> CLIENT_ERROR:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"客户端请求错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> SERVER_ERROR:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"服务器端错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">default</span>:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"不知道什么错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">restTemplate.setErrorHandler(responseErrorHandler);</span><br></pre></td></tr></table></figure>\n<h1 id=\"RestTemplate内部源码分析\"><a href=\"#RestTemplate内部源码分析\" class=\"headerlink\" title=\"RestTemplate内部源码分析\"></a>RestTemplate内部源码分析</h1><h2 id=\"doExecute方法\"><a href=\"#doExecute方法\" class=\"headerlink\" title=\"doExecute方法\"></a>doExecute方法</h2><p>以上所有方法都有一个最终方法，也是RestTemplate的核心方法：doExecute()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> &lt;T&gt; <span class=\"function\">T <span class=\"title\">doExecute</span><span class=\"params\">(URI url, HttpMethod method, RequestCallback requestCallback,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\tResponseExtractor&lt;T&gt; responseExtractor)</span> <span class=\"keyword\">throws</span> RestClientException </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//url和method都不能为空</span></span><br><span class=\"line\">\tAssert.notNull(url, <span class=\"string\">\"'url' must not be null\"</span>);</span><br><span class=\"line\">\tAssert.notNull(method, <span class=\"string\">\"'method' must not be null\"</span>);</span><br><span class=\"line\">\tClientHttpResponse response = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">           <span class=\"comment\">//使用ClientHttpRequestFactory创建一个ClientHttpRequest对象</span></span><br><span class=\"line\">\t\tClientHttpRequest request = createRequest(url, method);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (requestCallback != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">               <span class=\"comment\">//调用RequestCallback对象对requestBody数据进行解析</span></span><br><span class=\"line\">               <span class=\"comment\">//序列化后的数据封装到ClientHttpRequest对象中</span></span><br><span class=\"line\">\t\t\trequestCallback.doWithRequest(request);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">           <span class=\"comment\">//ClientHttpRequest对象执行http请求得到ClientHttpResponse对象</span></span><br><span class=\"line\">\t\tresponse = request.execute();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!getErrorHandler().hasError(response)) &#123;</span><br><span class=\"line\">\t\t\tlogResponseStatus(method, url, response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\thandleResponseError(method, url, response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (responseExtractor != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">               <span class=\"comment\">//将ClientHttpResponse对象反序列化为用户指定的数据类型</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> responseExtractor.extractData(response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">catch</span> (IOException ex) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ResourceAccessException(<span class=\"string\">\"I/O error on \"</span> + method.name() +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\" request for \\\"\"</span> + url + <span class=\"string\">\"\\\": \"</span> + ex.getMessage(), ex);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (response != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\tresponse.close();</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，实际执行http请求的是ClientHttpRequest，用户可以通过设置不同的ClientHttpRequestFactory自己定制http连接方式，例如HttpComponentsClientHttpRequestFactory就是基于Apache HttpClient实现：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//需要将HttpComponentsClientHttpRequestFactory暴露为spring bean，因为其实现了DisposableBean，可以在bean销毁后自动关闭连接池</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> HttpComponentsClientHttpRequestFactory <span class=\"title\">getFactory</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\tHttpComponentsClientHttpRequestFactory factory = <span class=\"keyword\">new</span> HttpComponentsClientHttpRequestFactory();</span><br><span class=\"line\">    <span class=\"comment\">//设置socket请求连接超时时间</span></span><br><span class=\"line\">\tfactory.setConnectTimeout(<span class=\"number\">5000</span>);</span><br><span class=\"line\">    <span class=\"comment\">//设置socket读取数据阻塞超时时间</span></span><br><span class=\"line\">\tfactory.setReadTimeout(<span class=\"number\">5000</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> factory;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"MessageConverter\"><a href=\"#MessageConverter\" class=\"headerlink\" title=\"MessageConverter\"></a>MessageConverter</h2><ul>\n<li><p>当调用不同的RestTemplate方法传输数据时，RestTemplate会自动检查ContentType并采用合适的MessageConverter进行序列化和反序列化，如果找不到合适的MessageConverter，将会报错</p>\n</li>\n<li><p>RestTemplate里面默认的几种MessageConverter已经能够满足大多数应用场景了</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>MessageConverter</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>StringHttpMessageConverter</td>\n<td>支持文本类型的数据格式：text/plain，text/*</td>\n</tr>\n<tr>\n<td>FormHttpMessageConverter</td>\n<td>支持表单类型的数据格式：application/x-www-form-urlencoded</td>\n</tr>\n<tr>\n<td>MappingJackson2HttpMessageConverter</td>\n<td>支持json类型的数据格式：application/json</td>\n</tr>\n<tr>\n<td>ResourceHttpMessageConverter</td>\n<td>可用于对Resource类型的文件io流进行序列化，并支持任意的MediaType</td>\n</tr>\n<tr>\n<td>BufferedImageHttpMessageConverter</td>\n<td>用于读取java.awt.image.BufferedImage格式的图片文件</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>用户可以自己定义满足自己业务需求的MessageConverter</li>\n</ul>\n<h1 id=\"WebClient\"><a href=\"#WebClient\" class=\"headerlink\" title=\"WebClient\"></a>WebClient</h1><p>spring 5.0全面引入了reactive响应式编程模式，同时也就有了RestTemplate的reactive版：WebClient</p>\n","site":{"data":{}},"excerpt":"<p><code>RestTemplate</code>是spring framework中对Http请求封装的一套方法，广泛运用于springboot和springcloud中的Http数据传输，官方文档描述如下：</p>\n<blockquote>\n<p><code>RestTemplate</code> is a synchronous client to perform HTTP requests. It is the original Spring REST client and exposes a simple, template-method API over underlying HTTP client libraries.</p>\n</blockquote>","more":"<p>不过在spring 5.0发行版中推出了基于NIO的响应式Http客户端：<code>WebClient</code>，将在未来代替RestTemplate:</p>\n<blockquote>\n<p>As of 5.0, the non-blocking, reactive <code>WebClient</code> offers a modern alternative to the <code>RestTemplate</code>, with efficient support for both synchronous and asynchronous, as well as streaming scenarios. The <code>RestTemplate</code> will be deprecated in a future version and will not have major new features added going forward.</p>\n</blockquote>\n<h3 id=\"RestTemplate的特点\"><a href=\"#RestTemplate的特点\" class=\"headerlink\" title=\"RestTemplate的特点\"></a>RestTemplate的特点</h3><ul>\n<li><p>使用方便：可直接传递Java实体对象，无需人工配置ContentType和Charset，自动识别序列化方式和消息格式，无需用户自己encode url，并对多种http传输方法进行了相应封装</p>\n</li>\n<li><p>可扩展性强：RestTemplate可配置多种底层通信框架如JDK HttpURLConnection、Apache HttpClient、OkHttp以及netty等</p>\n</li>\n<li><p>可配置性强：用户可灵活多种第三方组件，除了底层通信框架，还可扩展配置消息转换器，异常处理器，uri模板解析器和请求拦截器等组件</p>\n</li>\n<li><p>RestTemplate的多种组件过于依赖spring-framework，如果脱离了spring环境，用起来就很不方便了</p>\n</li>\n</ul>\n<h1 id=\"RestTemplate使用方法\"><a href=\"#RestTemplate使用方法\" class=\"headerlink\" title=\"RestTemplate使用方法\"></a>RestTemplate使用方法</h1><h2 id=\"1-初始化\"><a href=\"#1-初始化\" class=\"headerlink\" title=\"1. 初始化\"></a>1. 初始化</h2><p>RestTemplate有两种初始化方式：<code>构造方法形式</code>，<code>RestTemplateBuilder形式</code></p>\n<h3 id=\"构造方法形式\"><a href=\"#构造方法形式\" class=\"headerlink\" title=\"构造方法形式\"></a>构造方法形式</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> RestTemplate <span class=\"title\">restTemplate</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> RestTemplate();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RestTemplate有三种构造方法：</p>\n<ul>\n<li><p>无参数构造方法，也是使用得最普遍的一个：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> ByteArrayHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> StringHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> ResourceHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> SourceHttpMessageConverter&lt;Source&gt;());</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> AllEncompassingFormHttpMessageConverter());</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (romePresent) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> AtomFeedHttpMessageConverter());</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> RssChannelHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (jaxb2Present) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> Jaxb2RootElementHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (jackson2Present) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> MappingJackson2HttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (jacksonPresent) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.messageConverters.add(<span class=\"keyword\">new</span> MappingJacksonHttpMessageConverter());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RestTemplate中内置了多种HttpMessageConverter，用于对不同场景下的输入输出流进行序列化和反序列化，无参数构造方法主要对HttpMessageConverter列表进行初始化</p>\n</li>\n<li><p>有参数构造方法 — 初始化ClientHttpRequestFactory</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">(ClientHttpRequestFactory requestFactory)</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>();</span><br><span class=\"line\">\tsetRequestFactory(requestFactory);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>用户可以自己指定需要的ClientHttpRequestFactory，用于进行http连接和请求，默认是采用<code>SimpleClientHttpRequestFactory</code>，底层封装的是JDK的<code>HttpURLConnection</code>，用户可以指定其他种类的factory，比如以下几种：</p>\n<ul>\n<li><p><code>BufferingClientHttpRequestFactory</code>：可在内存中建立输入数据的缓存</p>\n</li>\n<li><p><code>HttpComponentsClientHttpRequestFactory</code>：采用Apache的HttpClient进行远程调用，可以配置连接池和证书信息，不过需要在pom.xml中加入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.httpcomponents<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>httpclient<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>4.5.2<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>InterceptingClientHttpRequestFactory</code>：可以配置ClientHttpRequestInterceptor拦截器对http请求进行拦截处理，springcloud中的Ribbon就用到了这个factory用于将server name url转换为实际调用的url</p>\n</li>\n<li>基于Netty4的<code>Netty4ClientHttpRequestFactory</code></li>\n<li>基于OkHttp2的<code>OkHttpClientHttpRequestFactory</code></li>\n</ul>\n</li>\n<li><p>有参数构造方法 — 添加多个HttpMessageConverter</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">RestTemplate</span><span class=\"params\">(List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters)</span> </span>&#123;</span><br><span class=\"line\">\tAssert.notEmpty(messageConverters, <span class=\"string\">\"'messageConverters' must not be empty\"</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">this</span>.messageConverters.addAll(messageConverters);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果用户觉得RestTemplate默认的几个序列化API无法满足要求，可以自己指定MessageConverter</p>\n</li>\n</ul>\n<h3 id=\"RestTemplateBuilder形式\"><a href=\"#RestTemplateBuilder形式\" class=\"headerlink\" title=\"RestTemplateBuilder形式\"></a>RestTemplateBuilder形式</h3><p>如果用户要对RestTemplate进行多种初始化配置的话，推荐使用RestTemplateBuilder建造器，属于高级用法：<br>  <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   <span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> RestTemplate <span class=\"title\">myRestTemplate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\tRestTemplateBuilder builder = <span class=\"keyword\">new</span> RestTemplateBuilder();</span><br><span class=\"line\">\tRestTemplate restTemplate = builder</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置ClientHttpRequestFactory</span></span><br><span class=\"line\">\t\t\t\t\t.requestFactory(HttpComponentsClientHttpRequestFactory.class)</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置MessageConverter</span></span><br><span class=\"line\">\t\t\t\t\t.messageConverters(<span class=\"keyword\">new</span> MappingJackson2HttpMessageConverter())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置ResponseErrorHandler</span></span><br><span class=\"line\">\t\t\t\t\t.errorHandler(<span class=\"keyword\">new</span> DefaultResponseErrorHandler())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置UriTemplateHandler</span></span><br><span class=\"line\">\t\t\t\t\t.uriTemplateHandler(<span class=\"keyword\">new</span> DefaultUriTemplateHandler())</span><br><span class=\"line\">           \t\t\t<span class=\"comment\">//配置连接超时时间和连接过期时间</span></span><br><span class=\"line\">\t\t\t\t\t.setConnectTimeout(<span class=\"number\">10000</span>)</span><br><span class=\"line\">\t\t\t\t\t.setReadTimeout(<span class=\"number\">5000</span>)</span><br><span class=\"line\">\t\t\t\t\t.build();</span><br><span class=\"line\">       <span class=\"keyword\">return</span> restTemplate;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-执行Http请求\"><a href=\"#2-执行Http请求\" class=\"headerlink\" title=\"2. 执行Http请求\"></a>2. 执行Http请求</h2><p>RestTemplate对多种http method的请求进行了封装，用户可以直接进行使用</p>\n<ul>\n<li>RestTemplate中几种常用的方法：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法名</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>getForObject</td>\n<td>通过get方法获取资源，返回用户指定的Object对象类型</td>\n</tr>\n<tr>\n<td>getForEntity</td>\n<td>通过get方法获取资源，返回一个封装好的HttpEntiry对象</td>\n</tr>\n<tr>\n<td>postForObject</td>\n<td>通过post方法发送Object对象，返回用户指定的Object对象类型</td>\n</tr>\n<tr>\n<td>postForEntity</td>\n<td>通过post方法发送Object对象，返回封装好的HttpEntiry对象</td>\n</tr>\n<tr>\n<td>put</td>\n<td>通过put方法上载资源</td>\n</tr>\n<tr>\n<td>delete</td>\n<td>通过delete方法删除服务器数据</td>\n</tr>\n<tr>\n<td>optionsForAllow</td>\n<td>获取目的资源支持的method</td>\n</tr>\n<tr>\n<td>exchange</td>\n<td>一种比较通用的方法，接受的参数分别为url，method，response数据类型，url参数，以及一个封装了http header数据和body数据的HttpEntity对象，统一返回一个封装了所有response数据的ResponseEntity对象</td>\n</tr>\n<tr>\n<td>execute</td>\n<td>该方法是RestTemplate中通用性最强的方法，以上所有方法最终都调用的是execute方法，接受的参数包括RequestCallback对象（用于选择合适的MessageConverter对requestBody数据进行解析并将结果封装到ClientHttpRequest中，进行最终的http请求），以及ResponseExtractor对象（用于将response数据解析为用户需要的数据类型）</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><p>get方法</p>\n<p>发起get请求，如果只是想获取ResponseBody数据的话直接采用getForObject()的一系列重载方法：</p>\n<ul>\n<li><p><code>Object</code>方式</p>\n<p>将返回数据封装到指定的实体类CommonResponse中，RestTemplate会自动进行序列化和反序列化</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class);</span><br><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><code>HttpEntity</code>方式</p>\n<p>如果想获取更详细的数据（比如响应头和http状态码等信息）就使用getForEntity()</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; responseEntity = restTemplate.getForEntity(url, CommonResponse.class, pathVariables);</span><br><span class=\"line\">CommonResponse responseBody = responseEntity.getBody();</span><br><span class=\"line\">HttpStatus status = responseEntity.getStatusCode();</span><br><span class=\"line\">HttpHeaders headers = responseEntity.getHeaders();</span><br></pre></td></tr></table></figure>\n<p>​    跟Object方式类似，不过返回的结果数据封装到了一个ResponseEntity对象中</p>\n<ul>\n<li><p>配置<code>queryParameters</code>和<code>pathVariables</code></p>\n<p>RestTemplate中没有为queryParameters设置对应的传参，需要用户自己将queryParameters写到url里面，不过RestTemplate为restful风格的pathVariables配置了专用的传参：</p>\n<p>（看得出来RestTemplate专业服务于restful API调用）</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">url = <span class=\"string\">\"http://127.0.0.1:8081/persons/&#123;id&#125;\"</span></span><br><span class=\"line\">Map&lt;String, Object&gt; urlVariables = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">pathVariables.put(<span class=\"string\">\"id\"</span>, <span class=\"string\">\"0\"</span>);</span><br><span class=\"line\">CommonResponse response = restTemplate.getForObject(url, CommonResponse.class, pathVariables);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>UriTemplateHandler</code>（高级用法）</p>\n<p>虽然RestTemplate中没有为queryParameters设置对应的传参，但是用户可以自己实现一个UriTemplateHandler：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">QueryParamsUrlTemplateHandler</span> <span class=\"keyword\">extends</span> <span class=\"title\">DefaultUriTemplateHandler</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> URI <span class=\"title\">expand</span><span class=\"params\">(String uriTemplate, Map&lt;String, ?&gt; params)</span> </span>&#123;</span><br><span class=\"line\">\t\tUriComponentsBuilder componentsBuilder = UriComponentsBuilder.fromHttpUrl(uriTemplate);</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span>(Map.Entry&lt;String, ?&gt; varEntry : params.entrySet())&#123;</span><br><span class=\"line\">\t\t\tcomponentsBuilder.queryParam(varEntry.getKey(), varEntry.getValue());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\turiTemplate = componentsBuilder.build().toUriString();</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.expand(uriTemplate, params);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">restTemplate.setUriTemplateHandler(urlTemplateHandler);</span><br></pre></td></tr></table></figure>\n<p>通过配置该UriTemplateHandler，就可以以Map的形式配置queryParameters了：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; params = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">params.put(<span class=\"string\">\"name\"</span>, <span class=\"string\">\"张三\"</span>);</span><br><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; responseEntity = restTemplate.getForEntity(url, CommonResponse.class, params);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p>post方法</p>\n<p>发起post请求跟get很类似，唯一不同的地方在于需要设置<code>RequestBody</code>参数：</p>\n<ul>\n<li><p><code>Object</code>方式：</p>\n<p>无需自己设置contentType和charset，只需要直接传递实体对象作为RequestBody，RestTemplate会进行自动判断并选择合适的MeesageConverter</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CommonResponse response = restTemplate.postForObject(url, person, CommonResponse.class);</span><br></pre></td></tr></table></figure>\n<p>使用实体类Person封装上传数据，也可以直接使用Map封装数据</p>\n<ul>\n<li><p><code>HttpEntity</code>方式</p>\n<p>跟Object方式类似，request数据可以直接传递实体，也可以将请求头和请求体封装到一个HttpEntity对象中进行发送，返回数据都是ResponseEntity对象，可以取HttpStatus和HttpHeaders</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//设置请求头</span></span><br><span class=\"line\">HttpHeaders headers = <span class=\"keyword\">new</span> HttpHeaders();</span><br><span class=\"line\">MediaType type = MediaType.parseMediaType(<span class=\"string\">\"application/json; charset=UTF-8\"</span>);</span><br><span class=\"line\">headers.setContentType(type);</span><br><span class=\"line\">headers.set(<span class=\"string\">\"headerName\"</span>, <span class=\"string\">\"headerValue\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//设置HttpEntity</span></span><br><span class=\"line\">HttpEntity&lt;List&lt;Person&gt;&gt; request = <span class=\"keyword\">new</span> HttpEntity&lt;&gt;(personList, headers);</span><br><span class=\"line\"><span class=\"comment\">//返回ResponseEntity对象，对Object实体数据进行封装</span></span><br><span class=\"line\">ResponseEntity&lt;CommonResponse&gt; response = restTemplate.postForEntity(url, request, CommonResponse.class);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>post方法上传文件</p>\n<p>上传文件需要用<code>MultiValueMap</code>进行文件数据的封装：</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultipartBodyBuilder builder = <span class=\"keyword\">new</span> MultipartBodyBuilder();</span><br><span class=\"line\">File file = <span class=\"keyword\">new</span> File(<span class=\"string\">\"D:\\\\xxx\\\\xxx.png\"</span>);</span><br><span class=\"line\">builder.part(<span class=\"string\">\"file\"</span>, <span class=\"keyword\">new</span> FileSystemResource(file));</span><br><span class=\"line\">MultiValueMap&lt;String, Object&gt; request = builder.build();</span><br><span class=\"line\"><span class=\"comment\">//上传文件</span></span><br><span class=\"line\">CommonResponse response = restTemplate.postForObject(url, request, CommonResponse.class)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>put和delete实现方式和上述方法都类似，不过这两种请求没有返回数据，不太实用</p>\n<ul>\n<li>在RestTemplate中，对GET, POST, PUT, DELETE, OPTIONS, HEAD 这几种http方法都有相应的封装，如果不想用它封装好的方法，可以选择exchange()方法自定义http请求</li>\n</ul>\n</li>\n<li><p>exchange方法：</p>\n<p>可以自己指定请求头和http method，其他的细节跟前面几种方法差不多</p>\n<p>对于某些不太常见的方法（比如HEAD或者TRACE），就需要使用exchange()方法了， exchange()方法也有多种重载</p>\n<p>以发起POST请求为例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//设置requestBody数据</span></span><br><span class=\"line\">Person person = <span class=\"keyword\">new</span> Person();</span><br><span class=\"line\"><span class=\"comment\">//设置queryParameters和pathVariables</span></span><br><span class=\"line\">Map&lt;String, Object&gt; params = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">params.put(<span class=\"string\">\"name\"</span>, <span class=\"string\">\"value\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//设置请求头</span></span><br><span class=\"line\">HttpHeaders headers = <span class=\"keyword\">new</span> HttpHeaders();</span><br><span class=\"line\">MediaType type = MediaType.parseMediaType(<span class=\"string\">\"application/json; charset=UTF-8\"</span>);</span><br><span class=\"line\">headers.setContentType(type);</span><br><span class=\"line\">headers.set(<span class=\"string\">\"headerName\"</span>, <span class=\"string\">\"headerValue\"</span>);</span><br><span class=\"line\"><span class=\"comment\">//配置requestEntity</span></span><br><span class=\"line\">HttpEntity&lt;MyData&gt; requestEntity = <span class=\"keyword\">new</span> HttpEntity(person, headers);</span><br><span class=\"line\"><span class=\"comment\">//发起请求</span></span><br><span class=\"line\">ResponseEntity&lt;Map&gt; responseEntity = restTemplate.exchange(url, HttpMethod.POST, requestEntity, Map.class, params);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>execute方法（一般不用）：</p>\n<p>可以定制request和response的序列化和反序列化方式：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> &lt;T&gt; <span class=\"function\">T <span class=\"title\">execute</span><span class=\"params\">(URI url, HttpMethod method, RequestCallback requestCallback,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\tResponseExtractor&lt;T&gt; responseExtractor)</span> <span class=\"keyword\">throws</span> RestClientException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> doExecute(url, method, requestCallback, responseExtractor);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>RequestCallback用于封装request信息，并对这部分信息进行解析</li>\n<li>ResponseExtractor用于对response返回数据进行解析</li>\n</ul>\n<p>用户可以自由定制序列化方式，并以回调函数的形式传入execute()中；RestTemplate默认实现是两个静态内部类，默认选取RestTemplate中已经初始化完成的那部分HttpMessageConverter实现，进行序列化和反序列化操作</p>\n</li>\n</ul>\n<h2 id=\"3-异常捕捉\"><a href=\"#3-异常捕捉\" class=\"headerlink\" title=\"3. 异常捕捉\"></a>3. 异常捕捉</h2><p>RestTemplate内部已经把http请求过程中会出现的各种异常，例如404或者500等异常，都包装为了RestClientException抛出</p>\n<p>在RestTemplate中进行异常处理的组件是<code>ResponseErrorHandler</code>，默认是<code>DefaultResponseErrorHandler</code></p>\n<p>用户可以自己实现ResponseErrorHandler来处理http异常：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyselfResponseErrorHandler</span> <span class=\"keyword\">extends</span> <span class=\"title\">DefaultResponseErrorHandler</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Logger logger = LoggerFactory.getLogger(MyselfResponseErrorHandler.class);</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">handleError</span><span class=\"params\">(ClientHttpResponse response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t\tHttpStatus statusCode = getHttpStatusCode(response);</span><br><span class=\"line\">\t\tString code = statusCode.toString();</span><br><span class=\"line\">\t\tString msg = statusCode.getReasonPhrase();</span><br><span class=\"line\">\t\t<span class=\"keyword\">switch</span> (statusCode.series()) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> CLIENT_ERROR:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"客户端请求错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> SERVER_ERROR:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"服务器端错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">default</span>:</span><br><span class=\"line\">\t\t\t\tlogger.error(<span class=\"string\">\"不知道什么错误，错误码：\"</span> + code + <span class=\"string\">\", 错误信息：\"</span> + msg);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">restTemplate.setErrorHandler(responseErrorHandler);</span><br></pre></td></tr></table></figure>\n<h1 id=\"RestTemplate内部源码分析\"><a href=\"#RestTemplate内部源码分析\" class=\"headerlink\" title=\"RestTemplate内部源码分析\"></a>RestTemplate内部源码分析</h1><h2 id=\"doExecute方法\"><a href=\"#doExecute方法\" class=\"headerlink\" title=\"doExecute方法\"></a>doExecute方法</h2><p>以上所有方法都有一个最终方法，也是RestTemplate的核心方法：doExecute()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> &lt;T&gt; <span class=\"function\">T <span class=\"title\">doExecute</span><span class=\"params\">(URI url, HttpMethod method, RequestCallback requestCallback,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\tResponseExtractor&lt;T&gt; responseExtractor)</span> <span class=\"keyword\">throws</span> RestClientException </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//url和method都不能为空</span></span><br><span class=\"line\">\tAssert.notNull(url, <span class=\"string\">\"'url' must not be null\"</span>);</span><br><span class=\"line\">\tAssert.notNull(method, <span class=\"string\">\"'method' must not be null\"</span>);</span><br><span class=\"line\">\tClientHttpResponse response = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">           <span class=\"comment\">//使用ClientHttpRequestFactory创建一个ClientHttpRequest对象</span></span><br><span class=\"line\">\t\tClientHttpRequest request = createRequest(url, method);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (requestCallback != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">               <span class=\"comment\">//调用RequestCallback对象对requestBody数据进行解析</span></span><br><span class=\"line\">               <span class=\"comment\">//序列化后的数据封装到ClientHttpRequest对象中</span></span><br><span class=\"line\">\t\t\trequestCallback.doWithRequest(request);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">           <span class=\"comment\">//ClientHttpRequest对象执行http请求得到ClientHttpResponse对象</span></span><br><span class=\"line\">\t\tresponse = request.execute();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!getErrorHandler().hasError(response)) &#123;</span><br><span class=\"line\">\t\t\tlogResponseStatus(method, url, response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\thandleResponseError(method, url, response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (responseExtractor != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">               <span class=\"comment\">//将ClientHttpResponse对象反序列化为用户指定的数据类型</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> responseExtractor.extractData(response);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">catch</span> (IOException ex) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ResourceAccessException(<span class=\"string\">\"I/O error on \"</span> + method.name() +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\" request for \\\"\"</span> + url + <span class=\"string\">\"\\\": \"</span> + ex.getMessage(), ex);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (response != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\tresponse.close();</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，实际执行http请求的是ClientHttpRequest，用户可以通过设置不同的ClientHttpRequestFactory自己定制http连接方式，例如HttpComponentsClientHttpRequestFactory就是基于Apache HttpClient实现：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//需要将HttpComponentsClientHttpRequestFactory暴露为spring bean，因为其实现了DisposableBean，可以在bean销毁后自动关闭连接池</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> HttpComponentsClientHttpRequestFactory <span class=\"title\">getFactory</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\tHttpComponentsClientHttpRequestFactory factory = <span class=\"keyword\">new</span> HttpComponentsClientHttpRequestFactory();</span><br><span class=\"line\">    <span class=\"comment\">//设置socket请求连接超时时间</span></span><br><span class=\"line\">\tfactory.setConnectTimeout(<span class=\"number\">5000</span>);</span><br><span class=\"line\">    <span class=\"comment\">//设置socket读取数据阻塞超时时间</span></span><br><span class=\"line\">\tfactory.setReadTimeout(<span class=\"number\">5000</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> factory;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"MessageConverter\"><a href=\"#MessageConverter\" class=\"headerlink\" title=\"MessageConverter\"></a>MessageConverter</h2><ul>\n<li><p>当调用不同的RestTemplate方法传输数据时，RestTemplate会自动检查ContentType并采用合适的MessageConverter进行序列化和反序列化，如果找不到合适的MessageConverter，将会报错</p>\n</li>\n<li><p>RestTemplate里面默认的几种MessageConverter已经能够满足大多数应用场景了</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>MessageConverter</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>StringHttpMessageConverter</td>\n<td>支持文本类型的数据格式：text/plain，text/*</td>\n</tr>\n<tr>\n<td>FormHttpMessageConverter</td>\n<td>支持表单类型的数据格式：application/x-www-form-urlencoded</td>\n</tr>\n<tr>\n<td>MappingJackson2HttpMessageConverter</td>\n<td>支持json类型的数据格式：application/json</td>\n</tr>\n<tr>\n<td>ResourceHttpMessageConverter</td>\n<td>可用于对Resource类型的文件io流进行序列化，并支持任意的MediaType</td>\n</tr>\n<tr>\n<td>BufferedImageHttpMessageConverter</td>\n<td>用于读取java.awt.image.BufferedImage格式的图片文件</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>用户可以自己定义满足自己业务需求的MessageConverter</li>\n</ul>\n<h1 id=\"WebClient\"><a href=\"#WebClient\" class=\"headerlink\" title=\"WebClient\"></a>WebClient</h1><p>spring 5.0全面引入了reactive响应式编程模式，同时也就有了RestTemplate的reactive版：WebClient</p>"},{"title":"vue学习笔记（1） —— 用vue-cli搭建spa工程","author":"天渊","date":"2019-03-18T05:09:22.000Z","_content":"使用webpack和vue搭建搭建单页面应用程序（SPA，Single-Page Application）是前端开发的发展趋势之一，现在来学习一下在Intellij IDEA中使用`vue-cli`搭建一个基于vue框架的SPA demo程序，并部署到nginx服务器\n<!--more-->\n\n### 初始化工程\n\n1. 首先保证本机安装有最新版的node.js和npm，使用`node -v`查看版本，不赘述\n\n2. 命令行执行`npm install -g vue-cli`全局安装`vue-cli`\n3. 进入想要构建工程的目录，执行命令`vue init webpack project-name`，webpack默认版本目前是2.0\n4. 接下来需要为初始化工程进行配置，根据提示按需配置，一般来说直接默认就够了\n\n### 配置IDEA\n\n需要在IDEA中安装vue相关的插件\n\n1. `File -> Settings -> Plugins -> Browse respositoties`搜索`Vue.js`安装，然后重启IDEA\n\n2. `File -> Settings -> Editor -> File Types -> HTML`，将`.vue`文件配置为默认的html类型\n\n3. `File -> Settings -> Language & Frameworks -> JavaScript`，将js版本设置为ES6\n\n4. 使用IDEA打开之前初始化完成的vue工程，点击工具栏的`Edit Configurations`进行启动配置：command选择`run`，Scripts选择`dev`环境\n\n5. 点击启动：\n   \n\t![upload successful](\\blog\\images\\pasted-4.png)\n\n6. 打开`http://localhost:8080/`即可看到官方的HelloWorld页面，接下来在此工程的基础上进行开发即可\n\n\n\n### 认识vue-cli工程结构\n\n在vue-cli工程初始化完成后的工程中，目录如下：\n\n![upload successful](\\blog\\images\\pasted-5.png)\n\n1. 最重要的文件夹是`src`，这下面包含了跟页面app有关的所有源代码，包括各类js, css, .vue模板，以及router\n2. 除了`src`文件夹，最外层的`index.html`即为项目主页，即`SPA`中那个`single-page`\n3. `static`文件夹存放其他类型的静态资源如图片和字体等\n4. 其他文件夹（test, build, config）是与项目构建，编译和测试相关的配置，现阶段暂时不用管\n\n为何vue工程有自己独特的`.vue`文件？官网叫其为单文件组件，通过webpack源码转换，会全部转换为对应的文件，通常用于自定义组件模板，包括`template`的html模板，`style`样式以及js脚本，如HelloWorld工程的`App.vue`文件：\n\n![upload successful](\\blog\\images\\pasted-6.png)\n\n`template`为html页面框架，`style`为当前组件的css样式，`script`则主要用于编写并导出当前组件脚本。\n\n### vue-cli工程运行流程\n\nSPA工程遵循一定的规则和流程对页面进行渲染，以当前HelloWorld工程为例：\n\n1. 首先打开主页面`index.html`，vue基于`el`属性，对`id=\"app\"`的这个div进行渲染，脚本位于入口js文件`main.js`中，有关的js脚本以及router文件都需要导入到这个入口js文件中来：\n\n   ```html\n   <body>\n       <div id=\"app\"></div>\n   </body>\n   ```\n\n   ```javascript\n   new Vue({\n     el: '#app',\n     router,\n     components: { App },\n     template: '<App/>'\n   })\n   ```\n\n2. 如上，`router`为vue-router路由对象，用于配置需要导入的组件页面，本例子中router配置于`/router/index.js`文件中：\n\n   ```javascript\n   Vue.use(Router)\n   export default new Router({\n     routes: [\n       {\n         path: '/',\n         name: 'HelloWorld',\n         component: HelloWorld\n       }\n     ]\n   })\n   ```\n\n   将`HelloWorld`组件配置到router对象中\n\n3. `components`用于配置自定义组件 `App`，来自于`App.vue`文件：\n\n   ```html\n   <template>\n     <div id=\"app\">\n       <img src=\"./assets/logo.png\">\n       <router-view/>\n     </div>\n   </template>\n   \n   <script>\n   // 命名并导出当前组件\n   export default {\n     name: 'App'\n   }\n   </script>\n   \n   <style>\n   ...\n   </style>\n   ```\n\n4. `template`用于将当前html替换为自定义组件模板`App`，也是来自于`App.vue`文件\n\n5. `<App/>`中的`<router-view/>`标签用于渲染之前router对象中配置的页面，即`HelloWorld.vue`中的内容\n\n至此整个流程完成","source":"_posts/vue学习笔记-——-用vue-cli搭建spa工程.md","raw":"title: vue学习笔记（1） —— 用vue-cli搭建spa工程\nauthor: 天渊\ndate: 2019-03-18 13:09:22\ntags:\n---\n使用webpack和vue搭建搭建单页面应用程序（SPA，Single-Page Application）是前端开发的发展趋势之一，现在来学习一下在Intellij IDEA中使用`vue-cli`搭建一个基于vue框架的SPA demo程序，并部署到nginx服务器\n<!--more-->\n\n### 初始化工程\n\n1. 首先保证本机安装有最新版的node.js和npm，使用`node -v`查看版本，不赘述\n\n2. 命令行执行`npm install -g vue-cli`全局安装`vue-cli`\n3. 进入想要构建工程的目录，执行命令`vue init webpack project-name`，webpack默认版本目前是2.0\n4. 接下来需要为初始化工程进行配置，根据提示按需配置，一般来说直接默认就够了\n\n### 配置IDEA\n\n需要在IDEA中安装vue相关的插件\n\n1. `File -> Settings -> Plugins -> Browse respositoties`搜索`Vue.js`安装，然后重启IDEA\n\n2. `File -> Settings -> Editor -> File Types -> HTML`，将`.vue`文件配置为默认的html类型\n\n3. `File -> Settings -> Language & Frameworks -> JavaScript`，将js版本设置为ES6\n\n4. 使用IDEA打开之前初始化完成的vue工程，点击工具栏的`Edit Configurations`进行启动配置：command选择`run`，Scripts选择`dev`环境\n\n5. 点击启动：\n   \n\t![upload successful](\\blog\\images\\pasted-4.png)\n\n6. 打开`http://localhost:8080/`即可看到官方的HelloWorld页面，接下来在此工程的基础上进行开发即可\n\n\n\n### 认识vue-cli工程结构\n\n在vue-cli工程初始化完成后的工程中，目录如下：\n\n![upload successful](\\blog\\images\\pasted-5.png)\n\n1. 最重要的文件夹是`src`，这下面包含了跟页面app有关的所有源代码，包括各类js, css, .vue模板，以及router\n2. 除了`src`文件夹，最外层的`index.html`即为项目主页，即`SPA`中那个`single-page`\n3. `static`文件夹存放其他类型的静态资源如图片和字体等\n4. 其他文件夹（test, build, config）是与项目构建，编译和测试相关的配置，现阶段暂时不用管\n\n为何vue工程有自己独特的`.vue`文件？官网叫其为单文件组件，通过webpack源码转换，会全部转换为对应的文件，通常用于自定义组件模板，包括`template`的html模板，`style`样式以及js脚本，如HelloWorld工程的`App.vue`文件：\n\n![upload successful](\\blog\\images\\pasted-6.png)\n\n`template`为html页面框架，`style`为当前组件的css样式，`script`则主要用于编写并导出当前组件脚本。\n\n### vue-cli工程运行流程\n\nSPA工程遵循一定的规则和流程对页面进行渲染，以当前HelloWorld工程为例：\n\n1. 首先打开主页面`index.html`，vue基于`el`属性，对`id=\"app\"`的这个div进行渲染，脚本位于入口js文件`main.js`中，有关的js脚本以及router文件都需要导入到这个入口js文件中来：\n\n   ```html\n   <body>\n       <div id=\"app\"></div>\n   </body>\n   ```\n\n   ```javascript\n   new Vue({\n     el: '#app',\n     router,\n     components: { App },\n     template: '<App/>'\n   })\n   ```\n\n2. 如上，`router`为vue-router路由对象，用于配置需要导入的组件页面，本例子中router配置于`/router/index.js`文件中：\n\n   ```javascript\n   Vue.use(Router)\n   export default new Router({\n     routes: [\n       {\n         path: '/',\n         name: 'HelloWorld',\n         component: HelloWorld\n       }\n     ]\n   })\n   ```\n\n   将`HelloWorld`组件配置到router对象中\n\n3. `components`用于配置自定义组件 `App`，来自于`App.vue`文件：\n\n   ```html\n   <template>\n     <div id=\"app\">\n       <img src=\"./assets/logo.png\">\n       <router-view/>\n     </div>\n   </template>\n   \n   <script>\n   // 命名并导出当前组件\n   export default {\n     name: 'App'\n   }\n   </script>\n   \n   <style>\n   ...\n   </style>\n   ```\n\n4. `template`用于将当前html替换为自定义组件模板`App`，也是来自于`App.vue`文件\n\n5. `<App/>`中的`<router-view/>`标签用于渲染之前router对象中配置的页面，即`HelloWorld.vue`中的内容\n\n至此整个流程完成","slug":"vue学习笔记-——-用vue-cli搭建spa工程","published":1,"updated":"2019-03-19T13:30:58.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js45001gg0qrkqml3fvt","content":"<p>使用webpack和vue搭建搭建单页面应用程序（SPA，Single-Page Application）是前端开发的发展趋势之一，现在来学习一下在Intellij IDEA中使用<code>vue-cli</code>搭建一个基于vue框架的SPA demo程序，并部署到nginx服务器<br><a id=\"more\"></a></p>\n<h3 id=\"初始化工程\"><a href=\"#初始化工程\" class=\"headerlink\" title=\"初始化工程\"></a>初始化工程</h3><ol>\n<li><p>首先保证本机安装有最新版的node.js和npm，使用<code>node -v</code>查看版本，不赘述</p>\n</li>\n<li><p>命令行执行<code>npm install -g vue-cli</code>全局安装<code>vue-cli</code></p>\n</li>\n<li>进入想要构建工程的目录，执行命令<code>vue init webpack project-name</code>，webpack默认版本目前是2.0</li>\n<li>接下来需要为初始化工程进行配置，根据提示按需配置，一般来说直接默认就够了</li>\n</ol>\n<h3 id=\"配置IDEA\"><a href=\"#配置IDEA\" class=\"headerlink\" title=\"配置IDEA\"></a>配置IDEA</h3><p>需要在IDEA中安装vue相关的插件</p>\n<ol>\n<li><p><code>File -&gt; Settings -&gt; Plugins -&gt; Browse respositoties</code>搜索<code>Vue.js</code>安装，然后重启IDEA</p>\n</li>\n<li><p><code>File -&gt; Settings -&gt; Editor -&gt; File Types -&gt; HTML</code>，将<code>.vue</code>文件配置为默认的html类型</p>\n</li>\n<li><p><code>File -&gt; Settings -&gt; Language &amp; Frameworks -&gt; JavaScript</code>，将js版本设置为ES6</p>\n</li>\n<li><p>使用IDEA打开之前初始化完成的vue工程，点击工具栏的<code>Edit Configurations</code>进行启动配置：command选择<code>run</code>，Scripts选择<code>dev</code>环境</p>\n</li>\n<li><p>点击启动：</p>\n<p> <img src=\"\\blog\\images\\pasted-4.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>打开<code>http://localhost:8080/</code>即可看到官方的HelloWorld页面，接下来在此工程的基础上进行开发即可</p>\n</li>\n</ol>\n<h3 id=\"认识vue-cli工程结构\"><a href=\"#认识vue-cli工程结构\" class=\"headerlink\" title=\"认识vue-cli工程结构\"></a>认识vue-cli工程结构</h3><p>在vue-cli工程初始化完成后的工程中，目录如下：</p>\n<p><img src=\"\\blog\\images\\pasted-5.png\" alt=\"upload successful\"></p>\n<ol>\n<li>最重要的文件夹是<code>src</code>，这下面包含了跟页面app有关的所有源代码，包括各类js, css, .vue模板，以及router</li>\n<li>除了<code>src</code>文件夹，最外层的<code>index.html</code>即为项目主页，即<code>SPA</code>中那个<code>single-page</code></li>\n<li><code>static</code>文件夹存放其他类型的静态资源如图片和字体等</li>\n<li>其他文件夹（test, build, config）是与项目构建，编译和测试相关的配置，现阶段暂时不用管</li>\n</ol>\n<p>为何vue工程有自己独特的<code>.vue</code>文件？官网叫其为单文件组件，通过webpack源码转换，会全部转换为对应的文件，通常用于自定义组件模板，包括<code>template</code>的html模板，<code>style</code>样式以及js脚本，如HelloWorld工程的<code>App.vue</code>文件：</p>\n<p><img src=\"\\blog\\images\\pasted-6.png\" alt=\"upload successful\"></p>\n<p><code>template</code>为html页面框架，<code>style</code>为当前组件的css样式，<code>script</code>则主要用于编写并导出当前组件脚本。</p>\n<h3 id=\"vue-cli工程运行流程\"><a href=\"#vue-cli工程运行流程\" class=\"headerlink\" title=\"vue-cli工程运行流程\"></a>vue-cli工程运行流程</h3><p>SPA工程遵循一定的规则和流程对页面进行渲染，以当前HelloWorld工程为例：</p>\n<ol>\n<li><p>首先打开主页面<code>index.html</code>，vue基于<code>el</code>属性，对<code>id=&quot;app&quot;</code>的这个div进行渲染，脚本位于入口js文件<code>main.js</code>中，有关的js脚本以及router文件都需要导入到这个入口js文件中来：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"app\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">new</span> Vue(&#123;</span><br><span class=\"line\">  el: <span class=\"string\">'#app'</span>,</span><br><span class=\"line\">  router,</span><br><span class=\"line\">  components: &#123; App &#125;,</span><br><span class=\"line\">  template: <span class=\"string\">'&lt;App/&gt;'</span></span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如上，<code>router</code>为vue-router路由对象，用于配置需要导入的组件页面，本例子中router配置于<code>/router/index.js</code>文件中：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Vue.use(Router)</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"keyword\">new</span> Router(&#123;</span><br><span class=\"line\">  routes: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      path: <span class=\"string\">'/'</span>,</span><br><span class=\"line\">      name: <span class=\"string\">'HelloWorld'</span>,</span><br><span class=\"line\">      component: HelloWorld</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>将<code>HelloWorld</code>组件配置到router对象中</p>\n</li>\n<li><p><code>components</code>用于配置自定义组件 <code>App</code>，来自于<code>App.vue</code>文件：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">template</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"app\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">\"./assets/logo.png\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">router-view</span>/&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">template</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"javascript\"><span class=\"comment\">// 命名并导出当前组件</span></span></span><br><span class=\"line\"><span class=\"javascript\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> &#123;</span></span><br><span class=\"line\"><span class=\"javascript\">  name: <span class=\"string\">'App'</span></span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">style</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"undefined\">...</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">style</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>template</code>用于将当前html替换为自定义组件模板<code>App</code>，也是来自于<code>App.vue</code>文件</p>\n</li>\n<li><p><code>&lt;App/&gt;</code>中的<code>&lt;router-view/&gt;</code>标签用于渲染之前router对象中配置的页面，即<code>HelloWorld.vue</code>中的内容</p>\n</li>\n</ol>\n<p>至此整个流程完成</p>\n","site":{"data":{}},"excerpt":"<p>使用webpack和vue搭建搭建单页面应用程序（SPA，Single-Page Application）是前端开发的发展趋势之一，现在来学习一下在Intellij IDEA中使用<code>vue-cli</code>搭建一个基于vue框架的SPA demo程序，并部署到nginx服务器<br>","more":"</p>\n<h3 id=\"初始化工程\"><a href=\"#初始化工程\" class=\"headerlink\" title=\"初始化工程\"></a>初始化工程</h3><ol>\n<li><p>首先保证本机安装有最新版的node.js和npm，使用<code>node -v</code>查看版本，不赘述</p>\n</li>\n<li><p>命令行执行<code>npm install -g vue-cli</code>全局安装<code>vue-cli</code></p>\n</li>\n<li>进入想要构建工程的目录，执行命令<code>vue init webpack project-name</code>，webpack默认版本目前是2.0</li>\n<li>接下来需要为初始化工程进行配置，根据提示按需配置，一般来说直接默认就够了</li>\n</ol>\n<h3 id=\"配置IDEA\"><a href=\"#配置IDEA\" class=\"headerlink\" title=\"配置IDEA\"></a>配置IDEA</h3><p>需要在IDEA中安装vue相关的插件</p>\n<ol>\n<li><p><code>File -&gt; Settings -&gt; Plugins -&gt; Browse respositoties</code>搜索<code>Vue.js</code>安装，然后重启IDEA</p>\n</li>\n<li><p><code>File -&gt; Settings -&gt; Editor -&gt; File Types -&gt; HTML</code>，将<code>.vue</code>文件配置为默认的html类型</p>\n</li>\n<li><p><code>File -&gt; Settings -&gt; Language &amp; Frameworks -&gt; JavaScript</code>，将js版本设置为ES6</p>\n</li>\n<li><p>使用IDEA打开之前初始化完成的vue工程，点击工具栏的<code>Edit Configurations</code>进行启动配置：command选择<code>run</code>，Scripts选择<code>dev</code>环境</p>\n</li>\n<li><p>点击启动：</p>\n<p> <img src=\"\\blog\\images\\pasted-4.png\" alt=\"upload successful\"></p>\n</li>\n<li><p>打开<code>http://localhost:8080/</code>即可看到官方的HelloWorld页面，接下来在此工程的基础上进行开发即可</p>\n</li>\n</ol>\n<h3 id=\"认识vue-cli工程结构\"><a href=\"#认识vue-cli工程结构\" class=\"headerlink\" title=\"认识vue-cli工程结构\"></a>认识vue-cli工程结构</h3><p>在vue-cli工程初始化完成后的工程中，目录如下：</p>\n<p><img src=\"\\blog\\images\\pasted-5.png\" alt=\"upload successful\"></p>\n<ol>\n<li>最重要的文件夹是<code>src</code>，这下面包含了跟页面app有关的所有源代码，包括各类js, css, .vue模板，以及router</li>\n<li>除了<code>src</code>文件夹，最外层的<code>index.html</code>即为项目主页，即<code>SPA</code>中那个<code>single-page</code></li>\n<li><code>static</code>文件夹存放其他类型的静态资源如图片和字体等</li>\n<li>其他文件夹（test, build, config）是与项目构建，编译和测试相关的配置，现阶段暂时不用管</li>\n</ol>\n<p>为何vue工程有自己独特的<code>.vue</code>文件？官网叫其为单文件组件，通过webpack源码转换，会全部转换为对应的文件，通常用于自定义组件模板，包括<code>template</code>的html模板，<code>style</code>样式以及js脚本，如HelloWorld工程的<code>App.vue</code>文件：</p>\n<p><img src=\"\\blog\\images\\pasted-6.png\" alt=\"upload successful\"></p>\n<p><code>template</code>为html页面框架，<code>style</code>为当前组件的css样式，<code>script</code>则主要用于编写并导出当前组件脚本。</p>\n<h3 id=\"vue-cli工程运行流程\"><a href=\"#vue-cli工程运行流程\" class=\"headerlink\" title=\"vue-cli工程运行流程\"></a>vue-cli工程运行流程</h3><p>SPA工程遵循一定的规则和流程对页面进行渲染，以当前HelloWorld工程为例：</p>\n<ol>\n<li><p>首先打开主页面<code>index.html</code>，vue基于<code>el</code>属性，对<code>id=&quot;app&quot;</code>的这个div进行渲染，脚本位于入口js文件<code>main.js</code>中，有关的js脚本以及router文件都需要导入到这个入口js文件中来：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"app\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">new</span> Vue(&#123;</span><br><span class=\"line\">  el: <span class=\"string\">'#app'</span>,</span><br><span class=\"line\">  router,</span><br><span class=\"line\">  components: &#123; App &#125;,</span><br><span class=\"line\">  template: <span class=\"string\">'&lt;App/&gt;'</span></span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如上，<code>router</code>为vue-router路由对象，用于配置需要导入的组件页面，本例子中router配置于<code>/router/index.js</code>文件中：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Vue.use(Router)</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"keyword\">new</span> Router(&#123;</span><br><span class=\"line\">  routes: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      path: <span class=\"string\">'/'</span>,</span><br><span class=\"line\">      name: <span class=\"string\">'HelloWorld'</span>,</span><br><span class=\"line\">      component: HelloWorld</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>将<code>HelloWorld</code>组件配置到router对象中</p>\n</li>\n<li><p><code>components</code>用于配置自定义组件 <code>App</code>，来自于<code>App.vue</code>文件：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">template</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"app\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">\"./assets/logo.png\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">router-view</span>/&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">template</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"javascript\"><span class=\"comment\">// 命名并导出当前组件</span></span></span><br><span class=\"line\"><span class=\"javascript\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> &#123;</span></span><br><span class=\"line\"><span class=\"javascript\">  name: <span class=\"string\">'App'</span></span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">style</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"undefined\">...</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">style</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>template</code>用于将当前html替换为自定义组件模板<code>App</code>，也是来自于<code>App.vue</code>文件</p>\n</li>\n<li><p><code>&lt;App/&gt;</code>中的<code>&lt;router-view/&gt;</code>标签用于渲染之前router对象中配置的页面，即<code>HelloWorld.vue</code>中的内容</p>\n</li>\n</ol>\n<p>至此整个流程完成</p>"},{"title":"kafka学习笔记（1）—— 基础知识","author":"天渊","date":"2019-02-13T08:31:00.000Z","_content":"Kafka是由LinkedIn开发并开源的分布式发布/订阅模式的消息队列系统，因其分布式及高吞吐率而被广泛使用，目前在大数据处理领域占有很重要的地位，能够很方便地与Hadoop, Spark, Storm, Flink和Flume等大数据处理工具进行集成\n<!--more-->\n\n### Kafka主要特点\n\nKafka有三个主要的作用：`传统意义上的消息系统`，`分布式存储`，`流处理工具`\n\n**Kafka最重要的作用就是企业级消息队列服务**：\n\n- 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能\n- Server间的topic分区（partition）消费，保证了高吞吐率，同时保证每个分区内的消息顺序传输\n- 发布/订阅模式中，单个topic支持多consumer，并支持consumer-group自动负载均衡\n\n- 不同于RabbitMQ等传统消息队列，kafka不会删除历史数据\n- 支持数据备份（repliaction），通过master-slave方式保证数据一致性以及高可用\n\n**Kafka-stream**：\n\nKafka Stream是Apache Kafka从0.10版本引入的一个新Feature，它提供了对存储于Kafka内的数据进行流式处理和分析的功能。\n\n### Kafka架构\n\nKafka涉及到的一些专用名词\n\n- **Broker**\n  　　Kafka集群包含一个或多个服务器，这种服务器被称为broker\n- **Topic**\n  　　每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）\n- **Partition**\n  　　分片，每个Topic包含一个或多个Partition.\n- **repliaction**\n  　　复制集，每个partition包含一个或多个repliaction，其中有一个master多个slave\n- **Producer**\n  　　消息发布者，负责发布消息到Kafka broker\n- **Consumer**\n  　　消息消费者，通过拉取的方式向Kafka broker读取消息\n- **Consumer Group**\n  　　每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nkafka集群拓扑：\n\n![](/blog/images/kafka.png)\n- Kafka集群中包含若干Producer，使用push模式将消息**批量**发布到broker\n- 包含若干broker，支持水平扩展，broker数量越多，集群吞吐率越高，新建的topic-partition及其复制集均分到各个broker上\n- 由Consumer Group管理consumer实例，单个consumer实例可以订阅多个topic，若同一个Consumer Group中的多个consumer实例订阅了同一个topic，则由kafka集群自动进行负载均衡，统一协调分配partition进行消费\n- kafka强依赖Zookeeper集群，通过Zookeeper管理集群配置，保存元数据，选举partition leader，以及在Consumer Group发生变化时进行rebalance\n\n### Kafka与常用MQ对比\n\n- RabbitMQ：支持多种协议栈（AMQP，XMPP, SMTP等），功能更加强大（推拉消费，延迟消费，优先消费等），安全性和可靠性要优于kafka，相比较之下Kafka设计更加简单，吞吐量更高，更加适用于大规模日志数据处理\n- RocketMQ：阿里开源的消息队列，最早思路来源于kafka，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点，相比于kafka在可靠性和稳定性方面均有提升，而且支持事务消息\n- ZeroMQ：号称最快的消息队列系统，具有超高吞吐量，但作用场景有限，大部分情况下作为数据流传输模块嵌入到各个中间件中\n\n### Kafka文件存储方式\n\nKafka以顺序I/O的方式将消息存入磁盘进行持久化，保证了足够的刷盘速度:\n\n- Kafka存储的每条消息数据称为`Message`，每条`Message`数据包含四个属性：`offset`，`MessageSize`，`data`，`timestamp`时间戳即时间戳类型\n\n- Kafka的消息队列在逻辑上是由`partition`的方式存在的，每个`partition`的`repliaction`在物理上由多个`segment`分段组成，每个`segment`数据文件以该段中最小的 offset 命名，文件扩展名为`.log`，查找指定 offset 的 Message 的时候，使用二分查找定位到该 Message 在哪个 `segment` 数据文件；写入消息时直接将消息添加到最新`segment`文件的末尾\n\n- 每个`segment`分段都有自己的索引文件，扩展名为`.index`，索引文件采用稀疏索引的方式建立索引，每隔一定字节的数据建立一条索引，这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中\n\n![upload successful](\\blog\\images\\pasted-0.png)\n\n  `log.dirs=/home1/irteam/apps/kafka/data/kafka/kafka-logs`\n\n  在该文件夹中，每个topic的partition都有自己单独的文件夹进行存放，比如`resource-v1-APIGateway-Api`这个topic编号为0的partition放如下位置：\n\n  `/home1/irteam/apps/kafka/data/kafka/kafka-logs/resource-v1-APIGateway-Api-0/`\n  \n![upload successful](\\blog\\images\\pasted-1.png)\n  其中`.index`，`.log`，`.timestamp`三个文件分别对应`分段索引文件`，`分段数据`和`时间索引`\n    \n  **时间索引**：`.timeindex`文件用于保存当前分段中消息发布时间与offset的稀疏索引，用于定期删除消息（`log.retention.hours`参数）\n    \n    \n### Kafka搭建集群即基本操作\n\n1. 前期工作\n\n   首先保证当前主机安装有jdk，推荐jdk 8 及其以上版本\n\n   Kafka发行版自带zookeeper，无需单独安装zookeeper集群，当然也可以自己另外搭建zookeeper集群\n\n   最新版kafka发行版下载地址：\n\n   https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz\n\n   下载到本地并解压tar文件得到`kafka_2.11-2.1.0`文件夹 （2.11是kafka源码的scala版本，2.1.0是kafka实际发行版本）：\n   \n2. 初始化配置\n\n   为了简便起见这里就只配置单点broker；进入`config`文件夹，`server.properties`是kafka集群的主配置文件，大部分配置都可以选择默认，部分配置需要注意一下：\n\n   ```properties\n   # 当前主机id，也是集群中唯一id，将保存于zookeeper中\n   broker.id=0\n   # 监听地址和端口，如果没有配置的话默认当前主机名；端口默认9092\n   listeners = PLAINTEXT://your.host.name:9092\n   # 给生产者和消费者的广播地址，如果没设置的话默认采用上面的listeners属性\n   advertised.listeners=PLAINTEXT://your.host.name:9092\n   # kafka数据存放路径\n   log.dirs=/tmp/kafka-logs\n   # topic默认的分片数，创建topic的适合可以单独指定该属性\n   # 理论上，分区数量越多，吞吐量越大，但会造成更严重的资源消耗\n   num.partitions=1\n   # 是否允许自动创建topic（当producer或者consumer发布/订阅某个不存在的topic时）\n   auto.create.topics.enable=true\n   # 是否允许删除topic（删除topic后需要重启broker，不过即使这样也无法完全删除topic数据，需要进入zookeeper删除topic元数据，不过很危险，不推荐）\n   delete.topic.enable=true\n   # kafka采取异步刷盘的方式将内存中收到的消息序列化到硬盘上，下面两个条件任意满足一项即开启刷盘\n   # 每收到10000条消息刷盘一次\n   log.flush.interval.messages=10000\n   # 每隔1000ms刷盘一次\n   log.flush.interval.ms=1000\n   # 消息删除策略，保存消息的最长时间\n   log.retention.hours=168\n   # 单个分区保留消息的最大容量，默认就是1G\n   log.retention.bytes=1073741824\n   # segment分段的最大容量，单个segment超出这个容量后将创建一个新的segment继续保存消息\n   log.segment.bytes=1073741824\n   # 可接收的消息最大大小（压缩后的大小）\n   message.max.bytes=1048576\n   # zookeeper配置\n   zookeeper.connect=localhost:2181\n   zookeeper.connection.timeout.ms=6000\n   ```\n\n   关于kafka broker更详细的配置策略请参考官网：https://kafka.apache.org/documentation/#brokerconfigs\n\n3. 配置完成后，首先启动zookeeper，如果启动的是kafka默认自带的zookeeper的话，进入kafka安装目录，按照以下方式启动：\n\n   ```shell\n   > ./bin/zookeeper-server-start.sh ./config/zookeeper.properties\n   ```\n\n   再执行以下命令可以进入zookeeper，即可查看zookeeper启动状态和执行各种zookeeper相关命令，输入quit退出：\n\n   ```shell\n   > ./bin/zookeeper-shell.sh localhost:2181\n   ```\n\n\n4. zookeeper启动完成后，启动kafka broker （-daemon 参数指定kafka进程后台运行）\n\n   ```shell\n   > ./bin/kafka-server-start.sh -daemon ./config/server.properties\n   ```\n\n   kafka运行日志存放于`安装路径/logs/server.log`中，可以查看运行状态和报错信息\n\n5. 启动完成后进入zookeeper控制台输入`ls /brokers/ids`命令即可查看注册的kafka broker信息\n\n\n### 基本操作\n\n再控制台中进行一些基本操作，包括创建topic，发布和消费数据\n\n#### 创建topic\n\n输入以下命令创建一个名为test_topic_1的topic：\n\n```shell\n> ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test_topic_1\n```\n\n每次执行kafka脚本都需要指定zookeeper；指定topic的`replication-factor`即复制集数量为1，分片数量`partitions`为1，名称为test_topic_1，然后通过以下命令查看当前所有topic：\n\n```shell\n> ./bin/kafka-topics.sh --list --zookeeper localhost:2181\n```\n\n执行以下命令查看某个topic的状态，包含分片信息，复制集信息，分片leader以及`Isr`：\n\n```shell\n> ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test_topic_1\n```\n\n需要注意的是， 创建topic时，`replication-factor`不能大于集群中broker的数量，因为每个partition的replication将会均匀分布到不同的broker上；以下是一个partition为2，replication为1的topic信息：\n\n![upload successful](\\blog\\images\\pasted-2.png)\n\n（`Isr`：repliaction副本存活列表，用于leader进行复制集数据同步，这个集合中的所有节点都是存活状态，并且跟leader同步，长时间未与leader进行同步的副本将被踢出该列表）\n\n#### 发布/订阅消息\n\n使用以下命令向`test_topic_1`进入producer控制台，发布消息：\n\n```shell\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic_1\n```\n\n另外再开一个session，使用以下命令进入consumer消费`test_topic_1`的消息：\n\n```shell\n./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_topic_1\n```","source":"_posts/kafka学习笔记（1）——-kafka基本特点以及与其他mq的对比-2.md","raw":"title: kafka学习笔记（1）—— 基础知识\nauthor: 天渊\ntags:\n  - Kafka\n  - 大数据\ncategories:\n  - 基础知识\ndate: 2019-02-13 16:31:00\n---\nKafka是由LinkedIn开发并开源的分布式发布/订阅模式的消息队列系统，因其分布式及高吞吐率而被广泛使用，目前在大数据处理领域占有很重要的地位，能够很方便地与Hadoop, Spark, Storm, Flink和Flume等大数据处理工具进行集成\n<!--more-->\n\n### Kafka主要特点\n\nKafka有三个主要的作用：`传统意义上的消息系统`，`分布式存储`，`流处理工具`\n\n**Kafka最重要的作用就是企业级消息队列服务**：\n\n- 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能\n- Server间的topic分区（partition）消费，保证了高吞吐率，同时保证每个分区内的消息顺序传输\n- 发布/订阅模式中，单个topic支持多consumer，并支持consumer-group自动负载均衡\n\n- 不同于RabbitMQ等传统消息队列，kafka不会删除历史数据\n- 支持数据备份（repliaction），通过master-slave方式保证数据一致性以及高可用\n\n**Kafka-stream**：\n\nKafka Stream是Apache Kafka从0.10版本引入的一个新Feature，它提供了对存储于Kafka内的数据进行流式处理和分析的功能。\n\n### Kafka架构\n\nKafka涉及到的一些专用名词\n\n- **Broker**\n  　　Kafka集群包含一个或多个服务器，这种服务器被称为broker\n- **Topic**\n  　　每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）\n- **Partition**\n  　　分片，每个Topic包含一个或多个Partition.\n- **repliaction**\n  　　复制集，每个partition包含一个或多个repliaction，其中有一个master多个slave\n- **Producer**\n  　　消息发布者，负责发布消息到Kafka broker\n- **Consumer**\n  　　消息消费者，通过拉取的方式向Kafka broker读取消息\n- **Consumer Group**\n  　　每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nkafka集群拓扑：\n\n![](/blog/images/kafka.png)\n- Kafka集群中包含若干Producer，使用push模式将消息**批量**发布到broker\n- 包含若干broker，支持水平扩展，broker数量越多，集群吞吐率越高，新建的topic-partition及其复制集均分到各个broker上\n- 由Consumer Group管理consumer实例，单个consumer实例可以订阅多个topic，若同一个Consumer Group中的多个consumer实例订阅了同一个topic，则由kafka集群自动进行负载均衡，统一协调分配partition进行消费\n- kafka强依赖Zookeeper集群，通过Zookeeper管理集群配置，保存元数据，选举partition leader，以及在Consumer Group发生变化时进行rebalance\n\n### Kafka与常用MQ对比\n\n- RabbitMQ：支持多种协议栈（AMQP，XMPP, SMTP等），功能更加强大（推拉消费，延迟消费，优先消费等），安全性和可靠性要优于kafka，相比较之下Kafka设计更加简单，吞吐量更高，更加适用于大规模日志数据处理\n- RocketMQ：阿里开源的消息队列，最早思路来源于kafka，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点，相比于kafka在可靠性和稳定性方面均有提升，而且支持事务消息\n- ZeroMQ：号称最快的消息队列系统，具有超高吞吐量，但作用场景有限，大部分情况下作为数据流传输模块嵌入到各个中间件中\n\n### Kafka文件存储方式\n\nKafka以顺序I/O的方式将消息存入磁盘进行持久化，保证了足够的刷盘速度:\n\n- Kafka存储的每条消息数据称为`Message`，每条`Message`数据包含四个属性：`offset`，`MessageSize`，`data`，`timestamp`时间戳即时间戳类型\n\n- Kafka的消息队列在逻辑上是由`partition`的方式存在的，每个`partition`的`repliaction`在物理上由多个`segment`分段组成，每个`segment`数据文件以该段中最小的 offset 命名，文件扩展名为`.log`，查找指定 offset 的 Message 的时候，使用二分查找定位到该 Message 在哪个 `segment` 数据文件；写入消息时直接将消息添加到最新`segment`文件的末尾\n\n- 每个`segment`分段都有自己的索引文件，扩展名为`.index`，索引文件采用稀疏索引的方式建立索引，每隔一定字节的数据建立一条索引，这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中\n\n![upload successful](\\blog\\images\\pasted-0.png)\n\n  `log.dirs=/home1/irteam/apps/kafka/data/kafka/kafka-logs`\n\n  在该文件夹中，每个topic的partition都有自己单独的文件夹进行存放，比如`resource-v1-APIGateway-Api`这个topic编号为0的partition放如下位置：\n\n  `/home1/irteam/apps/kafka/data/kafka/kafka-logs/resource-v1-APIGateway-Api-0/`\n  \n![upload successful](\\blog\\images\\pasted-1.png)\n  其中`.index`，`.log`，`.timestamp`三个文件分别对应`分段索引文件`，`分段数据`和`时间索引`\n    \n  **时间索引**：`.timeindex`文件用于保存当前分段中消息发布时间与offset的稀疏索引，用于定期删除消息（`log.retention.hours`参数）\n    \n    \n### Kafka搭建集群即基本操作\n\n1. 前期工作\n\n   首先保证当前主机安装有jdk，推荐jdk 8 及其以上版本\n\n   Kafka发行版自带zookeeper，无需单独安装zookeeper集群，当然也可以自己另外搭建zookeeper集群\n\n   最新版kafka发行版下载地址：\n\n   https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz\n\n   下载到本地并解压tar文件得到`kafka_2.11-2.1.0`文件夹 （2.11是kafka源码的scala版本，2.1.0是kafka实际发行版本）：\n   \n2. 初始化配置\n\n   为了简便起见这里就只配置单点broker；进入`config`文件夹，`server.properties`是kafka集群的主配置文件，大部分配置都可以选择默认，部分配置需要注意一下：\n\n   ```properties\n   # 当前主机id，也是集群中唯一id，将保存于zookeeper中\n   broker.id=0\n   # 监听地址和端口，如果没有配置的话默认当前主机名；端口默认9092\n   listeners = PLAINTEXT://your.host.name:9092\n   # 给生产者和消费者的广播地址，如果没设置的话默认采用上面的listeners属性\n   advertised.listeners=PLAINTEXT://your.host.name:9092\n   # kafka数据存放路径\n   log.dirs=/tmp/kafka-logs\n   # topic默认的分片数，创建topic的适合可以单独指定该属性\n   # 理论上，分区数量越多，吞吐量越大，但会造成更严重的资源消耗\n   num.partitions=1\n   # 是否允许自动创建topic（当producer或者consumer发布/订阅某个不存在的topic时）\n   auto.create.topics.enable=true\n   # 是否允许删除topic（删除topic后需要重启broker，不过即使这样也无法完全删除topic数据，需要进入zookeeper删除topic元数据，不过很危险，不推荐）\n   delete.topic.enable=true\n   # kafka采取异步刷盘的方式将内存中收到的消息序列化到硬盘上，下面两个条件任意满足一项即开启刷盘\n   # 每收到10000条消息刷盘一次\n   log.flush.interval.messages=10000\n   # 每隔1000ms刷盘一次\n   log.flush.interval.ms=1000\n   # 消息删除策略，保存消息的最长时间\n   log.retention.hours=168\n   # 单个分区保留消息的最大容量，默认就是1G\n   log.retention.bytes=1073741824\n   # segment分段的最大容量，单个segment超出这个容量后将创建一个新的segment继续保存消息\n   log.segment.bytes=1073741824\n   # 可接收的消息最大大小（压缩后的大小）\n   message.max.bytes=1048576\n   # zookeeper配置\n   zookeeper.connect=localhost:2181\n   zookeeper.connection.timeout.ms=6000\n   ```\n\n   关于kafka broker更详细的配置策略请参考官网：https://kafka.apache.org/documentation/#brokerconfigs\n\n3. 配置完成后，首先启动zookeeper，如果启动的是kafka默认自带的zookeeper的话，进入kafka安装目录，按照以下方式启动：\n\n   ```shell\n   > ./bin/zookeeper-server-start.sh ./config/zookeeper.properties\n   ```\n\n   再执行以下命令可以进入zookeeper，即可查看zookeeper启动状态和执行各种zookeeper相关命令，输入quit退出：\n\n   ```shell\n   > ./bin/zookeeper-shell.sh localhost:2181\n   ```\n\n\n4. zookeeper启动完成后，启动kafka broker （-daemon 参数指定kafka进程后台运行）\n\n   ```shell\n   > ./bin/kafka-server-start.sh -daemon ./config/server.properties\n   ```\n\n   kafka运行日志存放于`安装路径/logs/server.log`中，可以查看运行状态和报错信息\n\n5. 启动完成后进入zookeeper控制台输入`ls /brokers/ids`命令即可查看注册的kafka broker信息\n\n\n### 基本操作\n\n再控制台中进行一些基本操作，包括创建topic，发布和消费数据\n\n#### 创建topic\n\n输入以下命令创建一个名为test_topic_1的topic：\n\n```shell\n> ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test_topic_1\n```\n\n每次执行kafka脚本都需要指定zookeeper；指定topic的`replication-factor`即复制集数量为1，分片数量`partitions`为1，名称为test_topic_1，然后通过以下命令查看当前所有topic：\n\n```shell\n> ./bin/kafka-topics.sh --list --zookeeper localhost:2181\n```\n\n执行以下命令查看某个topic的状态，包含分片信息，复制集信息，分片leader以及`Isr`：\n\n```shell\n> ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test_topic_1\n```\n\n需要注意的是， 创建topic时，`replication-factor`不能大于集群中broker的数量，因为每个partition的replication将会均匀分布到不同的broker上；以下是一个partition为2，replication为1的topic信息：\n\n![upload successful](\\blog\\images\\pasted-2.png)\n\n（`Isr`：repliaction副本存活列表，用于leader进行复制集数据同步，这个集合中的所有节点都是存活状态，并且跟leader同步，长时间未与leader进行同步的副本将被踢出该列表）\n\n#### 发布/订阅消息\n\n使用以下命令向`test_topic_1`进入producer控制台，发布消息：\n\n```shell\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic_1\n```\n\n另外再开一个session，使用以下命令进入consumer消费`test_topic_1`的消息：\n\n```shell\n./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_topic_1\n```","slug":"kafka学习笔记（1）——-kafka基本特点以及与其他mq的对比-2","published":1,"updated":"2019-03-19T13:30:58.448Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js46001jg0qrrvptipqm","content":"<p>Kafka是由LinkedIn开发并开源的分布式发布/订阅模式的消息队列系统，因其分布式及高吞吐率而被广泛使用，目前在大数据处理领域占有很重要的地位，能够很方便地与Hadoop, Spark, Storm, Flink和Flume等大数据处理工具进行集成<br><a id=\"more\"></a></p>\n<h3 id=\"Kafka主要特点\"><a href=\"#Kafka主要特点\" class=\"headerlink\" title=\"Kafka主要特点\"></a>Kafka主要特点</h3><p>Kafka有三个主要的作用：<code>传统意义上的消息系统</code>，<code>分布式存储</code>，<code>流处理工具</code></p>\n<p><strong>Kafka最重要的作用就是企业级消息队列服务</strong>：</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能</li>\n<li>Server间的topic分区（partition）消费，保证了高吞吐率，同时保证每个分区内的消息顺序传输</li>\n<li><p>发布/订阅模式中，单个topic支持多consumer，并支持consumer-group自动负载均衡</p>\n</li>\n<li><p>不同于RabbitMQ等传统消息队列，kafka不会删除历史数据</p>\n</li>\n<li>支持数据备份（repliaction），通过master-slave方式保证数据一致性以及高可用</li>\n</ul>\n<p><strong>Kafka-stream</strong>：</p>\n<p>Kafka Stream是Apache Kafka从0.10版本引入的一个新Feature，它提供了对存储于Kafka内的数据进行流式处理和分析的功能。</p>\n<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><p>Kafka涉及到的一些专用名词</p>\n<ul>\n<li><strong>Broker</strong><br>　　Kafka集群包含一个或多个服务器，这种服务器被称为broker</li>\n<li><strong>Topic</strong><br>　　每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</li>\n<li><strong>Partition</strong><br>　　分片，每个Topic包含一个或多个Partition.</li>\n<li><strong>repliaction</strong><br>　　复制集，每个partition包含一个或多个repliaction，其中有一个master多个slave</li>\n<li><strong>Producer</strong><br>　　消息发布者，负责发布消息到Kafka broker</li>\n<li><strong>Consumer</strong><br>　　消息消费者，通过拉取的方式向Kafka broker读取消息</li>\n<li><strong>Consumer Group</strong><br>　　每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</li>\n</ul>\n<p>kafka集群拓扑：</p>\n<p><img src=\"/blog/images/kafka.png\" alt></p>\n<ul>\n<li>Kafka集群中包含若干Producer，使用push模式将消息<strong>批量</strong>发布到broker</li>\n<li>包含若干broker，支持水平扩展，broker数量越多，集群吞吐率越高，新建的topic-partition及其复制集均分到各个broker上</li>\n<li>由Consumer Group管理consumer实例，单个consumer实例可以订阅多个topic，若同一个Consumer Group中的多个consumer实例订阅了同一个topic，则由kafka集群自动进行负载均衡，统一协调分配partition进行消费</li>\n<li>kafka强依赖Zookeeper集群，通过Zookeeper管理集群配置，保存元数据，选举partition leader，以及在Consumer Group发生变化时进行rebalance</li>\n</ul>\n<h3 id=\"Kafka与常用MQ对比\"><a href=\"#Kafka与常用MQ对比\" class=\"headerlink\" title=\"Kafka与常用MQ对比\"></a>Kafka与常用MQ对比</h3><ul>\n<li>RabbitMQ：支持多种协议栈（AMQP，XMPP, SMTP等），功能更加强大（推拉消费，延迟消费，优先消费等），安全性和可靠性要优于kafka，相比较之下Kafka设计更加简单，吞吐量更高，更加适用于大规模日志数据处理</li>\n<li>RocketMQ：阿里开源的消息队列，最早思路来源于kafka，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点，相比于kafka在可靠性和稳定性方面均有提升，而且支持事务消息</li>\n<li>ZeroMQ：号称最快的消息队列系统，具有超高吞吐量，但作用场景有限，大部分情况下作为数据流传输模块嵌入到各个中间件中</li>\n</ul>\n<h3 id=\"Kafka文件存储方式\"><a href=\"#Kafka文件存储方式\" class=\"headerlink\" title=\"Kafka文件存储方式\"></a>Kafka文件存储方式</h3><p>Kafka以顺序I/O的方式将消息存入磁盘进行持久化，保证了足够的刷盘速度:</p>\n<ul>\n<li><p>Kafka存储的每条消息数据称为<code>Message</code>，每条<code>Message</code>数据包含四个属性：<code>offset</code>，<code>MessageSize</code>，<code>data</code>，<code>timestamp</code>时间戳即时间戳类型</p>\n</li>\n<li><p>Kafka的消息队列在逻辑上是由<code>partition</code>的方式存在的，每个<code>partition</code>的<code>repliaction</code>在物理上由多个<code>segment</code>分段组成，每个<code>segment</code>数据文件以该段中最小的 offset 命名，文件扩展名为<code>.log</code>，查找指定 offset 的 Message 的时候，使用二分查找定位到该 Message 在哪个 <code>segment</code> 数据文件；写入消息时直接将消息添加到最新<code>segment</code>文件的末尾</p>\n</li>\n<li><p>每个<code>segment</code>分段都有自己的索引文件，扩展名为<code>.index</code>，索引文件采用稀疏索引的方式建立索引，每隔一定字节的数据建立一条索引，这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中</p>\n</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-0.png\" alt=\"upload successful\"></p>\n<p>  <code>log.dirs=/home1/irteam/apps/kafka/data/kafka/kafka-logs</code></p>\n<p>  在该文件夹中，每个topic的partition都有自己单独的文件夹进行存放，比如<code>resource-v1-APIGateway-Api</code>这个topic编号为0的partition放如下位置：</p>\n<p>  <code>/home1/irteam/apps/kafka/data/kafka/kafka-logs/resource-v1-APIGateway-Api-0/</code></p>\n<p><img src=\"\\blog\\images\\pasted-1.png\" alt=\"upload successful\"><br>  其中<code>.index</code>，<code>.log</code>，<code>.timestamp</code>三个文件分别对应<code>分段索引文件</code>，<code>分段数据</code>和<code>时间索引</code></p>\n<p>  <strong>时间索引</strong>：<code>.timeindex</code>文件用于保存当前分段中消息发布时间与offset的稀疏索引，用于定期删除消息（<code>log.retention.hours</code>参数）</p>\n<h3 id=\"Kafka搭建集群即基本操作\"><a href=\"#Kafka搭建集群即基本操作\" class=\"headerlink\" title=\"Kafka搭建集群即基本操作\"></a>Kafka搭建集群即基本操作</h3><ol>\n<li><p>前期工作</p>\n<p>首先保证当前主机安装有jdk，推荐jdk 8 及其以上版本</p>\n<p>Kafka发行版自带zookeeper，无需单独安装zookeeper集群，当然也可以自己另外搭建zookeeper集群</p>\n<p>最新版kafka发行版下载地址：</p>\n<p><a href=\"https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz\" target=\"_blank\" rel=\"noopener\">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz</a></p>\n<p>下载到本地并解压tar文件得到<code>kafka_2.11-2.1.0</code>文件夹 （2.11是kafka源码的scala版本，2.1.0是kafka实际发行版本）：</p>\n</li>\n<li><p>初始化配置</p>\n<p>为了简便起见这里就只配置单点broker；进入<code>config</code>文件夹，<code>server.properties</code>是kafka集群的主配置文件，大部分配置都可以选择默认，部分配置需要注意一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 当前主机id，也是集群中唯一id，将保存于zookeeper中</span><br><span class=\"line\">broker.id=0</span><br><span class=\"line\"># 监听地址和端口，如果没有配置的话默认当前主机名；端口默认9092</span><br><span class=\"line\">listeners = PLAINTEXT://your.host.name:9092</span><br><span class=\"line\"># 给生产者和消费者的广播地址，如果没设置的话默认采用上面的listeners属性</span><br><span class=\"line\">advertised.listeners=PLAINTEXT://your.host.name:9092</span><br><span class=\"line\"># kafka数据存放路径</span><br><span class=\"line\">log.dirs=/tmp/kafka-logs</span><br><span class=\"line\"># topic默认的分片数，创建topic的适合可以单独指定该属性</span><br><span class=\"line\"># 理论上，分区数量越多，吞吐量越大，但会造成更严重的资源消耗</span><br><span class=\"line\">num.partitions=1</span><br><span class=\"line\"># 是否允许自动创建topic（当producer或者consumer发布/订阅某个不存在的topic时）</span><br><span class=\"line\">auto.create.topics.enable=true</span><br><span class=\"line\"># 是否允许删除topic（删除topic后需要重启broker，不过即使这样也无法完全删除topic数据，需要进入zookeeper删除topic元数据，不过很危险，不推荐）</span><br><span class=\"line\">delete.topic.enable=true</span><br><span class=\"line\"># kafka采取异步刷盘的方式将内存中收到的消息序列化到硬盘上，下面两个条件任意满足一项即开启刷盘</span><br><span class=\"line\"># 每收到10000条消息刷盘一次</span><br><span class=\"line\">log.flush.interval.messages=10000</span><br><span class=\"line\"># 每隔1000ms刷盘一次</span><br><span class=\"line\">log.flush.interval.ms=1000</span><br><span class=\"line\"># 消息删除策略，保存消息的最长时间</span><br><span class=\"line\">log.retention.hours=168</span><br><span class=\"line\"># 单个分区保留消息的最大容量，默认就是1G</span><br><span class=\"line\">log.retention.bytes=1073741824</span><br><span class=\"line\"># segment分段的最大容量，单个segment超出这个容量后将创建一个新的segment继续保存消息</span><br><span class=\"line\">log.segment.bytes=1073741824</span><br><span class=\"line\"># 可接收的消息最大大小（压缩后的大小）</span><br><span class=\"line\">message.max.bytes=1048576</span><br><span class=\"line\"># zookeeper配置</span><br><span class=\"line\">zookeeper.connect=localhost:2181</span><br><span class=\"line\">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure>\n<p>关于kafka broker更详细的配置策略请参考官网：<a href=\"https://kafka.apache.org/documentation/#brokerconfigs\" target=\"_blank\" rel=\"noopener\">https://kafka.apache.org/documentation/#brokerconfigs</a></p>\n</li>\n<li><p>配置完成后，首先启动zookeeper，如果启动的是kafka默认自带的zookeeper的话，进入kafka安装目录，按照以下方式启动：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/zookeeper-server-start.sh ./config/zookeeper.properties</span><br></pre></td></tr></table></figure>\n<p>再执行以下命令可以进入zookeeper，即可查看zookeeper启动状态和执行各种zookeeper相关命令，输入quit退出：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/zookeeper-shell.sh localhost:2181</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>zookeeper启动完成后，启动kafka broker （-daemon 参数指定kafka进程后台运行）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-server-start.sh -daemon ./config/server.properties</span><br></pre></td></tr></table></figure>\n<p>kafka运行日志存放于<code>安装路径/logs/server.log</code>中，可以查看运行状态和报错信息</p>\n</li>\n<li><p>启动完成后进入zookeeper控制台输入<code>ls /brokers/ids</code>命令即可查看注册的kafka broker信息</p>\n</li>\n</ol>\n<h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><p>再控制台中进行一些基本操作，包括创建topic，发布和消费数据</p>\n<h4 id=\"创建topic\"><a href=\"#创建topic\" class=\"headerlink\" title=\"创建topic\"></a>创建topic</h4><p>输入以下命令创建一个名为test_topic_1的topic：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>每次执行kafka脚本都需要指定zookeeper；指定topic的<code>replication-factor</code>即复制集数量为1，分片数量<code>partitions</code>为1，名称为test_topic_1，然后通过以下命令查看当前所有topic：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>\n<p>执行以下命令查看某个topic的状态，包含分片信息，复制集信息，分片leader以及<code>Isr</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>需要注意的是， 创建topic时，<code>replication-factor</code>不能大于集群中broker的数量，因为每个partition的replication将会均匀分布到不同的broker上；以下是一个partition为2，replication为1的topic信息：</p>\n<p><img src=\"\\blog\\images\\pasted-2.png\" alt=\"upload successful\"></p>\n<p>（<code>Isr</code>：repliaction副本存活列表，用于leader进行复制集数据同步，这个集合中的所有节点都是存活状态，并且跟leader同步，长时间未与leader进行同步的副本将被踢出该列表）</p>\n<h4 id=\"发布-订阅消息\"><a href=\"#发布-订阅消息\" class=\"headerlink\" title=\"发布/订阅消息\"></a>发布/订阅消息</h4><p>使用以下命令向<code>test_topic_1</code>进入producer控制台，发布消息：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>另外再开一个session，使用以下命令进入consumer消费<code>test_topic_1</code>的消息：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_topic_1</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>Kafka是由LinkedIn开发并开源的分布式发布/订阅模式的消息队列系统，因其分布式及高吞吐率而被广泛使用，目前在大数据处理领域占有很重要的地位，能够很方便地与Hadoop, Spark, Storm, Flink和Flume等大数据处理工具进行集成<br>","more":"</p>\n<h3 id=\"Kafka主要特点\"><a href=\"#Kafka主要特点\" class=\"headerlink\" title=\"Kafka主要特点\"></a>Kafka主要特点</h3><p>Kafka有三个主要的作用：<code>传统意义上的消息系统</code>，<code>分布式存储</code>，<code>流处理工具</code></p>\n<p><strong>Kafka最重要的作用就是企业级消息队列服务</strong>：</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能</li>\n<li>Server间的topic分区（partition）消费，保证了高吞吐率，同时保证每个分区内的消息顺序传输</li>\n<li><p>发布/订阅模式中，单个topic支持多consumer，并支持consumer-group自动负载均衡</p>\n</li>\n<li><p>不同于RabbitMQ等传统消息队列，kafka不会删除历史数据</p>\n</li>\n<li>支持数据备份（repliaction），通过master-slave方式保证数据一致性以及高可用</li>\n</ul>\n<p><strong>Kafka-stream</strong>：</p>\n<p>Kafka Stream是Apache Kafka从0.10版本引入的一个新Feature，它提供了对存储于Kafka内的数据进行流式处理和分析的功能。</p>\n<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><p>Kafka涉及到的一些专用名词</p>\n<ul>\n<li><strong>Broker</strong><br>　　Kafka集群包含一个或多个服务器，这种服务器被称为broker</li>\n<li><strong>Topic</strong><br>　　每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</li>\n<li><strong>Partition</strong><br>　　分片，每个Topic包含一个或多个Partition.</li>\n<li><strong>repliaction</strong><br>　　复制集，每个partition包含一个或多个repliaction，其中有一个master多个slave</li>\n<li><strong>Producer</strong><br>　　消息发布者，负责发布消息到Kafka broker</li>\n<li><strong>Consumer</strong><br>　　消息消费者，通过拉取的方式向Kafka broker读取消息</li>\n<li><strong>Consumer Group</strong><br>　　每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</li>\n</ul>\n<p>kafka集群拓扑：</p>\n<p><img src=\"/blog/images/kafka.png\" alt></p>\n<ul>\n<li>Kafka集群中包含若干Producer，使用push模式将消息<strong>批量</strong>发布到broker</li>\n<li>包含若干broker，支持水平扩展，broker数量越多，集群吞吐率越高，新建的topic-partition及其复制集均分到各个broker上</li>\n<li>由Consumer Group管理consumer实例，单个consumer实例可以订阅多个topic，若同一个Consumer Group中的多个consumer实例订阅了同一个topic，则由kafka集群自动进行负载均衡，统一协调分配partition进行消费</li>\n<li>kafka强依赖Zookeeper集群，通过Zookeeper管理集群配置，保存元数据，选举partition leader，以及在Consumer Group发生变化时进行rebalance</li>\n</ul>\n<h3 id=\"Kafka与常用MQ对比\"><a href=\"#Kafka与常用MQ对比\" class=\"headerlink\" title=\"Kafka与常用MQ对比\"></a>Kafka与常用MQ对比</h3><ul>\n<li>RabbitMQ：支持多种协议栈（AMQP，XMPP, SMTP等），功能更加强大（推拉消费，延迟消费，优先消费等），安全性和可靠性要优于kafka，相比较之下Kafka设计更加简单，吞吐量更高，更加适用于大规模日志数据处理</li>\n<li>RocketMQ：阿里开源的消息队列，最早思路来源于kafka，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点，相比于kafka在可靠性和稳定性方面均有提升，而且支持事务消息</li>\n<li>ZeroMQ：号称最快的消息队列系统，具有超高吞吐量，但作用场景有限，大部分情况下作为数据流传输模块嵌入到各个中间件中</li>\n</ul>\n<h3 id=\"Kafka文件存储方式\"><a href=\"#Kafka文件存储方式\" class=\"headerlink\" title=\"Kafka文件存储方式\"></a>Kafka文件存储方式</h3><p>Kafka以顺序I/O的方式将消息存入磁盘进行持久化，保证了足够的刷盘速度:</p>\n<ul>\n<li><p>Kafka存储的每条消息数据称为<code>Message</code>，每条<code>Message</code>数据包含四个属性：<code>offset</code>，<code>MessageSize</code>，<code>data</code>，<code>timestamp</code>时间戳即时间戳类型</p>\n</li>\n<li><p>Kafka的消息队列在逻辑上是由<code>partition</code>的方式存在的，每个<code>partition</code>的<code>repliaction</code>在物理上由多个<code>segment</code>分段组成，每个<code>segment</code>数据文件以该段中最小的 offset 命名，文件扩展名为<code>.log</code>，查找指定 offset 的 Message 的时候，使用二分查找定位到该 Message 在哪个 <code>segment</code> 数据文件；写入消息时直接将消息添加到最新<code>segment</code>文件的末尾</p>\n</li>\n<li><p>每个<code>segment</code>分段都有自己的索引文件，扩展名为<code>.index</code>，索引文件采用稀疏索引的方式建立索引，每隔一定字节的数据建立一条索引，这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中</p>\n</li>\n</ul>\n<p><img src=\"\\blog\\images\\pasted-0.png\" alt=\"upload successful\"></p>\n<p>  <code>log.dirs=/home1/irteam/apps/kafka/data/kafka/kafka-logs</code></p>\n<p>  在该文件夹中，每个topic的partition都有自己单独的文件夹进行存放，比如<code>resource-v1-APIGateway-Api</code>这个topic编号为0的partition放如下位置：</p>\n<p>  <code>/home1/irteam/apps/kafka/data/kafka/kafka-logs/resource-v1-APIGateway-Api-0/</code></p>\n<p><img src=\"\\blog\\images\\pasted-1.png\" alt=\"upload successful\"><br>  其中<code>.index</code>，<code>.log</code>，<code>.timestamp</code>三个文件分别对应<code>分段索引文件</code>，<code>分段数据</code>和<code>时间索引</code></p>\n<p>  <strong>时间索引</strong>：<code>.timeindex</code>文件用于保存当前分段中消息发布时间与offset的稀疏索引，用于定期删除消息（<code>log.retention.hours</code>参数）</p>\n<h3 id=\"Kafka搭建集群即基本操作\"><a href=\"#Kafka搭建集群即基本操作\" class=\"headerlink\" title=\"Kafka搭建集群即基本操作\"></a>Kafka搭建集群即基本操作</h3><ol>\n<li><p>前期工作</p>\n<p>首先保证当前主机安装有jdk，推荐jdk 8 及其以上版本</p>\n<p>Kafka发行版自带zookeeper，无需单独安装zookeeper集群，当然也可以自己另外搭建zookeeper集群</p>\n<p>最新版kafka发行版下载地址：</p>\n<p><a href=\"https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz\" target=\"_blank\" rel=\"noopener\">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz</a></p>\n<p>下载到本地并解压tar文件得到<code>kafka_2.11-2.1.0</code>文件夹 （2.11是kafka源码的scala版本，2.1.0是kafka实际发行版本）：</p>\n</li>\n<li><p>初始化配置</p>\n<p>为了简便起见这里就只配置单点broker；进入<code>config</code>文件夹，<code>server.properties</code>是kafka集群的主配置文件，大部分配置都可以选择默认，部分配置需要注意一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 当前主机id，也是集群中唯一id，将保存于zookeeper中</span><br><span class=\"line\">broker.id=0</span><br><span class=\"line\"># 监听地址和端口，如果没有配置的话默认当前主机名；端口默认9092</span><br><span class=\"line\">listeners = PLAINTEXT://your.host.name:9092</span><br><span class=\"line\"># 给生产者和消费者的广播地址，如果没设置的话默认采用上面的listeners属性</span><br><span class=\"line\">advertised.listeners=PLAINTEXT://your.host.name:9092</span><br><span class=\"line\"># kafka数据存放路径</span><br><span class=\"line\">log.dirs=/tmp/kafka-logs</span><br><span class=\"line\"># topic默认的分片数，创建topic的适合可以单独指定该属性</span><br><span class=\"line\"># 理论上，分区数量越多，吞吐量越大，但会造成更严重的资源消耗</span><br><span class=\"line\">num.partitions=1</span><br><span class=\"line\"># 是否允许自动创建topic（当producer或者consumer发布/订阅某个不存在的topic时）</span><br><span class=\"line\">auto.create.topics.enable=true</span><br><span class=\"line\"># 是否允许删除topic（删除topic后需要重启broker，不过即使这样也无法完全删除topic数据，需要进入zookeeper删除topic元数据，不过很危险，不推荐）</span><br><span class=\"line\">delete.topic.enable=true</span><br><span class=\"line\"># kafka采取异步刷盘的方式将内存中收到的消息序列化到硬盘上，下面两个条件任意满足一项即开启刷盘</span><br><span class=\"line\"># 每收到10000条消息刷盘一次</span><br><span class=\"line\">log.flush.interval.messages=10000</span><br><span class=\"line\"># 每隔1000ms刷盘一次</span><br><span class=\"line\">log.flush.interval.ms=1000</span><br><span class=\"line\"># 消息删除策略，保存消息的最长时间</span><br><span class=\"line\">log.retention.hours=168</span><br><span class=\"line\"># 单个分区保留消息的最大容量，默认就是1G</span><br><span class=\"line\">log.retention.bytes=1073741824</span><br><span class=\"line\"># segment分段的最大容量，单个segment超出这个容量后将创建一个新的segment继续保存消息</span><br><span class=\"line\">log.segment.bytes=1073741824</span><br><span class=\"line\"># 可接收的消息最大大小（压缩后的大小）</span><br><span class=\"line\">message.max.bytes=1048576</span><br><span class=\"line\"># zookeeper配置</span><br><span class=\"line\">zookeeper.connect=localhost:2181</span><br><span class=\"line\">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure>\n<p>关于kafka broker更详细的配置策略请参考官网：<a href=\"https://kafka.apache.org/documentation/#brokerconfigs\" target=\"_blank\" rel=\"noopener\">https://kafka.apache.org/documentation/#brokerconfigs</a></p>\n</li>\n<li><p>配置完成后，首先启动zookeeper，如果启动的是kafka默认自带的zookeeper的话，进入kafka安装目录，按照以下方式启动：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/zookeeper-server-start.sh ./config/zookeeper.properties</span><br></pre></td></tr></table></figure>\n<p>再执行以下命令可以进入zookeeper，即可查看zookeeper启动状态和执行各种zookeeper相关命令，输入quit退出：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/zookeeper-shell.sh localhost:2181</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>zookeeper启动完成后，启动kafka broker （-daemon 参数指定kafka进程后台运行）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-server-start.sh -daemon ./config/server.properties</span><br></pre></td></tr></table></figure>\n<p>kafka运行日志存放于<code>安装路径/logs/server.log</code>中，可以查看运行状态和报错信息</p>\n</li>\n<li><p>启动完成后进入zookeeper控制台输入<code>ls /brokers/ids</code>命令即可查看注册的kafka broker信息</p>\n</li>\n</ol>\n<h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><p>再控制台中进行一些基本操作，包括创建topic，发布和消费数据</p>\n<h4 id=\"创建topic\"><a href=\"#创建topic\" class=\"headerlink\" title=\"创建topic\"></a>创建topic</h4><p>输入以下命令创建一个名为test_topic_1的topic：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>每次执行kafka脚本都需要指定zookeeper；指定topic的<code>replication-factor</code>即复制集数量为1，分片数量<code>partitions</code>为1，名称为test_topic_1，然后通过以下命令查看当前所有topic：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>\n<p>执行以下命令查看某个topic的状态，包含分片信息，复制集信息，分片leader以及<code>Isr</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span> ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>需要注意的是， 创建topic时，<code>replication-factor</code>不能大于集群中broker的数量，因为每个partition的replication将会均匀分布到不同的broker上；以下是一个partition为2，replication为1的topic信息：</p>\n<p><img src=\"\\blog\\images\\pasted-2.png\" alt=\"upload successful\"></p>\n<p>（<code>Isr</code>：repliaction副本存活列表，用于leader进行复制集数据同步，这个集合中的所有节点都是存活状态，并且跟leader同步，长时间未与leader进行同步的副本将被踢出该列表）</p>\n<h4 id=\"发布-订阅消息\"><a href=\"#发布-订阅消息\" class=\"headerlink\" title=\"发布/订阅消息\"></a>发布/订阅消息</h4><p>使用以下命令向<code>test_topic_1</code>进入producer控制台，发布消息：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic_1</span><br></pre></td></tr></table></figure>\n<p>另外再开一个session，使用以下命令进入consumer消费<code>test_topic_1</code>的消息：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_topic_1</span><br></pre></td></tr></table></figure>"},{"title":"kafka学习笔记（2）—— 生产者 producer","author":"天渊","date":"2019-03-18T04:43:00.000Z","_content":"kafka作为大数据日志收集系统，能够接收来自多端的生产者数据，下图是消息经由`kafka-producer-api`向kafka集群发送消息的基本过程：\n<!--more-->\n\n![upload successful](\\blog\\images\\pasted-3.png)\n\n下面用kafka-producer的java api来进行说明\n\n\n### kafka-producer java api\n\n#### KafkaProducer基本操作\n\n项目中引入以下依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>2.1.1</version>\n</dependency>\n```\n\n最新版本的kafka-producer-api取消了同步发送消息的模式，全部默认采用异步发送消息，使用异步线程从发送队列中批量发送消息，然后返回一个`Future`对象\n\n首先创建producer并进行配置，以下三个配置项是必选配置，其他配置都是可选配置：\n\n```java\nMap<String, Object> producerConfig = new HashMap<>();\nproducerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\nproducerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nproducerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nProducer<String, String> producer = new KafkaProducer<>(producerConfig);\n```\n\n构建record并发送，发送成功后返回topic和partition的元数据：\n\n```java\nProducerRecord<String, String> record = new ProducerRecord<>(\"test-topic-1\", \"it's a msg\");\nFuture<RecordMetadata> future = producer.send(record);\nRecordMetadata recordMetadata = future.get();\nSystem.out.println(\"offset:\" + recordMetadata.offset());\nSystem.out.println(\"partition id: \" + recordMetadata.partition());\nSystem.out.println(\"topic: \" + recordMetadata.topic());\n```\n\n也可以设置回调函数，对Future进行消费：\n\n```java\nCountDownLatch latch = new CountDownLatch(1);\nproducer.send(record, (recordMetadata, e) -> {\n    System.out.println(\"offset:\" + recordMetadata.offset());\n    System.out.println(\"partition id: \" + recordMetadata.partition());\n    System.out.println(\"topic: \" + recordMetadata.topic());\n    if (e != null) {\n        e.printStackTrace();\n    }\n    latch.countDown();\n});\nlatch.await();\n```\n\n#### KafkaProducer配置解析\n\n除了`bootstrap.servers`, `key.serializer`,`value.serializer` 这三个配置项是必选配置，其他配置都是可选的：\n\n```properties\n# 指定目标分区有多少个副本成功收到消息时，producer才会收到消息发送成功的响应\n# 1：leader节点成功收到消息后即认为消息发送成功，一般采用这个\n# all：所有replica节点都成功收到消息后才认为消息发送成功\n# 0：producer无需等待任何发送成功的响应，消息发送完毕后即返回\nacks=1\n# 生产者缓冲区大小，消息发送到broker前可以在producer内存中进行缓冲，如果待发送的消息大小超过该值，后续发送请求则会阻塞\nbuffer.memory=33554432\n# 和上述配置协同工作，当缓冲区不足时后续请求能够阻塞的最大时间，超过该值仍然阻塞则会抛异常\nmax.block.ms=60000\n# 消息压缩方式\n# snappy：cpu消耗低，性能好； gzip：cpu消耗高，压缩比较snappy更高\ncompression.type=snappy\n# 消息发送失败后的重试次数（部分错误像“消息太大”之类的错误默认不重试直接报错）\nretries=3\n# 消息发送失败后每次重试的间隔时间\nretry.backoff.ms=100\n# 消息发送的单个batch大小，把多个消息合并为一个请求可以提高网络利用率，提高吞吐量，但也某种程度造成了消息延迟\nbatch.size=16384\n# 同样服务于消息batch，producer将等待直到消息填满一个batch或者达到linger时间后直接进行发送该批次消息\nlinger.ms=5\n# producer允许的未返回响应的最大请求个数，如果为1，则producer在未收到当前请求的响应前不会发送后续请求\n# 调高该值可以提高吞吐量，前提是对消息发送顺序没要求（如果开启retry的话有可能打乱消息发布的顺序）\nmax.in.flight.requests.per.connection=5\n# producer单次发送的最大容量，保证这个值不超过broker的message.max.bytes属性即可\nmax.request.size=1048576\n# 发布消息的请求响应超时时间，超出该值要么重试要么报错\nrequest.timeout.ms=30000\n```\n\n关于kafka-producer配置的一些问题如下：\n\n1. 如何保证消息发布顺序：如果同时配置了`retries`和`max.in.flight.requests.per.connection`，当后者大于1时，有可能造成消息发布乱序（比如，消息1发布失败，消息2发布成功，紧接着重试发送消息1并成功），所以官方建议如果配置了大于0的`retries`，`max.in.flight.requests.per.connection`最好设置为1\n\n2. 幂等消息：为了避免消息重复发布，支持单个producer对于同一个`Topic,Partition`的`Exactly Once`语义，kafka在`0.11.0.0`后引入了对幂等消息的支持，通过以下配置进行开启：\n\n   ```properties\n   # 开启幂等producer\n   enable.idempotence=true\n   # 如果开启幂等producer，必须对以下配置进行如下的设置\n   acks = all\n   retries = （大于0）\n   max.inflight.requests.per.connection = （小于等于5）\n   ```\n\n3. 事务支持：kafka同时在`0.11.0.0`版本引入了事务支持，支持跨partition幂等发布消息，保证了跨分区发布消息的原子性，通过以下配置进行开启：\n\n   ```properties\n   # 设置一个字符串表示事务id\n   # 开启事务后，enable.idempotence默认设置为true\n   transactional.id=my_tx_id_1\n   ```\n\n\n#### Kafka-producer序列化器\n\nKafka消息队列没有规定具体的消息传输协议和消息格式，队列中统一传输二进制数据流，因此需要用户根据自己消息的协议和格式选取合适的序列化器，或者自定义序列化器\n\nKafka默认提供的常用序列化器有有以下几种，基本上只能用于基本数据类型和字节数组或者ByteBuffer对象的序列化：\n\n```properties\nStringDeserializer\nIntegerSerializer\nByteArraySerializer\nByteBufferSerializer\nDoubleSerializer\nUUIDSerializer\n```\n\n用户自定义序列化器直接实现`Serializer`接口即可，如下实现一个简单的Json格式的序列化器：\n\n```java\npublic class SimpleJsonSerializer implements Serializer {\n\tprivate final Gson gson = new Gson();\n\t@Override\n\tpublic void configure(Map configs, boolean isKey) {\n\t}\n\t@Override\n\tpublic byte[] serialize(String topic, Object data) {\n\t\tString json = gson.toJson(data);\n\t\treturn json.getBytes(Charset.defaultCharset());\n\t}\n\t@Override\n\tpublic void close() {\n\t}\n}\n```\n\n然后在producer中进行配置，即可将java对象序列化为json字节数组了：\n\n```java\nMap<String, Object> producerConfig = new HashMap<>();\nproducerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\nproducerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nproducerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, SimpleJsonSerializer.class);\nproducerConfig.put(ProducerConfig.ACKS_CONFIG, \"1\");\nProducer<String, Person> producer = new KafkaProducer<>(producerConfig);\n\nProducerRecord<String, Person> record = new ProducerRecord<>(\"test-topic-1\", Person.builder().id(0).name(\"liugeng\").build());\n\n```\n\n#### kafka-producer分区器\n\nproducer发送消息时无需手动指定发送到某个partition，producer-api默认的`DefaultPartitioner`会根据消息中有无设置key来进行分区操作：\n\n1. key不为null：计算key的hash值，然后和partition数量取模，映射到不同的partition中\n2. key为null：使用Round-Robin进行轮询映射\n\n如果需要自己实现分区器（例如需要根据key指定特定的分区，或者某个key的消息需要占用多个分区），可以实现`Partitioner`接口，以下是一个例子，将key为`last`的record映射到最后一个partition上，剩下的record通过hash映射到其他partition：\n\n```java\npublic class SimpleCustomerPartitioner implements Partitioner {\n\t@Override\n\tpublic int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n\t\tList<PartitionInfo> partitionInfoList = cluster.availablePartitionsForTopic(topic);\n\t\tint partitionNum = partitionInfoList.size();\n\t\tif (keyBytes == null || !(key instanceof String)) {\n\t\t\tthrow new InvalidRecordException(\"the key is necessary !\");\n\t\t}\n\t\tif (\"last\".equals(key)) {\n\t\t\treturn partitionNum - 1;\n\t\t}\n\t\treturn Math.abs(Utils.murmur2(keyBytes)) % (partitionNum -1);\n\t}\n\t@Override\n\tpublic void close() {\n\t}\n\t@Override\n\tpublic void configure(Map<String, ?> configs) {\n\t}\n}\n```\n\n分区器的`partition`方法返回指定partition的id，需要注意的是partition的id是从0开始的\n","source":"_posts/kafka学习笔记（2）——-生产者-producer.md","raw":"title: kafka学习笔记（2）—— 生产者 producer\nauthor: 天渊\ntags:\n  - Kafka\n  - 大数据\ncategories:\n  - 基础知识\ndate: 2019-03-18 12:43:00\n---\nkafka作为大数据日志收集系统，能够接收来自多端的生产者数据，下图是消息经由`kafka-producer-api`向kafka集群发送消息的基本过程：\n<!--more-->\n\n![upload successful](\\blog\\images\\pasted-3.png)\n\n下面用kafka-producer的java api来进行说明\n\n\n### kafka-producer java api\n\n#### KafkaProducer基本操作\n\n项目中引入以下依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>2.1.1</version>\n</dependency>\n```\n\n最新版本的kafka-producer-api取消了同步发送消息的模式，全部默认采用异步发送消息，使用异步线程从发送队列中批量发送消息，然后返回一个`Future`对象\n\n首先创建producer并进行配置，以下三个配置项是必选配置，其他配置都是可选配置：\n\n```java\nMap<String, Object> producerConfig = new HashMap<>();\nproducerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\nproducerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nproducerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nProducer<String, String> producer = new KafkaProducer<>(producerConfig);\n```\n\n构建record并发送，发送成功后返回topic和partition的元数据：\n\n```java\nProducerRecord<String, String> record = new ProducerRecord<>(\"test-topic-1\", \"it's a msg\");\nFuture<RecordMetadata> future = producer.send(record);\nRecordMetadata recordMetadata = future.get();\nSystem.out.println(\"offset:\" + recordMetadata.offset());\nSystem.out.println(\"partition id: \" + recordMetadata.partition());\nSystem.out.println(\"topic: \" + recordMetadata.topic());\n```\n\n也可以设置回调函数，对Future进行消费：\n\n```java\nCountDownLatch latch = new CountDownLatch(1);\nproducer.send(record, (recordMetadata, e) -> {\n    System.out.println(\"offset:\" + recordMetadata.offset());\n    System.out.println(\"partition id: \" + recordMetadata.partition());\n    System.out.println(\"topic: \" + recordMetadata.topic());\n    if (e != null) {\n        e.printStackTrace();\n    }\n    latch.countDown();\n});\nlatch.await();\n```\n\n#### KafkaProducer配置解析\n\n除了`bootstrap.servers`, `key.serializer`,`value.serializer` 这三个配置项是必选配置，其他配置都是可选的：\n\n```properties\n# 指定目标分区有多少个副本成功收到消息时，producer才会收到消息发送成功的响应\n# 1：leader节点成功收到消息后即认为消息发送成功，一般采用这个\n# all：所有replica节点都成功收到消息后才认为消息发送成功\n# 0：producer无需等待任何发送成功的响应，消息发送完毕后即返回\nacks=1\n# 生产者缓冲区大小，消息发送到broker前可以在producer内存中进行缓冲，如果待发送的消息大小超过该值，后续发送请求则会阻塞\nbuffer.memory=33554432\n# 和上述配置协同工作，当缓冲区不足时后续请求能够阻塞的最大时间，超过该值仍然阻塞则会抛异常\nmax.block.ms=60000\n# 消息压缩方式\n# snappy：cpu消耗低，性能好； gzip：cpu消耗高，压缩比较snappy更高\ncompression.type=snappy\n# 消息发送失败后的重试次数（部分错误像“消息太大”之类的错误默认不重试直接报错）\nretries=3\n# 消息发送失败后每次重试的间隔时间\nretry.backoff.ms=100\n# 消息发送的单个batch大小，把多个消息合并为一个请求可以提高网络利用率，提高吞吐量，但也某种程度造成了消息延迟\nbatch.size=16384\n# 同样服务于消息batch，producer将等待直到消息填满一个batch或者达到linger时间后直接进行发送该批次消息\nlinger.ms=5\n# producer允许的未返回响应的最大请求个数，如果为1，则producer在未收到当前请求的响应前不会发送后续请求\n# 调高该值可以提高吞吐量，前提是对消息发送顺序没要求（如果开启retry的话有可能打乱消息发布的顺序）\nmax.in.flight.requests.per.connection=5\n# producer单次发送的最大容量，保证这个值不超过broker的message.max.bytes属性即可\nmax.request.size=1048576\n# 发布消息的请求响应超时时间，超出该值要么重试要么报错\nrequest.timeout.ms=30000\n```\n\n关于kafka-producer配置的一些问题如下：\n\n1. 如何保证消息发布顺序：如果同时配置了`retries`和`max.in.flight.requests.per.connection`，当后者大于1时，有可能造成消息发布乱序（比如，消息1发布失败，消息2发布成功，紧接着重试发送消息1并成功），所以官方建议如果配置了大于0的`retries`，`max.in.flight.requests.per.connection`最好设置为1\n\n2. 幂等消息：为了避免消息重复发布，支持单个producer对于同一个`Topic,Partition`的`Exactly Once`语义，kafka在`0.11.0.0`后引入了对幂等消息的支持，通过以下配置进行开启：\n\n   ```properties\n   # 开启幂等producer\n   enable.idempotence=true\n   # 如果开启幂等producer，必须对以下配置进行如下的设置\n   acks = all\n   retries = （大于0）\n   max.inflight.requests.per.connection = （小于等于5）\n   ```\n\n3. 事务支持：kafka同时在`0.11.0.0`版本引入了事务支持，支持跨partition幂等发布消息，保证了跨分区发布消息的原子性，通过以下配置进行开启：\n\n   ```properties\n   # 设置一个字符串表示事务id\n   # 开启事务后，enable.idempotence默认设置为true\n   transactional.id=my_tx_id_1\n   ```\n\n\n#### Kafka-producer序列化器\n\nKafka消息队列没有规定具体的消息传输协议和消息格式，队列中统一传输二进制数据流，因此需要用户根据自己消息的协议和格式选取合适的序列化器，或者自定义序列化器\n\nKafka默认提供的常用序列化器有有以下几种，基本上只能用于基本数据类型和字节数组或者ByteBuffer对象的序列化：\n\n```properties\nStringDeserializer\nIntegerSerializer\nByteArraySerializer\nByteBufferSerializer\nDoubleSerializer\nUUIDSerializer\n```\n\n用户自定义序列化器直接实现`Serializer`接口即可，如下实现一个简单的Json格式的序列化器：\n\n```java\npublic class SimpleJsonSerializer implements Serializer {\n\tprivate final Gson gson = new Gson();\n\t@Override\n\tpublic void configure(Map configs, boolean isKey) {\n\t}\n\t@Override\n\tpublic byte[] serialize(String topic, Object data) {\n\t\tString json = gson.toJson(data);\n\t\treturn json.getBytes(Charset.defaultCharset());\n\t}\n\t@Override\n\tpublic void close() {\n\t}\n}\n```\n\n然后在producer中进行配置，即可将java对象序列化为json字节数组了：\n\n```java\nMap<String, Object> producerConfig = new HashMap<>();\nproducerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"10.106.151.187:9092\");\nproducerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\nproducerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, SimpleJsonSerializer.class);\nproducerConfig.put(ProducerConfig.ACKS_CONFIG, \"1\");\nProducer<String, Person> producer = new KafkaProducer<>(producerConfig);\n\nProducerRecord<String, Person> record = new ProducerRecord<>(\"test-topic-1\", Person.builder().id(0).name(\"liugeng\").build());\n\n```\n\n#### kafka-producer分区器\n\nproducer发送消息时无需手动指定发送到某个partition，producer-api默认的`DefaultPartitioner`会根据消息中有无设置key来进行分区操作：\n\n1. key不为null：计算key的hash值，然后和partition数量取模，映射到不同的partition中\n2. key为null：使用Round-Robin进行轮询映射\n\n如果需要自己实现分区器（例如需要根据key指定特定的分区，或者某个key的消息需要占用多个分区），可以实现`Partitioner`接口，以下是一个例子，将key为`last`的record映射到最后一个partition上，剩下的record通过hash映射到其他partition：\n\n```java\npublic class SimpleCustomerPartitioner implements Partitioner {\n\t@Override\n\tpublic int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n\t\tList<PartitionInfo> partitionInfoList = cluster.availablePartitionsForTopic(topic);\n\t\tint partitionNum = partitionInfoList.size();\n\t\tif (keyBytes == null || !(key instanceof String)) {\n\t\t\tthrow new InvalidRecordException(\"the key is necessary !\");\n\t\t}\n\t\tif (\"last\".equals(key)) {\n\t\t\treturn partitionNum - 1;\n\t\t}\n\t\treturn Math.abs(Utils.murmur2(keyBytes)) % (partitionNum -1);\n\t}\n\t@Override\n\tpublic void close() {\n\t}\n\t@Override\n\tpublic void configure(Map<String, ?> configs) {\n\t}\n}\n```\n\n分区器的`partition`方法返回指定partition的id，需要注意的是partition的id是从0开始的\n","slug":"kafka学习笔记（2）——-生产者-producer","published":1,"updated":"2019-03-19T13:30:58.454Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js47001mg0qrd8g11oil","content":"<p>kafka作为大数据日志收集系统，能够接收来自多端的生产者数据，下图是消息经由<code>kafka-producer-api</code>向kafka集群发送消息的基本过程：<br><a id=\"more\"></a></p>\n<p><img src=\"\\blog\\images\\pasted-3.png\" alt=\"upload successful\"></p>\n<p>下面用kafka-producer的java api来进行说明</p>\n<h3 id=\"kafka-producer-java-api\"><a href=\"#kafka-producer-java-api\" class=\"headerlink\" title=\"kafka-producer java api\"></a>kafka-producer java api</h3><h4 id=\"KafkaProducer基本操作\"><a href=\"#KafkaProducer基本操作\" class=\"headerlink\" title=\"KafkaProducer基本操作\"></a>KafkaProducer基本操作</h4><p>项目中引入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.kafka<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>kafka-clients<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.1.1<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>最新版本的kafka-producer-api取消了同步发送消息的模式，全部默认采用异步发送消息，使用异步线程从发送队列中批量发送消息，然后返回一个<code>Future</code>对象</p>\n<p>首先创建producer并进行配置，以下三个配置项是必选配置，其他配置都是可选配置：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; producerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">Producer&lt;String, String&gt; producer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(producerConfig);</span><br></pre></td></tr></table></figure>\n<p>构建record并发送，发送成功后返回topic和partition的元数据：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ProducerRecord&lt;String, String&gt; record = <span class=\"keyword\">new</span> ProducerRecord&lt;&gt;(<span class=\"string\">\"test-topic-1\"</span>, <span class=\"string\">\"it's a msg\"</span>);</span><br><span class=\"line\">Future&lt;RecordMetadata&gt; future = producer.send(record);</span><br><span class=\"line\">RecordMetadata recordMetadata = future.get();</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"offset:\"</span> + recordMetadata.offset());</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"partition id: \"</span> + recordMetadata.partition());</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"topic: \"</span> + recordMetadata.topic());</span><br></pre></td></tr></table></figure>\n<p>也可以设置回调函数，对Future进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CountDownLatch latch = <span class=\"keyword\">new</span> CountDownLatch(<span class=\"number\">1</span>);</span><br><span class=\"line\">producer.send(record, (recordMetadata, e) -&gt; &#123;</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"offset:\"</span> + recordMetadata.offset());</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"partition id: \"</span> + recordMetadata.partition());</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"topic: \"</span> + recordMetadata.topic());</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (e != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    latch.countDown();</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\">latch.await();</span><br></pre></td></tr></table></figure>\n<h4 id=\"KafkaProducer配置解析\"><a href=\"#KafkaProducer配置解析\" class=\"headerlink\" title=\"KafkaProducer配置解析\"></a>KafkaProducer配置解析</h4><p>除了<code>bootstrap.servers</code>, <code>key.serializer</code>,<code>value.serializer</code> 这三个配置项是必选配置，其他配置都是可选的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 指定目标分区有多少个副本成功收到消息时，producer才会收到消息发送成功的响应</span><br><span class=\"line\"># 1：leader节点成功收到消息后即认为消息发送成功，一般采用这个</span><br><span class=\"line\"># all：所有replica节点都成功收到消息后才认为消息发送成功</span><br><span class=\"line\"># 0：producer无需等待任何发送成功的响应，消息发送完毕后即返回</span><br><span class=\"line\">acks=1</span><br><span class=\"line\"># 生产者缓冲区大小，消息发送到broker前可以在producer内存中进行缓冲，如果待发送的消息大小超过该值，后续发送请求则会阻塞</span><br><span class=\"line\">buffer.memory=33554432</span><br><span class=\"line\"># 和上述配置协同工作，当缓冲区不足时后续请求能够阻塞的最大时间，超过该值仍然阻塞则会抛异常</span><br><span class=\"line\">max.block.ms=60000</span><br><span class=\"line\"># 消息压缩方式</span><br><span class=\"line\"># snappy：cpu消耗低，性能好； gzip：cpu消耗高，压缩比较snappy更高</span><br><span class=\"line\">compression.type=snappy</span><br><span class=\"line\"># 消息发送失败后的重试次数（部分错误像“消息太大”之类的错误默认不重试直接报错）</span><br><span class=\"line\">retries=3</span><br><span class=\"line\"># 消息发送失败后每次重试的间隔时间</span><br><span class=\"line\">retry.backoff.ms=100</span><br><span class=\"line\"># 消息发送的单个batch大小，把多个消息合并为一个请求可以提高网络利用率，提高吞吐量，但也某种程度造成了消息延迟</span><br><span class=\"line\">batch.size=16384</span><br><span class=\"line\"># 同样服务于消息batch，producer将等待直到消息填满一个batch或者达到linger时间后直接进行发送该批次消息</span><br><span class=\"line\">linger.ms=5</span><br><span class=\"line\"># producer允许的未返回响应的最大请求个数，如果为1，则producer在未收到当前请求的响应前不会发送后续请求</span><br><span class=\"line\"># 调高该值可以提高吞吐量，前提是对消息发送顺序没要求（如果开启retry的话有可能打乱消息发布的顺序）</span><br><span class=\"line\">max.in.flight.requests.per.connection=5</span><br><span class=\"line\"># producer单次发送的最大容量，保证这个值不超过broker的message.max.bytes属性即可</span><br><span class=\"line\">max.request.size=1048576</span><br><span class=\"line\"># 发布消息的请求响应超时时间，超出该值要么重试要么报错</span><br><span class=\"line\">request.timeout.ms=30000</span><br></pre></td></tr></table></figure>\n<p>关于kafka-producer配置的一些问题如下：</p>\n<ol>\n<li><p>如何保证消息发布顺序：如果同时配置了<code>retries</code>和<code>max.in.flight.requests.per.connection</code>，当后者大于1时，有可能造成消息发布乱序（比如，消息1发布失败，消息2发布成功，紧接着重试发送消息1并成功），所以官方建议如果配置了大于0的<code>retries</code>，<code>max.in.flight.requests.per.connection</code>最好设置为1</p>\n</li>\n<li><p>幂等消息：为了避免消息重复发布，支持单个producer对于同一个<code>Topic,Partition</code>的<code>Exactly Once</code>语义，kafka在<code>0.11.0.0</code>后引入了对幂等消息的支持，通过以下配置进行开启：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 开启幂等producer</span><br><span class=\"line\">enable.idempotence=true</span><br><span class=\"line\"># 如果开启幂等producer，必须对以下配置进行如下的设置</span><br><span class=\"line\">acks = all</span><br><span class=\"line\">retries = （大于0）</span><br><span class=\"line\">max.inflight.requests.per.connection = （小于等于5）</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>事务支持：kafka同时在<code>0.11.0.0</code>版本引入了事务支持，支持跨partition幂等发布消息，保证了跨分区发布消息的原子性，通过以下配置进行开启：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置一个字符串表示事务id</span><br><span class=\"line\"># 开启事务后，enable.idempotence默认设置为true</span><br><span class=\"line\">transactional.id=my_tx_id_1</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h4 id=\"Kafka-producer序列化器\"><a href=\"#Kafka-producer序列化器\" class=\"headerlink\" title=\"Kafka-producer序列化器\"></a>Kafka-producer序列化器</h4><p>Kafka消息队列没有规定具体的消息传输协议和消息格式，队列中统一传输二进制数据流，因此需要用户根据自己消息的协议和格式选取合适的序列化器，或者自定义序列化器</p>\n<p>Kafka默认提供的常用序列化器有有以下几种，基本上只能用于基本数据类型和字节数组或者ByteBuffer对象的序列化：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StringDeserializer</span><br><span class=\"line\">IntegerSerializer</span><br><span class=\"line\">ByteArraySerializer</span><br><span class=\"line\">ByteBufferSerializer</span><br><span class=\"line\">DoubleSerializer</span><br><span class=\"line\">UUIDSerializer</span><br></pre></td></tr></table></figure>\n<p>用户自定义序列化器直接实现<code>Serializer</code>接口即可，如下实现一个简单的Json格式的序列化器：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleJsonSerializer</span> <span class=\"keyword\">implements</span> <span class=\"title\">Serializer</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Gson gson = <span class=\"keyword\">new</span> Gson();</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(Map configs, <span class=\"keyword\">boolean</span> isKey)</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">byte</span>[] serialize(String topic, Object data) &#123;</span><br><span class=\"line\">\t\tString json = gson.toJson(data);</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> json.getBytes(Charset.defaultCharset());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后在producer中进行配置，即可将java对象序列化为json字节数组了：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; producerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, SimpleJsonSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class=\"string\">\"1\"</span>);</span><br><span class=\"line\">Producer&lt;String, Person&gt; producer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(producerConfig);</span><br><span class=\"line\"></span><br><span class=\"line\">ProducerRecord&lt;String, Person&gt; record = <span class=\"keyword\">new</span> ProducerRecord&lt;&gt;(<span class=\"string\">\"test-topic-1\"</span>, Person.builder().id(<span class=\"number\">0</span>).name(<span class=\"string\">\"liugeng\"</span>).build());</span><br></pre></td></tr></table></figure>\n<h4 id=\"kafka-producer分区器\"><a href=\"#kafka-producer分区器\" class=\"headerlink\" title=\"kafka-producer分区器\"></a>kafka-producer分区器</h4><p>producer发送消息时无需手动指定发送到某个partition，producer-api默认的<code>DefaultPartitioner</code>会根据消息中有无设置key来进行分区操作：</p>\n<ol>\n<li>key不为null：计算key的hash值，然后和partition数量取模，映射到不同的partition中</li>\n<li>key为null：使用Round-Robin进行轮询映射</li>\n</ol>\n<p>如果需要自己实现分区器（例如需要根据key指定特定的分区，或者某个key的消息需要占用多个分区），可以实现<code>Partitioner</code>接口，以下是一个例子，将key为<code>last</code>的record映射到最后一个partition上，剩下的record通过hash映射到其他partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleCustomerPartitioner</span> <span class=\"keyword\">implements</span> <span class=\"title\">Partitioner</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(String topic, Object key, <span class=\"keyword\">byte</span>[] keyBytes, Object value, <span class=\"keyword\">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">\t\tList&lt;PartitionInfo&gt; partitionInfoList = cluster.availablePartitionsForTopic(topic);</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> partitionNum = partitionInfoList.size();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (keyBytes == <span class=\"keyword\">null</span> || !(key <span class=\"keyword\">instanceof</span> String)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InvalidRecordException(<span class=\"string\">\"the key is necessary !\"</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (<span class=\"string\">\"last\"</span>.equals(key)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> partitionNum - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> Math.abs(Utils.murmur2(keyBytes)) % (partitionNum -<span class=\"number\">1</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>分区器的<code>partition</code>方法返回指定partition的id，需要注意的是partition的id是从0开始的</p>\n","site":{"data":{}},"excerpt":"<p>kafka作为大数据日志收集系统，能够接收来自多端的生产者数据，下图是消息经由<code>kafka-producer-api</code>向kafka集群发送消息的基本过程：<br>","more":"</p>\n<p><img src=\"\\blog\\images\\pasted-3.png\" alt=\"upload successful\"></p>\n<p>下面用kafka-producer的java api来进行说明</p>\n<h3 id=\"kafka-producer-java-api\"><a href=\"#kafka-producer-java-api\" class=\"headerlink\" title=\"kafka-producer java api\"></a>kafka-producer java api</h3><h4 id=\"KafkaProducer基本操作\"><a href=\"#KafkaProducer基本操作\" class=\"headerlink\" title=\"KafkaProducer基本操作\"></a>KafkaProducer基本操作</h4><p>项目中引入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.kafka<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>kafka-clients<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.1.1<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>最新版本的kafka-producer-api取消了同步发送消息的模式，全部默认采用异步发送消息，使用异步线程从发送队列中批量发送消息，然后返回一个<code>Future</code>对象</p>\n<p>首先创建producer并进行配置，以下三个配置项是必选配置，其他配置都是可选配置：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; producerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">Producer&lt;String, String&gt; producer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(producerConfig);</span><br></pre></td></tr></table></figure>\n<p>构建record并发送，发送成功后返回topic和partition的元数据：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ProducerRecord&lt;String, String&gt; record = <span class=\"keyword\">new</span> ProducerRecord&lt;&gt;(<span class=\"string\">\"test-topic-1\"</span>, <span class=\"string\">\"it's a msg\"</span>);</span><br><span class=\"line\">Future&lt;RecordMetadata&gt; future = producer.send(record);</span><br><span class=\"line\">RecordMetadata recordMetadata = future.get();</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"offset:\"</span> + recordMetadata.offset());</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"partition id: \"</span> + recordMetadata.partition());</span><br><span class=\"line\">System.out.println(<span class=\"string\">\"topic: \"</span> + recordMetadata.topic());</span><br></pre></td></tr></table></figure>\n<p>也可以设置回调函数，对Future进行消费：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CountDownLatch latch = <span class=\"keyword\">new</span> CountDownLatch(<span class=\"number\">1</span>);</span><br><span class=\"line\">producer.send(record, (recordMetadata, e) -&gt; &#123;</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"offset:\"</span> + recordMetadata.offset());</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"partition id: \"</span> + recordMetadata.partition());</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"topic: \"</span> + recordMetadata.topic());</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (e != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    latch.countDown();</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\">latch.await();</span><br></pre></td></tr></table></figure>\n<h4 id=\"KafkaProducer配置解析\"><a href=\"#KafkaProducer配置解析\" class=\"headerlink\" title=\"KafkaProducer配置解析\"></a>KafkaProducer配置解析</h4><p>除了<code>bootstrap.servers</code>, <code>key.serializer</code>,<code>value.serializer</code> 这三个配置项是必选配置，其他配置都是可选的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 指定目标分区有多少个副本成功收到消息时，producer才会收到消息发送成功的响应</span><br><span class=\"line\"># 1：leader节点成功收到消息后即认为消息发送成功，一般采用这个</span><br><span class=\"line\"># all：所有replica节点都成功收到消息后才认为消息发送成功</span><br><span class=\"line\"># 0：producer无需等待任何发送成功的响应，消息发送完毕后即返回</span><br><span class=\"line\">acks=1</span><br><span class=\"line\"># 生产者缓冲区大小，消息发送到broker前可以在producer内存中进行缓冲，如果待发送的消息大小超过该值，后续发送请求则会阻塞</span><br><span class=\"line\">buffer.memory=33554432</span><br><span class=\"line\"># 和上述配置协同工作，当缓冲区不足时后续请求能够阻塞的最大时间，超过该值仍然阻塞则会抛异常</span><br><span class=\"line\">max.block.ms=60000</span><br><span class=\"line\"># 消息压缩方式</span><br><span class=\"line\"># snappy：cpu消耗低，性能好； gzip：cpu消耗高，压缩比较snappy更高</span><br><span class=\"line\">compression.type=snappy</span><br><span class=\"line\"># 消息发送失败后的重试次数（部分错误像“消息太大”之类的错误默认不重试直接报错）</span><br><span class=\"line\">retries=3</span><br><span class=\"line\"># 消息发送失败后每次重试的间隔时间</span><br><span class=\"line\">retry.backoff.ms=100</span><br><span class=\"line\"># 消息发送的单个batch大小，把多个消息合并为一个请求可以提高网络利用率，提高吞吐量，但也某种程度造成了消息延迟</span><br><span class=\"line\">batch.size=16384</span><br><span class=\"line\"># 同样服务于消息batch，producer将等待直到消息填满一个batch或者达到linger时间后直接进行发送该批次消息</span><br><span class=\"line\">linger.ms=5</span><br><span class=\"line\"># producer允许的未返回响应的最大请求个数，如果为1，则producer在未收到当前请求的响应前不会发送后续请求</span><br><span class=\"line\"># 调高该值可以提高吞吐量，前提是对消息发送顺序没要求（如果开启retry的话有可能打乱消息发布的顺序）</span><br><span class=\"line\">max.in.flight.requests.per.connection=5</span><br><span class=\"line\"># producer单次发送的最大容量，保证这个值不超过broker的message.max.bytes属性即可</span><br><span class=\"line\">max.request.size=1048576</span><br><span class=\"line\"># 发布消息的请求响应超时时间，超出该值要么重试要么报错</span><br><span class=\"line\">request.timeout.ms=30000</span><br></pre></td></tr></table></figure>\n<p>关于kafka-producer配置的一些问题如下：</p>\n<ol>\n<li><p>如何保证消息发布顺序：如果同时配置了<code>retries</code>和<code>max.in.flight.requests.per.connection</code>，当后者大于1时，有可能造成消息发布乱序（比如，消息1发布失败，消息2发布成功，紧接着重试发送消息1并成功），所以官方建议如果配置了大于0的<code>retries</code>，<code>max.in.flight.requests.per.connection</code>最好设置为1</p>\n</li>\n<li><p>幂等消息：为了避免消息重复发布，支持单个producer对于同一个<code>Topic,Partition</code>的<code>Exactly Once</code>语义，kafka在<code>0.11.0.0</code>后引入了对幂等消息的支持，通过以下配置进行开启：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 开启幂等producer</span><br><span class=\"line\">enable.idempotence=true</span><br><span class=\"line\"># 如果开启幂等producer，必须对以下配置进行如下的设置</span><br><span class=\"line\">acks = all</span><br><span class=\"line\">retries = （大于0）</span><br><span class=\"line\">max.inflight.requests.per.connection = （小于等于5）</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>事务支持：kafka同时在<code>0.11.0.0</code>版本引入了事务支持，支持跨partition幂等发布消息，保证了跨分区发布消息的原子性，通过以下配置进行开启：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置一个字符串表示事务id</span><br><span class=\"line\"># 开启事务后，enable.idempotence默认设置为true</span><br><span class=\"line\">transactional.id=my_tx_id_1</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h4 id=\"Kafka-producer序列化器\"><a href=\"#Kafka-producer序列化器\" class=\"headerlink\" title=\"Kafka-producer序列化器\"></a>Kafka-producer序列化器</h4><p>Kafka消息队列没有规定具体的消息传输协议和消息格式，队列中统一传输二进制数据流，因此需要用户根据自己消息的协议和格式选取合适的序列化器，或者自定义序列化器</p>\n<p>Kafka默认提供的常用序列化器有有以下几种，基本上只能用于基本数据类型和字节数组或者ByteBuffer对象的序列化：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StringDeserializer</span><br><span class=\"line\">IntegerSerializer</span><br><span class=\"line\">ByteArraySerializer</span><br><span class=\"line\">ByteBufferSerializer</span><br><span class=\"line\">DoubleSerializer</span><br><span class=\"line\">UUIDSerializer</span><br></pre></td></tr></table></figure>\n<p>用户自定义序列化器直接实现<code>Serializer</code>接口即可，如下实现一个简单的Json格式的序列化器：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleJsonSerializer</span> <span class=\"keyword\">implements</span> <span class=\"title\">Serializer</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Gson gson = <span class=\"keyword\">new</span> Gson();</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(Map configs, <span class=\"keyword\">boolean</span> isKey)</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">byte</span>[] serialize(String topic, Object data) &#123;</span><br><span class=\"line\">\t\tString json = gson.toJson(data);</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> json.getBytes(Charset.defaultCharset());</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后在producer中进行配置，即可将java对象序列化为json字节数组了：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, Object&gt; producerConfig = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"10.106.151.187:9092\"</span>);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, SimpleJsonSerializer.class);</span><br><span class=\"line\">producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class=\"string\">\"1\"</span>);</span><br><span class=\"line\">Producer&lt;String, Person&gt; producer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(producerConfig);</span><br><span class=\"line\"></span><br><span class=\"line\">ProducerRecord&lt;String, Person&gt; record = <span class=\"keyword\">new</span> ProducerRecord&lt;&gt;(<span class=\"string\">\"test-topic-1\"</span>, Person.builder().id(<span class=\"number\">0</span>).name(<span class=\"string\">\"liugeng\"</span>).build());</span><br></pre></td></tr></table></figure>\n<h4 id=\"kafka-producer分区器\"><a href=\"#kafka-producer分区器\" class=\"headerlink\" title=\"kafka-producer分区器\"></a>kafka-producer分区器</h4><p>producer发送消息时无需手动指定发送到某个partition，producer-api默认的<code>DefaultPartitioner</code>会根据消息中有无设置key来进行分区操作：</p>\n<ol>\n<li>key不为null：计算key的hash值，然后和partition数量取模，映射到不同的partition中</li>\n<li>key为null：使用Round-Robin进行轮询映射</li>\n</ol>\n<p>如果需要自己实现分区器（例如需要根据key指定特定的分区，或者某个key的消息需要占用多个分区），可以实现<code>Partitioner</code>接口，以下是一个例子，将key为<code>last</code>的record映射到最后一个partition上，剩下的record通过hash映射到其他partition：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleCustomerPartitioner</span> <span class=\"keyword\">implements</span> <span class=\"title\">Partitioner</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(String topic, Object key, <span class=\"keyword\">byte</span>[] keyBytes, Object value, <span class=\"keyword\">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">\t\tList&lt;PartitionInfo&gt; partitionInfoList = cluster.availablePartitionsForTopic(topic);</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> partitionNum = partitionInfoList.size();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (keyBytes == <span class=\"keyword\">null</span> || !(key <span class=\"keyword\">instanceof</span> String)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InvalidRecordException(<span class=\"string\">\"the key is necessary !\"</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (<span class=\"string\">\"last\"</span>.equals(key)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> partitionNum - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> Math.abs(Utils.murmur2(keyBytes)) % (partitionNum -<span class=\"number\">1</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>分区器的<code>partition</code>方法返回指定partition的id，需要注意的是partition的id是从0开始的</p>"},{"title":"如何理解3PC协议","author":"天渊","date":"2019-03-28T14:33:00.000Z","_content":"3PC协议是在2PC协议的基础上发展而来，全称是`Three-Phase Commit`，即三阶段提交\n\n<!--more-->\n\n### 3PC整体流程\n\n3PC将2PC第二阶段提交协议的\"提交事务请求\"一分为二，总共划分为`CanCommit`，`PreCommit`和`DoCommit`三个阶段：\n\n\n![upload successful](\\blog\\images\\3pc-1.png)\n\n![upload successful](\\blog\\images\\3pc-2.png)\n\n![upload successful](\\blog\\images\\3pc-3.png)\n\n##### 阶段一：CanCommit\n\n1. 事务询问：协调者向参与者发送包含事务内容的`CanCommit`请求，询问是否可以执行事务提交\n2. 反馈询问响应：参与者如果认为其自身可以顺利执行事务，则反馈`Yes`响应并进入预备状态，否则反馈`No`状态\n\n##### 阶段二：PreCommit\n\n如果协调者从所有参与者获得的反馈都是`Yes`，则执行事务预提交：\n\n1. 发送预提交请求：协调者向参与者发出`PreCommit`请求并进入`Prepare`阶段\n2. 事务预提交：参与者收到协调者的`PreCommit`请求后，执行事务操作并记录undo和redo日志，\n3. 反馈事务执行响应：参与者向协调者反馈事务的预提交结果，`Yes`或者`No`\n\n如果阶段一有参与者反馈`No`或者与协调者通讯超时，协调者则会发起事务中断：\n\n1. 发送`abort`请求：协调者向参与者发起`abort`请求，中断事务\n2. 中断事务：参与者收到`abort`请求，执行中断事务操作\n3. 阶段二中，如果参与者等待协调者的任何请求超时均会执行中断事务操作\n\n##### 阶段三：DoCommit\n\n如果上一阶段协调者从所有参与者那里都获得`Yes`反馈，则执行事务最终提交：\n\n1. 发送提交请求：协调者进入`Commit`状态，向所有参与者发起`DoCommit`请求\n2. 事务提交：参与者收到`DoCommit`请求，会执行事务提交操作，最后释放事务资源\n3. 反馈提交结果：参与者反馈事务提交结果，协调者收到所有反馈结果后完成事务\n\n如果阶段二有参与者反馈`No`或者通讯超时，协调者则会发起事务中断：\n\n1. 发送`abort`请求：协调者向参与者发起`abort`请求，中断事务向\n2. 事务回滚：参与者收到`abort`请求后，会利用undo log执行`RollBack`操作，并在回滚完成后释放资源\n3. 反馈回滚结果：参与者反馈回滚结果，协调者收到所有回滚结果后中断整个事务\n4. 阶段三中，如果参与者等待协调者的任何请求超时均会直接执行事务提交操作\n\n#### 结论\n\n3PC相比于2PC，最大优点降低了参与者的阻塞范围，每个参与者不需要等待全局响应完成就能够自己做出判断（中断 or 提交），并且即使在协调者出现单点故障后仍然能够尽最大可能保障数据一致\n\n**3PC的缺点**：3PC虽然提高了效率，但并没有解决完全保障数据强一致性的问题，那就是在`PreCommit`后，如果协调者和某个参与者无法正常通信，该参与者仍然会执行提交，若另外有参与者提交失败，则会造成数据不一致\n\n保障分布式系统数据一致性最终还得靠`Paxos`算法","source":"_posts/如何理解3PC协议-1.md","raw":"title: 如何理解3PC协议\nauthor: 天渊\ntags:\n  - 分布式理论\ncategories: []\ndate: 2019-03-28 22:33:00\n---\n3PC协议是在2PC协议的基础上发展而来，全称是`Three-Phase Commit`，即三阶段提交\n\n<!--more-->\n\n### 3PC整体流程\n\n3PC将2PC第二阶段提交协议的\"提交事务请求\"一分为二，总共划分为`CanCommit`，`PreCommit`和`DoCommit`三个阶段：\n\n\n![upload successful](\\blog\\images\\3pc-1.png)\n\n![upload successful](\\blog\\images\\3pc-2.png)\n\n![upload successful](\\blog\\images\\3pc-3.png)\n\n##### 阶段一：CanCommit\n\n1. 事务询问：协调者向参与者发送包含事务内容的`CanCommit`请求，询问是否可以执行事务提交\n2. 反馈询问响应：参与者如果认为其自身可以顺利执行事务，则反馈`Yes`响应并进入预备状态，否则反馈`No`状态\n\n##### 阶段二：PreCommit\n\n如果协调者从所有参与者获得的反馈都是`Yes`，则执行事务预提交：\n\n1. 发送预提交请求：协调者向参与者发出`PreCommit`请求并进入`Prepare`阶段\n2. 事务预提交：参与者收到协调者的`PreCommit`请求后，执行事务操作并记录undo和redo日志，\n3. 反馈事务执行响应：参与者向协调者反馈事务的预提交结果，`Yes`或者`No`\n\n如果阶段一有参与者反馈`No`或者与协调者通讯超时，协调者则会发起事务中断：\n\n1. 发送`abort`请求：协调者向参与者发起`abort`请求，中断事务\n2. 中断事务：参与者收到`abort`请求，执行中断事务操作\n3. 阶段二中，如果参与者等待协调者的任何请求超时均会执行中断事务操作\n\n##### 阶段三：DoCommit\n\n如果上一阶段协调者从所有参与者那里都获得`Yes`反馈，则执行事务最终提交：\n\n1. 发送提交请求：协调者进入`Commit`状态，向所有参与者发起`DoCommit`请求\n2. 事务提交：参与者收到`DoCommit`请求，会执行事务提交操作，最后释放事务资源\n3. 反馈提交结果：参与者反馈事务提交结果，协调者收到所有反馈结果后完成事务\n\n如果阶段二有参与者反馈`No`或者通讯超时，协调者则会发起事务中断：\n\n1. 发送`abort`请求：协调者向参与者发起`abort`请求，中断事务向\n2. 事务回滚：参与者收到`abort`请求后，会利用undo log执行`RollBack`操作，并在回滚完成后释放资源\n3. 反馈回滚结果：参与者反馈回滚结果，协调者收到所有回滚结果后中断整个事务\n4. 阶段三中，如果参与者等待协调者的任何请求超时均会直接执行事务提交操作\n\n#### 结论\n\n3PC相比于2PC，最大优点降低了参与者的阻塞范围，每个参与者不需要等待全局响应完成就能够自己做出判断（中断 or 提交），并且即使在协调者出现单点故障后仍然能够尽最大可能保障数据一致\n\n**3PC的缺点**：3PC虽然提高了效率，但并没有解决完全保障数据强一致性的问题，那就是在`PreCommit`后，如果协调者和某个参与者无法正常通信，该参与者仍然会执行提交，若另外有参与者提交失败，则会造成数据不一致\n\n保障分布式系统数据一致性最终还得靠`Paxos`算法","slug":"如何理解3PC协议-1","published":1,"updated":"2019-03-28T14:50:22.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js4y002mg0qrbjm25qcp","content":"<p>3PC协议是在2PC协议的基础上发展而来，全称是<code>Three-Phase Commit</code>，即三阶段提交</p>\n<a id=\"more\"></a>\n<h3 id=\"3PC整体流程\"><a href=\"#3PC整体流程\" class=\"headerlink\" title=\"3PC整体流程\"></a>3PC整体流程</h3><p>3PC将2PC第二阶段提交协议的”提交事务请求”一分为二，总共划分为<code>CanCommit</code>，<code>PreCommit</code>和<code>DoCommit</code>三个阶段：</p>\n<p><img src=\"\\blog\\images\\3pc-1.png\" alt=\"upload successful\"></p>\n<p><img src=\"\\blog\\images\\3pc-2.png\" alt=\"upload successful\"></p>\n<p><img src=\"\\blog\\images\\3pc-3.png\" alt=\"upload successful\"></p>\n<h5 id=\"阶段一：CanCommit\"><a href=\"#阶段一：CanCommit\" class=\"headerlink\" title=\"阶段一：CanCommit\"></a>阶段一：CanCommit</h5><ol>\n<li>事务询问：协调者向参与者发送包含事务内容的<code>CanCommit</code>请求，询问是否可以执行事务提交</li>\n<li>反馈询问响应：参与者如果认为其自身可以顺利执行事务，则反馈<code>Yes</code>响应并进入预备状态，否则反馈<code>No</code>状态</li>\n</ol>\n<h5 id=\"阶段二：PreCommit\"><a href=\"#阶段二：PreCommit\" class=\"headerlink\" title=\"阶段二：PreCommit\"></a>阶段二：PreCommit</h5><p>如果协调者从所有参与者获得的反馈都是<code>Yes</code>，则执行事务预提交：</p>\n<ol>\n<li>发送预提交请求：协调者向参与者发出<code>PreCommit</code>请求并进入<code>Prepare</code>阶段</li>\n<li>事务预提交：参与者收到协调者的<code>PreCommit</code>请求后，执行事务操作并记录undo和redo日志，</li>\n<li>反馈事务执行响应：参与者向协调者反馈事务的预提交结果，<code>Yes</code>或者<code>No</code></li>\n</ol>\n<p>如果阶段一有参与者反馈<code>No</code>或者与协调者通讯超时，协调者则会发起事务中断：</p>\n<ol>\n<li>发送<code>abort</code>请求：协调者向参与者发起<code>abort</code>请求，中断事务</li>\n<li>中断事务：参与者收到<code>abort</code>请求，执行中断事务操作</li>\n<li>阶段二中，如果参与者等待协调者的任何请求超时均会执行中断事务操作</li>\n</ol>\n<h5 id=\"阶段三：DoCommit\"><a href=\"#阶段三：DoCommit\" class=\"headerlink\" title=\"阶段三：DoCommit\"></a>阶段三：DoCommit</h5><p>如果上一阶段协调者从所有参与者那里都获得<code>Yes</code>反馈，则执行事务最终提交：</p>\n<ol>\n<li>发送提交请求：协调者进入<code>Commit</code>状态，向所有参与者发起<code>DoCommit</code>请求</li>\n<li>事务提交：参与者收到<code>DoCommit</code>请求，会执行事务提交操作，最后释放事务资源</li>\n<li>反馈提交结果：参与者反馈事务提交结果，协调者收到所有反馈结果后完成事务</li>\n</ol>\n<p>如果阶段二有参与者反馈<code>No</code>或者通讯超时，协调者则会发起事务中断：</p>\n<ol>\n<li>发送<code>abort</code>请求：协调者向参与者发起<code>abort</code>请求，中断事务向</li>\n<li>事务回滚：参与者收到<code>abort</code>请求后，会利用undo log执行<code>RollBack</code>操作，并在回滚完成后释放资源</li>\n<li>反馈回滚结果：参与者反馈回滚结果，协调者收到所有回滚结果后中断整个事务</li>\n<li>阶段三中，如果参与者等待协调者的任何请求超时均会直接执行事务提交操作</li>\n</ol>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><p>3PC相比于2PC，最大优点降低了参与者的阻塞范围，每个参与者不需要等待全局响应完成就能够自己做出判断（中断 or 提交），并且即使在协调者出现单点故障后仍然能够尽最大可能保障数据一致</p>\n<p><strong>3PC的缺点</strong>：3PC虽然提高了效率，但并没有解决完全保障数据强一致性的问题，那就是在<code>PreCommit</code>后，如果协调者和某个参与者无法正常通信，该参与者仍然会执行提交，若另外有参与者提交失败，则会造成数据不一致</p>\n<p>保障分布式系统数据一致性最终还得靠<code>Paxos</code>算法</p>\n","site":{"data":{}},"excerpt":"<p>3PC协议是在2PC协议的基础上发展而来，全称是<code>Three-Phase Commit</code>，即三阶段提交</p>","more":"<h3 id=\"3PC整体流程\"><a href=\"#3PC整体流程\" class=\"headerlink\" title=\"3PC整体流程\"></a>3PC整体流程</h3><p>3PC将2PC第二阶段提交协议的”提交事务请求”一分为二，总共划分为<code>CanCommit</code>，<code>PreCommit</code>和<code>DoCommit</code>三个阶段：</p>\n<p><img src=\"\\blog\\images\\3pc-1.png\" alt=\"upload successful\"></p>\n<p><img src=\"\\blog\\images\\3pc-2.png\" alt=\"upload successful\"></p>\n<p><img src=\"\\blog\\images\\3pc-3.png\" alt=\"upload successful\"></p>\n<h5 id=\"阶段一：CanCommit\"><a href=\"#阶段一：CanCommit\" class=\"headerlink\" title=\"阶段一：CanCommit\"></a>阶段一：CanCommit</h5><ol>\n<li>事务询问：协调者向参与者发送包含事务内容的<code>CanCommit</code>请求，询问是否可以执行事务提交</li>\n<li>反馈询问响应：参与者如果认为其自身可以顺利执行事务，则反馈<code>Yes</code>响应并进入预备状态，否则反馈<code>No</code>状态</li>\n</ol>\n<h5 id=\"阶段二：PreCommit\"><a href=\"#阶段二：PreCommit\" class=\"headerlink\" title=\"阶段二：PreCommit\"></a>阶段二：PreCommit</h5><p>如果协调者从所有参与者获得的反馈都是<code>Yes</code>，则执行事务预提交：</p>\n<ol>\n<li>发送预提交请求：协调者向参与者发出<code>PreCommit</code>请求并进入<code>Prepare</code>阶段</li>\n<li>事务预提交：参与者收到协调者的<code>PreCommit</code>请求后，执行事务操作并记录undo和redo日志，</li>\n<li>反馈事务执行响应：参与者向协调者反馈事务的预提交结果，<code>Yes</code>或者<code>No</code></li>\n</ol>\n<p>如果阶段一有参与者反馈<code>No</code>或者与协调者通讯超时，协调者则会发起事务中断：</p>\n<ol>\n<li>发送<code>abort</code>请求：协调者向参与者发起<code>abort</code>请求，中断事务</li>\n<li>中断事务：参与者收到<code>abort</code>请求，执行中断事务操作</li>\n<li>阶段二中，如果参与者等待协调者的任何请求超时均会执行中断事务操作</li>\n</ol>\n<h5 id=\"阶段三：DoCommit\"><a href=\"#阶段三：DoCommit\" class=\"headerlink\" title=\"阶段三：DoCommit\"></a>阶段三：DoCommit</h5><p>如果上一阶段协调者从所有参与者那里都获得<code>Yes</code>反馈，则执行事务最终提交：</p>\n<ol>\n<li>发送提交请求：协调者进入<code>Commit</code>状态，向所有参与者发起<code>DoCommit</code>请求</li>\n<li>事务提交：参与者收到<code>DoCommit</code>请求，会执行事务提交操作，最后释放事务资源</li>\n<li>反馈提交结果：参与者反馈事务提交结果，协调者收到所有反馈结果后完成事务</li>\n</ol>\n<p>如果阶段二有参与者反馈<code>No</code>或者通讯超时，协调者则会发起事务中断：</p>\n<ol>\n<li>发送<code>abort</code>请求：协调者向参与者发起<code>abort</code>请求，中断事务向</li>\n<li>事务回滚：参与者收到<code>abort</code>请求后，会利用undo log执行<code>RollBack</code>操作，并在回滚完成后释放资源</li>\n<li>反馈回滚结果：参与者反馈回滚结果，协调者收到所有回滚结果后中断整个事务</li>\n<li>阶段三中，如果参与者等待协调者的任何请求超时均会直接执行事务提交操作</li>\n</ol>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><p>3PC相比于2PC，最大优点降低了参与者的阻塞范围，每个参与者不需要等待全局响应完成就能够自己做出判断（中断 or 提交），并且即使在协调者出现单点故障后仍然能够尽最大可能保障数据一致</p>\n<p><strong>3PC的缺点</strong>：3PC虽然提高了效率，但并没有解决完全保障数据强一致性的问题，那就是在<code>PreCommit</code>后，如果协调者和某个参与者无法正常通信，该参与者仍然会执行提交，若另外有参与者提交失败，则会造成数据不一致</p>\n<p>保障分布式系统数据一致性最终还得靠<code>Paxos</code>算法</p>"},{"title":"自定义类加载器实践","author":"天渊","date":"2019-04-23T14:11:00.000Z","_content":"在进行java编程时，一般情况下不需要指定类加载器，jvm会根据需要加载的类自动选择合适的类加载器，不过在某些情况下就需要自定义类加载器实现对不同类别，不同来源或者不同版本的类分别进行加载，例如在tomcat中针对用户类库和tomcat自己的核心类库就实现了不同的类加载器\n\n<!-- more -->\n\n### 自定义ClassLoader\n\n#### 双亲委派\n\n实现自定义类加载器之前需要先了解`双亲委派`模式，jvm通过这个机制保证特定的类只能有特定的类加载器来加载\n\n每个类加载器都会指定一个父类加载器，收到加载任务后会提交给父类加载器进行加载，如果父类加载器加载不了则再交给当前类加载进行加载，目前默认的三种主要的类加载器如下：\n\n- `BootstrapClassLoader`：启动类加载器，复杂加载最为基础和重要的类，例如`$JAVA_HOME/jre/lib`目录下的类，以及虚拟机参数`Xbootclasspat`指定的类\n- `ExtClassLoader`：扩展类加载器，用于加载java类库中的扩展类库，即`$JAVA_HOME/jre/lib/ext`目录下的类，以及系统变量`java.ext.dirs`指定的类\n- `AppClassLoader`：应用类加载器，用于加载用户classpath下的所有类，用自己编写的类或者导入的第三方类库默认情况下都由应用类加载器进行加载\n\n在`ClassLoader`类（除BootstrapClassLoader外的所有类加载器均继承自这个类）中，`loadClass`方法的一部分源码如下：\n\n```java\nprotected Class<?> loadClass(String name, boolean resolve)\n    throws ClassNotFoundException {\n    synchronized (getClassLoadingLock(name)) {\n        // 检查这个类是不是已经被当前类加载器加载过了\n        Class<?> c = findLoadedClass(name);\n        if (c == null) {\n            long t0 = System.nanoTime();\n            try {\n                if (parent != null) {\n                    // 交给父加载器进行加载\n                    c = parent.loadClass(name, false);\n                } else {\n                    // 父加载器为空，则交给BootstrapClassLoader进行加载\n                    c = findBootstrapClassOrNull(name);\n                }\n            } catch (ClassNotFoundException e) {\n                // 父类加载器或者启动加载器都加载不了这个类\n            }\n            if (c == null) {               \n                long t1 = System.nanoTime();\n                // 调用findClass方法寻找需要加载的类\n                c = findClass(name);\n            }\n        }\n        // 如果需要解析，则对该类进行解析\n        if (resolve) {\n            resolveClass(c);\n        }\n        return c;\n    }\n}\n```\n\n可以看出，类加载器在拿到加载任务后，会交由父加载器进行加载，如果父加载器为null则直接交给启动类加载器进行加载（可以把启动类加载器看作所有加载器的父加载器），如果都加载不了，最后再由自己加载，调用`findClass`方法加载当前类，最后再判断是否需要解析\n\n在`ClassLoader`中默认的`findClass`方法默认抛出异常，需要子类自己去实现，如果要实现自己的类加载器就必须要重写`findClass`方法，如果要打破`双亲委派`模式（即加载类的时候不交给父加载器或者启动类加载器）的话，还需要额外重写`loadClass`方法\n\n##### 为何不能打破双亲委派模式\n\n在之前的例子中，自定义类加载器`SelfClassloader`没有重写`loadClass`方法，因此调用该方法加载类时会首先交给父加载器进行加载，由于父加载器为null，会交由`BootstrapClassLoader`进行加载\n\n在不打破双亲委派模式的情况下，用户无法自己额外伪造一个java核心类库中的类（例如`java.lang.String`）进行加载，因此保证了安全性\n\n##### 什么情况下需要打破双亲委派模式\n\n\n\n#### 重写findClass方法\n\n不打破双亲委派模式，将parent设置为null（此时父加载器为`BootstrapClassLoader`），创建自定义类加载器`SelfClassLoader`，重写`findClass`方法：\n\n```java\npublic class SelfClassLoader extends ClassLoader {\n\tprivate String root;\n    /**\n    * 指定这个类加载器可以加载的类的根路径\n    */\n\tpublic SelfClassLoader(String root) {\n\t\tsuper(null);\n\t\tthis.root = root;\n\t}\n\n\t@Override\n\tpublic Class<?> loadClass(String name) throws ClassNotFoundException {\n\t\treturn super.loadClass(name);\n\t}\n\n\t@Override\n\tprotected Class<?> findClass(String name) throws ClassNotFoundException {\n\t\tFile file;\n        // 将类名转换为path\n\t\tString path = root + name.replace('.', '/').concat(\".class\");\n\t\tfile = new File(path);\n\t\tif (!file.exists()) {\n\t\t\tthrow new ClassNotFoundException(name);\n\t\t}\n\t\tClass<?> clazz;\n\t\ttry {\n            // 读取字节码并调用defineClass方法加载类\n\t\t\tInputStream inputStream = new FileInputStream(file);\n\t\t\tbyte[] bytes = new byte[inputStream.available()];\n\t\t\tint result = inputStream.read(bytes);\n\t\t\tclazz = defineClass(null, bytes, 0, result);\n\t\t} catch (IOException e) {\n\t\t\tthrow new ClassNotFoundException(name, e);\n\t\t}\n\t\treturn clazz;\n\t}\n}\n```\n\n#### 加载指定类\n\n先创建一个类叫`People`，放到桌面的self_class文件夹中：\n\n```java\npublic class People {\n    public String name;\n    public int age;\n}\n```\n\n`SelfClassLoader`这个类加载器用于加载用户指定路径下的类，如下例子指定这个类加载器只能加载位于桌面self_class文件夹中的`.class`文件：\n\n```java\npublic static void main(String[] args) throws Exception {\n    // 指定根路径\n    SelfClassLoader selfClassLoader = new SelfClassLoader(\"C:\\\\Users\\\\admin\\\\Desktop\\\\self_class\\\\\");\n    // 加载该路径下的People这个类\n    Class<?> clazzCustomized = selfClassLoader.loadClass(\"People\");\n    Object instanceCustomized = clazzCustomized.newInstance();\n    Field field1 = clazzCustomized.getField(\"name\");\n    Field field2 = clazzCustomized.getField(\"age\");\n    field1.set(instanceCustomized, \"mike\");\n    field2.set(instanceCustomized, 18);\n    System.out.println(\"name is \" + field1.get(instanceCustomized));\n    System.out.println(\"age is \" + field2.get(instanceCustomized));\n}\n```\n\n#### 类隔离\n\n用自定义类加载器加载的类能够实现与系统默认类加载器（或者是其他的自定义类加载器）的隔离，现在来与系统默认类加载器加载的同一个类进行对比\n\n把People类拷贝一份放到项目classpath中：\n\n```java\n// 这个People类是AppClassLoader加载的\nClass<?> clazzDefault = Class.forName(\"People\");\nObject instanceDefault = clazzDefault.newInstance();\nSystem.out.println(clazzDefault.isInstance(instanceCustomized));\nSystem.out.println(clazzDefault.isInstance(instanceDefault));\nSystem.out.println(instanceCustomized instanceof People);\nSystem.out.println(instanceDefault instanceof People);\n```\n\n打印结果：\n\n> false\n\n> true\n\n> false\n\n> true\n\nclazzDefault由系统默认类加载器（也就是`AppClassLoader`）加载，与自定义类加载器加载的clazzCustomized很显然不是同一个类，这样就实现了不同来源类的隔离\n","source":"_posts/自定义类加载器实践.md","raw":"title: 自定义类加载器实践\nauthor: 天渊\ntags:\n  - java\n  - 类加载器\ncategories:\n  - 基础知识\ndate: 2019-04-23 22:11:00\n---\n在进行java编程时，一般情况下不需要指定类加载器，jvm会根据需要加载的类自动选择合适的类加载器，不过在某些情况下就需要自定义类加载器实现对不同类别，不同来源或者不同版本的类分别进行加载，例如在tomcat中针对用户类库和tomcat自己的核心类库就实现了不同的类加载器\n\n<!-- more -->\n\n### 自定义ClassLoader\n\n#### 双亲委派\n\n实现自定义类加载器之前需要先了解`双亲委派`模式，jvm通过这个机制保证特定的类只能有特定的类加载器来加载\n\n每个类加载器都会指定一个父类加载器，收到加载任务后会提交给父类加载器进行加载，如果父类加载器加载不了则再交给当前类加载进行加载，目前默认的三种主要的类加载器如下：\n\n- `BootstrapClassLoader`：启动类加载器，复杂加载最为基础和重要的类，例如`$JAVA_HOME/jre/lib`目录下的类，以及虚拟机参数`Xbootclasspat`指定的类\n- `ExtClassLoader`：扩展类加载器，用于加载java类库中的扩展类库，即`$JAVA_HOME/jre/lib/ext`目录下的类，以及系统变量`java.ext.dirs`指定的类\n- `AppClassLoader`：应用类加载器，用于加载用户classpath下的所有类，用自己编写的类或者导入的第三方类库默认情况下都由应用类加载器进行加载\n\n在`ClassLoader`类（除BootstrapClassLoader外的所有类加载器均继承自这个类）中，`loadClass`方法的一部分源码如下：\n\n```java\nprotected Class<?> loadClass(String name, boolean resolve)\n    throws ClassNotFoundException {\n    synchronized (getClassLoadingLock(name)) {\n        // 检查这个类是不是已经被当前类加载器加载过了\n        Class<?> c = findLoadedClass(name);\n        if (c == null) {\n            long t0 = System.nanoTime();\n            try {\n                if (parent != null) {\n                    // 交给父加载器进行加载\n                    c = parent.loadClass(name, false);\n                } else {\n                    // 父加载器为空，则交给BootstrapClassLoader进行加载\n                    c = findBootstrapClassOrNull(name);\n                }\n            } catch (ClassNotFoundException e) {\n                // 父类加载器或者启动加载器都加载不了这个类\n            }\n            if (c == null) {               \n                long t1 = System.nanoTime();\n                // 调用findClass方法寻找需要加载的类\n                c = findClass(name);\n            }\n        }\n        // 如果需要解析，则对该类进行解析\n        if (resolve) {\n            resolveClass(c);\n        }\n        return c;\n    }\n}\n```\n\n可以看出，类加载器在拿到加载任务后，会交由父加载器进行加载，如果父加载器为null则直接交给启动类加载器进行加载（可以把启动类加载器看作所有加载器的父加载器），如果都加载不了，最后再由自己加载，调用`findClass`方法加载当前类，最后再判断是否需要解析\n\n在`ClassLoader`中默认的`findClass`方法默认抛出异常，需要子类自己去实现，如果要实现自己的类加载器就必须要重写`findClass`方法，如果要打破`双亲委派`模式（即加载类的时候不交给父加载器或者启动类加载器）的话，还需要额外重写`loadClass`方法\n\n##### 为何不能打破双亲委派模式\n\n在之前的例子中，自定义类加载器`SelfClassloader`没有重写`loadClass`方法，因此调用该方法加载类时会首先交给父加载器进行加载，由于父加载器为null，会交由`BootstrapClassLoader`进行加载\n\n在不打破双亲委派模式的情况下，用户无法自己额外伪造一个java核心类库中的类（例如`java.lang.String`）进行加载，因此保证了安全性\n\n##### 什么情况下需要打破双亲委派模式\n\n\n\n#### 重写findClass方法\n\n不打破双亲委派模式，将parent设置为null（此时父加载器为`BootstrapClassLoader`），创建自定义类加载器`SelfClassLoader`，重写`findClass`方法：\n\n```java\npublic class SelfClassLoader extends ClassLoader {\n\tprivate String root;\n    /**\n    * 指定这个类加载器可以加载的类的根路径\n    */\n\tpublic SelfClassLoader(String root) {\n\t\tsuper(null);\n\t\tthis.root = root;\n\t}\n\n\t@Override\n\tpublic Class<?> loadClass(String name) throws ClassNotFoundException {\n\t\treturn super.loadClass(name);\n\t}\n\n\t@Override\n\tprotected Class<?> findClass(String name) throws ClassNotFoundException {\n\t\tFile file;\n        // 将类名转换为path\n\t\tString path = root + name.replace('.', '/').concat(\".class\");\n\t\tfile = new File(path);\n\t\tif (!file.exists()) {\n\t\t\tthrow new ClassNotFoundException(name);\n\t\t}\n\t\tClass<?> clazz;\n\t\ttry {\n            // 读取字节码并调用defineClass方法加载类\n\t\t\tInputStream inputStream = new FileInputStream(file);\n\t\t\tbyte[] bytes = new byte[inputStream.available()];\n\t\t\tint result = inputStream.read(bytes);\n\t\t\tclazz = defineClass(null, bytes, 0, result);\n\t\t} catch (IOException e) {\n\t\t\tthrow new ClassNotFoundException(name, e);\n\t\t}\n\t\treturn clazz;\n\t}\n}\n```\n\n#### 加载指定类\n\n先创建一个类叫`People`，放到桌面的self_class文件夹中：\n\n```java\npublic class People {\n    public String name;\n    public int age;\n}\n```\n\n`SelfClassLoader`这个类加载器用于加载用户指定路径下的类，如下例子指定这个类加载器只能加载位于桌面self_class文件夹中的`.class`文件：\n\n```java\npublic static void main(String[] args) throws Exception {\n    // 指定根路径\n    SelfClassLoader selfClassLoader = new SelfClassLoader(\"C:\\\\Users\\\\admin\\\\Desktop\\\\self_class\\\\\");\n    // 加载该路径下的People这个类\n    Class<?> clazzCustomized = selfClassLoader.loadClass(\"People\");\n    Object instanceCustomized = clazzCustomized.newInstance();\n    Field field1 = clazzCustomized.getField(\"name\");\n    Field field2 = clazzCustomized.getField(\"age\");\n    field1.set(instanceCustomized, \"mike\");\n    field2.set(instanceCustomized, 18);\n    System.out.println(\"name is \" + field1.get(instanceCustomized));\n    System.out.println(\"age is \" + field2.get(instanceCustomized));\n}\n```\n\n#### 类隔离\n\n用自定义类加载器加载的类能够实现与系统默认类加载器（或者是其他的自定义类加载器）的隔离，现在来与系统默认类加载器加载的同一个类进行对比\n\n把People类拷贝一份放到项目classpath中：\n\n```java\n// 这个People类是AppClassLoader加载的\nClass<?> clazzDefault = Class.forName(\"People\");\nObject instanceDefault = clazzDefault.newInstance();\nSystem.out.println(clazzDefault.isInstance(instanceCustomized));\nSystem.out.println(clazzDefault.isInstance(instanceDefault));\nSystem.out.println(instanceCustomized instanceof People);\nSystem.out.println(instanceDefault instanceof People);\n```\n\n打印结果：\n\n> false\n\n> true\n\n> false\n\n> true\n\nclazzDefault由系统默认类加载器（也就是`AppClassLoader`）加载，与自定义类加载器加载的clazzCustomized很显然不是同一个类，这样就实现了不同来源类的隔离\n","slug":"自定义类加载器实践","published":1,"updated":"2019-04-23T14:12:28.231Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js50002ng0qr03a81zcr","content":"<p>在进行java编程时，一般情况下不需要指定类加载器，jvm会根据需要加载的类自动选择合适的类加载器，不过在某些情况下就需要自定义类加载器实现对不同类别，不同来源或者不同版本的类分别进行加载，例如在tomcat中针对用户类库和tomcat自己的核心类库就实现了不同的类加载器</p>\n<a id=\"more\"></a>\n<h3 id=\"自定义ClassLoader\"><a href=\"#自定义ClassLoader\" class=\"headerlink\" title=\"自定义ClassLoader\"></a>自定义ClassLoader</h3><h4 id=\"双亲委派\"><a href=\"#双亲委派\" class=\"headerlink\" title=\"双亲委派\"></a>双亲委派</h4><p>实现自定义类加载器之前需要先了解<code>双亲委派</code>模式，jvm通过这个机制保证特定的类只能有特定的类加载器来加载</p>\n<p>每个类加载器都会指定一个父类加载器，收到加载任务后会提交给父类加载器进行加载，如果父类加载器加载不了则再交给当前类加载进行加载，目前默认的三种主要的类加载器如下：</p>\n<ul>\n<li><code>BootstrapClassLoader</code>：启动类加载器，复杂加载最为基础和重要的类，例如<code>$JAVA_HOME/jre/lib</code>目录下的类，以及虚拟机参数<code>Xbootclasspat</code>指定的类</li>\n<li><code>ExtClassLoader</code>：扩展类加载器，用于加载java类库中的扩展类库，即<code>$JAVA_HOME/jre/lib/ext</code>目录下的类，以及系统变量<code>java.ext.dirs</code>指定的类</li>\n<li><code>AppClassLoader</code>：应用类加载器，用于加载用户classpath下的所有类，用自己编写的类或者导入的第三方类库默认情况下都由应用类加载器进行加载</li>\n</ul>\n<p>在<code>ClassLoader</code>类（除BootstrapClassLoader外的所有类加载器均继承自这个类）中，<code>loadClass</code>方法的一部分源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> Class&lt;?&gt; loadClass(String name, <span class=\"keyword\">boolean</span> resolve)</span><br><span class=\"line\">    <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 检查这个类是不是已经被当前类加载器加载过了</span></span><br><span class=\"line\">        Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> t0 = System.nanoTime();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (parent != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 交给父加载器进行加载</span></span><br><span class=\"line\">                    c = parent.loadClass(name, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 父加载器为空，则交给BootstrapClassLoader进行加载</span></span><br><span class=\"line\">                    c = findBootstrapClassOrNull(name);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (ClassNotFoundException e) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 父类加载器或者启动加载器都加载不了这个类</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (c == <span class=\"keyword\">null</span>) &#123;               </span><br><span class=\"line\">                <span class=\"keyword\">long</span> t1 = System.nanoTime();</span><br><span class=\"line\">                <span class=\"comment\">// 调用findClass方法寻找需要加载的类</span></span><br><span class=\"line\">                c = findClass(name);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 如果需要解析，则对该类进行解析</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (resolve) &#123;</span><br><span class=\"line\">            resolveClass(c);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> c;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，类加载器在拿到加载任务后，会交由父加载器进行加载，如果父加载器为null则直接交给启动类加载器进行加载（可以把启动类加载器看作所有加载器的父加载器），如果都加载不了，最后再由自己加载，调用<code>findClass</code>方法加载当前类，最后再判断是否需要解析</p>\n<p>在<code>ClassLoader</code>中默认的<code>findClass</code>方法默认抛出异常，需要子类自己去实现，如果要实现自己的类加载器就必须要重写<code>findClass</code>方法，如果要打破<code>双亲委派</code>模式（即加载类的时候不交给父加载器或者启动类加载器）的话，还需要额外重写<code>loadClass</code>方法</p>\n<h5 id=\"为何不能打破双亲委派模式\"><a href=\"#为何不能打破双亲委派模式\" class=\"headerlink\" title=\"为何不能打破双亲委派模式\"></a>为何不能打破双亲委派模式</h5><p>在之前的例子中，自定义类加载器<code>SelfClassloader</code>没有重写<code>loadClass</code>方法，因此调用该方法加载类时会首先交给父加载器进行加载，由于父加载器为null，会交由<code>BootstrapClassLoader</code>进行加载</p>\n<p>在不打破双亲委派模式的情况下，用户无法自己额外伪造一个java核心类库中的类（例如<code>java.lang.String</code>）进行加载，因此保证了安全性</p>\n<h5 id=\"什么情况下需要打破双亲委派模式\"><a href=\"#什么情况下需要打破双亲委派模式\" class=\"headerlink\" title=\"什么情况下需要打破双亲委派模式\"></a>什么情况下需要打破双亲委派模式</h5><h4 id=\"重写findClass方法\"><a href=\"#重写findClass方法\" class=\"headerlink\" title=\"重写findClass方法\"></a>重写findClass方法</h4><p>不打破双亲委派模式，将parent设置为null（此时父加载器为<code>BootstrapClassLoader</code>），创建自定义类加载器<code>SelfClassLoader</code>，重写<code>findClass</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SelfClassLoader</span> <span class=\"keyword\">extends</span> <span class=\"title\">ClassLoader</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> String root;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * 指定这个类加载器可以加载的类的根路径</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SelfClassLoader</span><span class=\"params\">(String root)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">super</span>(<span class=\"keyword\">null</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.root = root;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> Class&lt;?&gt; loadClass(String name) <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.loadClass(name);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Class&lt;?&gt; findClass(String name) <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">\t\tFile file;</span><br><span class=\"line\">        <span class=\"comment\">// 将类名转换为path</span></span><br><span class=\"line\">\t\tString path = root + name.replace(<span class=\"string\">'.'</span>, <span class=\"string\">'/'</span>).concat(<span class=\"string\">\".class\"</span>);</span><br><span class=\"line\">\t\tfile = <span class=\"keyword\">new</span> File(path);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!file.exists()) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ClassNotFoundException(name);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tClass&lt;?&gt; clazz;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 读取字节码并调用defineClass方法加载类</span></span><br><span class=\"line\">\t\t\tInputStream inputStream = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">byte</span>[] bytes = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[inputStream.available()];</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> result = inputStream.read(bytes);</span><br><span class=\"line\">\t\t\tclazz = defineClass(<span class=\"keyword\">null</span>, bytes, <span class=\"number\">0</span>, result);</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ClassNotFoundException(name, e);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> clazz;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"加载指定类\"><a href=\"#加载指定类\" class=\"headerlink\" title=\"加载指定类\"></a>加载指定类</h4><p>先创建一个类叫<code>People</code>，放到桌面的self_class文件夹中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">People</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>SelfClassLoader</code>这个类加载器用于加载用户指定路径下的类，如下例子指定这个类加载器只能加载位于桌面self_class文件夹中的<code>.class</code>文件：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 指定根路径</span></span><br><span class=\"line\">    SelfClassLoader selfClassLoader = <span class=\"keyword\">new</span> SelfClassLoader(<span class=\"string\">\"C:\\\\Users\\\\admin\\\\Desktop\\\\self_class\\\\\"</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 加载该路径下的People这个类</span></span><br><span class=\"line\">    Class&lt;?&gt; clazzCustomized = selfClassLoader.loadClass(<span class=\"string\">\"People\"</span>);</span><br><span class=\"line\">    Object instanceCustomized = clazzCustomized.newInstance();</span><br><span class=\"line\">    Field field1 = clazzCustomized.getField(<span class=\"string\">\"name\"</span>);</span><br><span class=\"line\">    Field field2 = clazzCustomized.getField(<span class=\"string\">\"age\"</span>);</span><br><span class=\"line\">    field1.set(instanceCustomized, <span class=\"string\">\"mike\"</span>);</span><br><span class=\"line\">    field2.set(instanceCustomized, <span class=\"number\">18</span>);</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"name is \"</span> + field1.get(instanceCustomized));</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"age is \"</span> + field2.get(instanceCustomized));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"类隔离\"><a href=\"#类隔离\" class=\"headerlink\" title=\"类隔离\"></a>类隔离</h4><p>用自定义类加载器加载的类能够实现与系统默认类加载器（或者是其他的自定义类加载器）的隔离，现在来与系统默认类加载器加载的同一个类进行对比</p>\n<p>把People类拷贝一份放到项目classpath中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 这个People类是AppClassLoader加载的</span></span><br><span class=\"line\">Class&lt;?&gt; clazzDefault = Class.forName(<span class=\"string\">\"People\"</span>);</span><br><span class=\"line\">Object instanceDefault = clazzDefault.newInstance();</span><br><span class=\"line\">System.out.println(clazzDefault.isInstance(instanceCustomized));</span><br><span class=\"line\">System.out.println(clazzDefault.isInstance(instanceDefault));</span><br><span class=\"line\">System.out.println(instanceCustomized <span class=\"keyword\">instanceof</span> People);</span><br><span class=\"line\">System.out.println(instanceDefault <span class=\"keyword\">instanceof</span> People);</span><br></pre></td></tr></table></figure>\n<p>打印结果：</p>\n<blockquote>\n<p>false</p>\n</blockquote>\n<blockquote>\n<p>true</p>\n</blockquote>\n<blockquote>\n<p>false</p>\n</blockquote>\n<blockquote>\n<p>true</p>\n</blockquote>\n<p>clazzDefault由系统默认类加载器（也就是<code>AppClassLoader</code>）加载，与自定义类加载器加载的clazzCustomized很显然不是同一个类，这样就实现了不同来源类的隔离</p>\n","site":{"data":{}},"excerpt":"<p>在进行java编程时，一般情况下不需要指定类加载器，jvm会根据需要加载的类自动选择合适的类加载器，不过在某些情况下就需要自定义类加载器实现对不同类别，不同来源或者不同版本的类分别进行加载，例如在tomcat中针对用户类库和tomcat自己的核心类库就实现了不同的类加载器</p>","more":"<h3 id=\"自定义ClassLoader\"><a href=\"#自定义ClassLoader\" class=\"headerlink\" title=\"自定义ClassLoader\"></a>自定义ClassLoader</h3><h4 id=\"双亲委派\"><a href=\"#双亲委派\" class=\"headerlink\" title=\"双亲委派\"></a>双亲委派</h4><p>实现自定义类加载器之前需要先了解<code>双亲委派</code>模式，jvm通过这个机制保证特定的类只能有特定的类加载器来加载</p>\n<p>每个类加载器都会指定一个父类加载器，收到加载任务后会提交给父类加载器进行加载，如果父类加载器加载不了则再交给当前类加载进行加载，目前默认的三种主要的类加载器如下：</p>\n<ul>\n<li><code>BootstrapClassLoader</code>：启动类加载器，复杂加载最为基础和重要的类，例如<code>$JAVA_HOME/jre/lib</code>目录下的类，以及虚拟机参数<code>Xbootclasspat</code>指定的类</li>\n<li><code>ExtClassLoader</code>：扩展类加载器，用于加载java类库中的扩展类库，即<code>$JAVA_HOME/jre/lib/ext</code>目录下的类，以及系统变量<code>java.ext.dirs</code>指定的类</li>\n<li><code>AppClassLoader</code>：应用类加载器，用于加载用户classpath下的所有类，用自己编写的类或者导入的第三方类库默认情况下都由应用类加载器进行加载</li>\n</ul>\n<p>在<code>ClassLoader</code>类（除BootstrapClassLoader外的所有类加载器均继承自这个类）中，<code>loadClass</code>方法的一部分源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> Class&lt;?&gt; loadClass(String name, <span class=\"keyword\">boolean</span> resolve)</span><br><span class=\"line\">    <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 检查这个类是不是已经被当前类加载器加载过了</span></span><br><span class=\"line\">        Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> t0 = System.nanoTime();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (parent != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 交给父加载器进行加载</span></span><br><span class=\"line\">                    c = parent.loadClass(name, <span class=\"keyword\">false</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 父加载器为空，则交给BootstrapClassLoader进行加载</span></span><br><span class=\"line\">                    c = findBootstrapClassOrNull(name);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (ClassNotFoundException e) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 父类加载器或者启动加载器都加载不了这个类</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (c == <span class=\"keyword\">null</span>) &#123;               </span><br><span class=\"line\">                <span class=\"keyword\">long</span> t1 = System.nanoTime();</span><br><span class=\"line\">                <span class=\"comment\">// 调用findClass方法寻找需要加载的类</span></span><br><span class=\"line\">                c = findClass(name);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 如果需要解析，则对该类进行解析</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (resolve) &#123;</span><br><span class=\"line\">            resolveClass(c);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> c;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，类加载器在拿到加载任务后，会交由父加载器进行加载，如果父加载器为null则直接交给启动类加载器进行加载（可以把启动类加载器看作所有加载器的父加载器），如果都加载不了，最后再由自己加载，调用<code>findClass</code>方法加载当前类，最后再判断是否需要解析</p>\n<p>在<code>ClassLoader</code>中默认的<code>findClass</code>方法默认抛出异常，需要子类自己去实现，如果要实现自己的类加载器就必须要重写<code>findClass</code>方法，如果要打破<code>双亲委派</code>模式（即加载类的时候不交给父加载器或者启动类加载器）的话，还需要额外重写<code>loadClass</code>方法</p>\n<h5 id=\"为何不能打破双亲委派模式\"><a href=\"#为何不能打破双亲委派模式\" class=\"headerlink\" title=\"为何不能打破双亲委派模式\"></a>为何不能打破双亲委派模式</h5><p>在之前的例子中，自定义类加载器<code>SelfClassloader</code>没有重写<code>loadClass</code>方法，因此调用该方法加载类时会首先交给父加载器进行加载，由于父加载器为null，会交由<code>BootstrapClassLoader</code>进行加载</p>\n<p>在不打破双亲委派模式的情况下，用户无法自己额外伪造一个java核心类库中的类（例如<code>java.lang.String</code>）进行加载，因此保证了安全性</p>\n<h5 id=\"什么情况下需要打破双亲委派模式\"><a href=\"#什么情况下需要打破双亲委派模式\" class=\"headerlink\" title=\"什么情况下需要打破双亲委派模式\"></a>什么情况下需要打破双亲委派模式</h5><h4 id=\"重写findClass方法\"><a href=\"#重写findClass方法\" class=\"headerlink\" title=\"重写findClass方法\"></a>重写findClass方法</h4><p>不打破双亲委派模式，将parent设置为null（此时父加载器为<code>BootstrapClassLoader</code>），创建自定义类加载器<code>SelfClassLoader</code>，重写<code>findClass</code>方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SelfClassLoader</span> <span class=\"keyword\">extends</span> <span class=\"title\">ClassLoader</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> String root;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * 指定这个类加载器可以加载的类的根路径</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SelfClassLoader</span><span class=\"params\">(String root)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">super</span>(<span class=\"keyword\">null</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>.root = root;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> Class&lt;?&gt; loadClass(String name) <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.loadClass(name);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Class&lt;?&gt; findClass(String name) <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\">\t\tFile file;</span><br><span class=\"line\">        <span class=\"comment\">// 将类名转换为path</span></span><br><span class=\"line\">\t\tString path = root + name.replace(<span class=\"string\">'.'</span>, <span class=\"string\">'/'</span>).concat(<span class=\"string\">\".class\"</span>);</span><br><span class=\"line\">\t\tfile = <span class=\"keyword\">new</span> File(path);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!file.exists()) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ClassNotFoundException(name);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tClass&lt;?&gt; clazz;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 读取字节码并调用defineClass方法加载类</span></span><br><span class=\"line\">\t\t\tInputStream inputStream = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">byte</span>[] bytes = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[inputStream.available()];</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> result = inputStream.read(bytes);</span><br><span class=\"line\">\t\t\tclazz = defineClass(<span class=\"keyword\">null</span>, bytes, <span class=\"number\">0</span>, result);</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ClassNotFoundException(name, e);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> clazz;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"加载指定类\"><a href=\"#加载指定类\" class=\"headerlink\" title=\"加载指定类\"></a>加载指定类</h4><p>先创建一个类叫<code>People</code>，放到桌面的self_class文件夹中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">People</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>SelfClassLoader</code>这个类加载器用于加载用户指定路径下的类，如下例子指定这个类加载器只能加载位于桌面self_class文件夹中的<code>.class</code>文件：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 指定根路径</span></span><br><span class=\"line\">    SelfClassLoader selfClassLoader = <span class=\"keyword\">new</span> SelfClassLoader(<span class=\"string\">\"C:\\\\Users\\\\admin\\\\Desktop\\\\self_class\\\\\"</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 加载该路径下的People这个类</span></span><br><span class=\"line\">    Class&lt;?&gt; clazzCustomized = selfClassLoader.loadClass(<span class=\"string\">\"People\"</span>);</span><br><span class=\"line\">    Object instanceCustomized = clazzCustomized.newInstance();</span><br><span class=\"line\">    Field field1 = clazzCustomized.getField(<span class=\"string\">\"name\"</span>);</span><br><span class=\"line\">    Field field2 = clazzCustomized.getField(<span class=\"string\">\"age\"</span>);</span><br><span class=\"line\">    field1.set(instanceCustomized, <span class=\"string\">\"mike\"</span>);</span><br><span class=\"line\">    field2.set(instanceCustomized, <span class=\"number\">18</span>);</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"name is \"</span> + field1.get(instanceCustomized));</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"age is \"</span> + field2.get(instanceCustomized));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"类隔离\"><a href=\"#类隔离\" class=\"headerlink\" title=\"类隔离\"></a>类隔离</h4><p>用自定义类加载器加载的类能够实现与系统默认类加载器（或者是其他的自定义类加载器）的隔离，现在来与系统默认类加载器加载的同一个类进行对比</p>\n<p>把People类拷贝一份放到项目classpath中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 这个People类是AppClassLoader加载的</span></span><br><span class=\"line\">Class&lt;?&gt; clazzDefault = Class.forName(<span class=\"string\">\"People\"</span>);</span><br><span class=\"line\">Object instanceDefault = clazzDefault.newInstance();</span><br><span class=\"line\">System.out.println(clazzDefault.isInstance(instanceCustomized));</span><br><span class=\"line\">System.out.println(clazzDefault.isInstance(instanceDefault));</span><br><span class=\"line\">System.out.println(instanceCustomized <span class=\"keyword\">instanceof</span> People);</span><br><span class=\"line\">System.out.println(instanceDefault <span class=\"keyword\">instanceof</span> People);</span><br></pre></td></tr></table></figure>\n<p>打印结果：</p>\n<blockquote>\n<p>false</p>\n</blockquote>\n<blockquote>\n<p>true</p>\n</blockquote>\n<blockquote>\n<p>false</p>\n</blockquote>\n<blockquote>\n<p>true</p>\n</blockquote>\n<p>clazzDefault由系统默认类加载器（也就是<code>AppClassLoader</code>）加载，与自定义类加载器加载的clazzCustomized很显然不是同一个类，这样就实现了不同来源类的隔离</p>"},{"title":"Yarn任务调度机制探析","author":"天渊","date":"2019-06-04T12:54:00.000Z","_content":"yarn作为hadoop任务调度组件，具有良好的可扩展性、高可用性以及其独特的“多租户”特性，它为不同的作业场景提供了几种特性各异的任务调度机制：`FIFO调度器`，`Capacity调度器`，`Fair调度器`\n<!-- more -->\n\n### FIFO调度器\n\nFIFO调度器采用一个先进先出队列对提交的任务执行顺序进行调度，按照提交的顺序首先为第一个任务的请求分配资源，第一个应用的请求被满足后，再依次为队列中下一个应用分配资源\n\nFIFO调度器的好处是实现简单，无需任务额外配置，但是缺点也很明显，不适合那种大型任务和小型任务穿插执行的共享集群，因为大型任务可能会独占集群中的全部计算资源，并且任务执行时间会很长，yarn短时间为无法为其他任务分配资源，因此只能阻塞在队列中等待大型任务执行完成释放资源：\n\n![1559481447073](\\blog\\images\\1559481447073.png)\n\n如图，横轴表示集群资源利用情况，当job1执行时，由于集群资源有限，job2必须等待job1执行完成后才能执行\n\n### Capacity调度器\n\n容量调度器可以为不同体量的任务提供一个或多个专用的等待队列，保证小型任务一旦提交就可以分配资源启动执行，因此解决了FIFO调度无法兼顾大型任务和小型任务的问题，大型任务的执行不会造成小型任务的长时间等待\n\n不过容量调度器也有自己的缺点，由于yarn要专门为小型任务预留一部分集群资源，分配给大型任务的资源就会相应减少，执行时间也就变长了：\n\n![1559482380158](\\blog\\images\\1559482380158.png)\n\n如图，queue B配置为小型任务服务，分配的资源较少，queue A配置为大型任务服务，分配的集群资源更多，保证大型任务和小型任务能够在集群中共存而不会相互阻塞\n\n配置容量调度器时，可以根据实际需要配置多个队列，每个队列分配不同数额的集群资源，不过如果某个队列的任务在执行过程中分配的集群资源不够用，为了不让该任务等待其他队列释放资源，需要为队列设置`maximun-capacity`，能够在资源不够用时进行动态扩容，如果集群中有空闲资源，则会为这个队列分配更多的资源，这种方式称为`队列弹性`，扩容后的资源总量保证不超过`maximun-capacity`即可\n\n提交map-reduce任务时，通过指定`mapreduce.job.<queue-name>`来指定当前任务分配给哪一个队列\n\n### Fair调度器\n\n即公平调度器，目的是为所有运行的任务公平分配集群资源，在容量调度器的基础上进行了改进，能够在不同任务之间**动态**地调度集群资源：\n\n![1559568139040](\\blog\\images\\1559568139040.png)\n\n如图，在公平调度器模式下，与容量调度类似，根据实际需要分配多个队列用于执行任务：\n\n1. job1率先提交，当前集群中没有其他任务共享资源，因为job1独享集群中queue A和queue B的全部资源\n\n2. job1执行过程中，job2提交，此时job1享有queue A为其分配的资源，而job2享有queue B为其分配的资源，job1和job2共享集群资源\n\n3. job2独享queue B的资源时，job3同样提交到queue B中执行，此时job2和job3共享queue B的资源\n\n4. 待job2执行完成后，job3独享queue B的资源\n\n可以看出，相比于容量调度器，公平调度模式下几乎不会出现饥饿情况（即有任务长期无法分配到集群资源而长时间处于阻塞状态），在满足一定的分配权重和调度策略的情况下，每个任务都能分享到一定数量的集群资源\n\nhadoop默认使用容量调度器，如果要在yarn中启用公平调度器，需要在`yarn-site.xml`作以下配置\n\n```xml\n<property>\n  <name>yarn.resourcemanager.scheduler.class</name>\n  <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n```\n\n#### Fair调度器的队列放置策略\n\n与容量调度器相似，公平调度器也可以执行某个任务提交到特定的队列中执行，也可以执行任务放置到以任务提交的用户名为队列名的队列下进行执行\n\n在yarn-site.xml中配置`yarn.scheduler.fair.allocation.file`执行队列分配文件，在队列分配文件中制定任务的队列分配策略(来自[hadoop官网yarn-FairScheduler文档](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html))：\n\n```xml\n<?xml version=\"1.0\"?>\n<allocations>\n    <queue name=\"sample_queue\">\n        <minResources>10000 mb,0vcores</minResources>\n        <maxResources>90000 mb,0vcores</maxResources>\n        <maxRunningApps>50</maxRunningApps>\n        <maxAMShare>0.1</maxAMShare>\n        <weight>2.0</weight>\n        <schedulingPolicy>fair</schedulingPolicy>\n        <queue name=\"sample_sub_queue\">\n            <aclSubmitApps>charlie</aclSubmitApps>\n            <minResources>5000 mb,0vcores</minResources>\n        </queue>\n        <queue name=\"sample_reservable_queue\">\n            <reservation></reservation>\n        </queue>\n    </queue>\n\n    <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>\n    <queueMaxResourcesDefault>40000 mb,0vcores</queueMaxResourcesDefault>\n\n    <queue name=\"secondary_group_queue\" type=\"parent\">\n        <weight>3.0</weight>\n        <maxChildResources>4096 mb,4vcores</maxChildResources>\n    </queue>\n\n    <user name=\"sample_user\">\n        <maxRunningApps>30</maxRunningApps>\n    </user>\n    <userMaxAppsDefault>5</userMaxAppsDefault>\n\n    <queuePlacementPolicy>\n        <rule name=\"specified\" />\n        <rule name=\"primaryGroup\" create=\"false\" />\n        <rule name=\"nestedUserQueue\">\n            <rule name=\"secondaryGroupExistingQueue\" create=\"false\" />\n        </rule>\n        <rule name=\"default\" queue=\"sample_queue\"/>\n    </queuePlacementPolicy>\n</allocations>\n```\n\n如上队列分配文件，分配了多个队列，每个队列还可在其内部指定多个叶子队列\n\n最外层隐藏的最顶级队列是root队列，这里面配置的所有队列都是root队列的叶子队列，如果完全没有指定队列分配文件，则所有任务都会默认提交到root队列中执行\n\n队列中可以指定最小和最大分配资源数以及最大可运行的任务数，权重weight值（为同层级队列指定资源分配比例），指定权限用户（aclSubmitApps，拥有这个权限的用户可以提交和杀死这个队列中的任务，需要注意root队列的acl是`*`，即每个用户都有权限），调度策略（schedulingPolicy，在一个队列中一共有三种调度策略），最重要的是队列放置策略`queuePlacementPolicy`：\n\n```xml\n<queuePlacementPolicy>\n    <rule name=\"specified\" create=\"true\"/>\n    <rule name=\"user\" create=\"false\" />\n    <rule name=\"primaryGroup\" create=\"false\" />\n    <rule name=\"nestedUserQueue\">\n        <rule name=\"secondaryGroupExistingQueue\" create=\"false\" />\n    </rule>\n    <rule name=\"default\" queue=\"sample_queue\"/>\n</queuePlacementPolicy>\n```\n\n这是一个规则列表：\n\n- 首先如果specified为true，当前提交任务若指定了队列名则放置于指定队列执行，否则进行下一级判断\n- 如果user为true，则寻找以当前用户名为队列名的队列，若未创建则创建该队列；在这里user为false，就跳过这条判断\n- 如果primaryGroup为true，则寻找以当前用户的主group名命名的队列\n- nestedUserQueue：嵌套用户队列，与primaryGroup不同之处在于，user是应用到root队列的，而nestedUserQueue则会为任务寻找type为parent的队列，在这个队列下再去寻找有没有以任务用户名命名的队列\n- secondaryGroupExistingQueue：寻找以用户的Secondary group名字命名的队列；上面的配置中，yarn会在parent队列下寻找符合要求的嵌套队列\n- 最终如果没有找到匹配以上规则的队列，则执行提交到default队列，这里即为sample_queue队列\n\n##### Fair调度器的三种调度策略\n\n不同于FIFO调度器和容量调度器固定的调度策略，对于所有提交到某个队列中的任务，公平调度器为这个队列提供了三种调度策略，即：默认的`fair调度策略`，传统模式的`FIFO调度策略`，还有一种是`Dominant Resource Fairness(drf)策略`\n\n##### 抢占\n\n公平调度器中，当一个新的任务提交到一个队列，而该队列并没有空闲资源分配给它，这时该任务需要等待其他任务完成一部分container计算然后释放资源给新任务，以达到公平运行的目的\n\n为了使作业从提交到执行所需的时间可控，可以设置抢占模式，当等待时间超过一定阈值时即启动抢占，强迫队列中其他任务立刻让出一部分资源给新任务，达到强行公平运行的目的\n\nyarn-site.xml中将`yarn.scheduler.fair.preemption`设置为true即可打开抢占模式，并至少配置以下两个参数中的一个：\n\n- 最小资源抢占超时时间`minSharePreemptionTimeout`：若指定等待时间内未获得承诺的最小共享资源则会启动抢占\n- 公平资源抢占超时时间`fairSharePreemptionTimeout`：若指定等待时间内未获得承诺的公平共享资源则会启动抢占；承诺的公平共享资源由公平资源抢占阈值`fairSharePreemptionThreshold`和队列公平资源分配值的乘积决定，例如，当前队列一共提交了2个job，job1独占了队列资源，job2的公平资源理应为当前队列的0.5倍资源，若`fairSharePreemptionThreshold`为0.8，则承诺给这个任务的队列资源为0.4；该阈值默认是0.5\n\n以上两个参数均可以设置root队列级别的默认值：`defaultFairSharePreemptionThreshold `，`defaultMinSharePreemptionTimeout `\n\n##### 延迟调度\n\nyarn的资源管理器为任务分配节点的原则是基于任务所需数据先本地后远程，本地如果有资源就优先分配本地节点，如果本地没有资源再寻找远程节点\n\n不过有些时候稍微等待一些时间，待本地节点释放后就可以直接在本地启动任务了，不需要再寻找远程节点，这种行为称为`延迟调度`，容量调度器和公平调度器都支持这种方式\n\n- 对于容量调度器，设置`yarn.scheduler.capacity.node-locality-delay`开启本地延迟，该值为正整数，表示等待本地资源释放期间最多错过多少个远程资源释放的机会，比如设置为3，则表示最多等待3次远程资源释放的信息后，如果本地节点的资源仍然没释放，就直接寻找远程节点的资源，不再等本地了\n- 对于公平调度器，实现稍有不同，是将`yarn.scheduler.fair.locality.threshold.node`设置某个值，比如0.5，表示等待集群中最多半数节点给过资源释放信息后，再考虑远程节点，否则在这之前都将等待本地节点释放\n\n##### 主导资源公平性\n\n对于容量调度或公平调度，都是基于“资源”这一概念进行策略的，资源为内存或者cpu资源的抽象，两种调度模式都是基于某种资源的分配进行调度（内存或者cpu）\n\n不过如果某些任务对于内存或者cpu的依赖各异，这时候分配起来就比较复杂了，往往需要`Dominant Resource Fairness(drf)策略`进行支持\n\n**Dominant Resource Fairness(drf)，主导资源公平策略**：首先观察任务的主导资源（Dominant Resource）是内存还是cpu，选出主导资源，然后根据任务之间主导资源的占比来分配资源\n\n例如：\n\n- job1所需内存资源占集群总内存3%，所需cpu资源占集群总cpu1%，因此job1的主导资源是内存，占比3%\n- job2所需内存资源占2%，所需cpu资源占6%，job2的主导资源是cpu，占比6%\n- 因此job1和job2申请资源比例为`3% : 6%`，也就是1：2，job2分配的container数量为job1的两倍","source":"_posts/Yarn任务调度机制探析.md","raw":"title: Yarn任务调度机制探析\nauthor: 天渊\ntags:\n  - yarn\n  - hadoop\ncategories:\n  - 大数据\ndate: 2019-06-04 20:54:00\n---\nyarn作为hadoop任务调度组件，具有良好的可扩展性、高可用性以及其独特的“多租户”特性，它为不同的作业场景提供了几种特性各异的任务调度机制：`FIFO调度器`，`Capacity调度器`，`Fair调度器`\n<!-- more -->\n\n### FIFO调度器\n\nFIFO调度器采用一个先进先出队列对提交的任务执行顺序进行调度，按照提交的顺序首先为第一个任务的请求分配资源，第一个应用的请求被满足后，再依次为队列中下一个应用分配资源\n\nFIFO调度器的好处是实现简单，无需任务额外配置，但是缺点也很明显，不适合那种大型任务和小型任务穿插执行的共享集群，因为大型任务可能会独占集群中的全部计算资源，并且任务执行时间会很长，yarn短时间为无法为其他任务分配资源，因此只能阻塞在队列中等待大型任务执行完成释放资源：\n\n![1559481447073](\\blog\\images\\1559481447073.png)\n\n如图，横轴表示集群资源利用情况，当job1执行时，由于集群资源有限，job2必须等待job1执行完成后才能执行\n\n### Capacity调度器\n\n容量调度器可以为不同体量的任务提供一个或多个专用的等待队列，保证小型任务一旦提交就可以分配资源启动执行，因此解决了FIFO调度无法兼顾大型任务和小型任务的问题，大型任务的执行不会造成小型任务的长时间等待\n\n不过容量调度器也有自己的缺点，由于yarn要专门为小型任务预留一部分集群资源，分配给大型任务的资源就会相应减少，执行时间也就变长了：\n\n![1559482380158](\\blog\\images\\1559482380158.png)\n\n如图，queue B配置为小型任务服务，分配的资源较少，queue A配置为大型任务服务，分配的集群资源更多，保证大型任务和小型任务能够在集群中共存而不会相互阻塞\n\n配置容量调度器时，可以根据实际需要配置多个队列，每个队列分配不同数额的集群资源，不过如果某个队列的任务在执行过程中分配的集群资源不够用，为了不让该任务等待其他队列释放资源，需要为队列设置`maximun-capacity`，能够在资源不够用时进行动态扩容，如果集群中有空闲资源，则会为这个队列分配更多的资源，这种方式称为`队列弹性`，扩容后的资源总量保证不超过`maximun-capacity`即可\n\n提交map-reduce任务时，通过指定`mapreduce.job.<queue-name>`来指定当前任务分配给哪一个队列\n\n### Fair调度器\n\n即公平调度器，目的是为所有运行的任务公平分配集群资源，在容量调度器的基础上进行了改进，能够在不同任务之间**动态**地调度集群资源：\n\n![1559568139040](\\blog\\images\\1559568139040.png)\n\n如图，在公平调度器模式下，与容量调度类似，根据实际需要分配多个队列用于执行任务：\n\n1. job1率先提交，当前集群中没有其他任务共享资源，因为job1独享集群中queue A和queue B的全部资源\n\n2. job1执行过程中，job2提交，此时job1享有queue A为其分配的资源，而job2享有queue B为其分配的资源，job1和job2共享集群资源\n\n3. job2独享queue B的资源时，job3同样提交到queue B中执行，此时job2和job3共享queue B的资源\n\n4. 待job2执行完成后，job3独享queue B的资源\n\n可以看出，相比于容量调度器，公平调度模式下几乎不会出现饥饿情况（即有任务长期无法分配到集群资源而长时间处于阻塞状态），在满足一定的分配权重和调度策略的情况下，每个任务都能分享到一定数量的集群资源\n\nhadoop默认使用容量调度器，如果要在yarn中启用公平调度器，需要在`yarn-site.xml`作以下配置\n\n```xml\n<property>\n  <name>yarn.resourcemanager.scheduler.class</name>\n  <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n```\n\n#### Fair调度器的队列放置策略\n\n与容量调度器相似，公平调度器也可以执行某个任务提交到特定的队列中执行，也可以执行任务放置到以任务提交的用户名为队列名的队列下进行执行\n\n在yarn-site.xml中配置`yarn.scheduler.fair.allocation.file`执行队列分配文件，在队列分配文件中制定任务的队列分配策略(来自[hadoop官网yarn-FairScheduler文档](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html))：\n\n```xml\n<?xml version=\"1.0\"?>\n<allocations>\n    <queue name=\"sample_queue\">\n        <minResources>10000 mb,0vcores</minResources>\n        <maxResources>90000 mb,0vcores</maxResources>\n        <maxRunningApps>50</maxRunningApps>\n        <maxAMShare>0.1</maxAMShare>\n        <weight>2.0</weight>\n        <schedulingPolicy>fair</schedulingPolicy>\n        <queue name=\"sample_sub_queue\">\n            <aclSubmitApps>charlie</aclSubmitApps>\n            <minResources>5000 mb,0vcores</minResources>\n        </queue>\n        <queue name=\"sample_reservable_queue\">\n            <reservation></reservation>\n        </queue>\n    </queue>\n\n    <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>\n    <queueMaxResourcesDefault>40000 mb,0vcores</queueMaxResourcesDefault>\n\n    <queue name=\"secondary_group_queue\" type=\"parent\">\n        <weight>3.0</weight>\n        <maxChildResources>4096 mb,4vcores</maxChildResources>\n    </queue>\n\n    <user name=\"sample_user\">\n        <maxRunningApps>30</maxRunningApps>\n    </user>\n    <userMaxAppsDefault>5</userMaxAppsDefault>\n\n    <queuePlacementPolicy>\n        <rule name=\"specified\" />\n        <rule name=\"primaryGroup\" create=\"false\" />\n        <rule name=\"nestedUserQueue\">\n            <rule name=\"secondaryGroupExistingQueue\" create=\"false\" />\n        </rule>\n        <rule name=\"default\" queue=\"sample_queue\"/>\n    </queuePlacementPolicy>\n</allocations>\n```\n\n如上队列分配文件，分配了多个队列，每个队列还可在其内部指定多个叶子队列\n\n最外层隐藏的最顶级队列是root队列，这里面配置的所有队列都是root队列的叶子队列，如果完全没有指定队列分配文件，则所有任务都会默认提交到root队列中执行\n\n队列中可以指定最小和最大分配资源数以及最大可运行的任务数，权重weight值（为同层级队列指定资源分配比例），指定权限用户（aclSubmitApps，拥有这个权限的用户可以提交和杀死这个队列中的任务，需要注意root队列的acl是`*`，即每个用户都有权限），调度策略（schedulingPolicy，在一个队列中一共有三种调度策略），最重要的是队列放置策略`queuePlacementPolicy`：\n\n```xml\n<queuePlacementPolicy>\n    <rule name=\"specified\" create=\"true\"/>\n    <rule name=\"user\" create=\"false\" />\n    <rule name=\"primaryGroup\" create=\"false\" />\n    <rule name=\"nestedUserQueue\">\n        <rule name=\"secondaryGroupExistingQueue\" create=\"false\" />\n    </rule>\n    <rule name=\"default\" queue=\"sample_queue\"/>\n</queuePlacementPolicy>\n```\n\n这是一个规则列表：\n\n- 首先如果specified为true，当前提交任务若指定了队列名则放置于指定队列执行，否则进行下一级判断\n- 如果user为true，则寻找以当前用户名为队列名的队列，若未创建则创建该队列；在这里user为false，就跳过这条判断\n- 如果primaryGroup为true，则寻找以当前用户的主group名命名的队列\n- nestedUserQueue：嵌套用户队列，与primaryGroup不同之处在于，user是应用到root队列的，而nestedUserQueue则会为任务寻找type为parent的队列，在这个队列下再去寻找有没有以任务用户名命名的队列\n- secondaryGroupExistingQueue：寻找以用户的Secondary group名字命名的队列；上面的配置中，yarn会在parent队列下寻找符合要求的嵌套队列\n- 最终如果没有找到匹配以上规则的队列，则执行提交到default队列，这里即为sample_queue队列\n\n##### Fair调度器的三种调度策略\n\n不同于FIFO调度器和容量调度器固定的调度策略，对于所有提交到某个队列中的任务，公平调度器为这个队列提供了三种调度策略，即：默认的`fair调度策略`，传统模式的`FIFO调度策略`，还有一种是`Dominant Resource Fairness(drf)策略`\n\n##### 抢占\n\n公平调度器中，当一个新的任务提交到一个队列，而该队列并没有空闲资源分配给它，这时该任务需要等待其他任务完成一部分container计算然后释放资源给新任务，以达到公平运行的目的\n\n为了使作业从提交到执行所需的时间可控，可以设置抢占模式，当等待时间超过一定阈值时即启动抢占，强迫队列中其他任务立刻让出一部分资源给新任务，达到强行公平运行的目的\n\nyarn-site.xml中将`yarn.scheduler.fair.preemption`设置为true即可打开抢占模式，并至少配置以下两个参数中的一个：\n\n- 最小资源抢占超时时间`minSharePreemptionTimeout`：若指定等待时间内未获得承诺的最小共享资源则会启动抢占\n- 公平资源抢占超时时间`fairSharePreemptionTimeout`：若指定等待时间内未获得承诺的公平共享资源则会启动抢占；承诺的公平共享资源由公平资源抢占阈值`fairSharePreemptionThreshold`和队列公平资源分配值的乘积决定，例如，当前队列一共提交了2个job，job1独占了队列资源，job2的公平资源理应为当前队列的0.5倍资源，若`fairSharePreemptionThreshold`为0.8，则承诺给这个任务的队列资源为0.4；该阈值默认是0.5\n\n以上两个参数均可以设置root队列级别的默认值：`defaultFairSharePreemptionThreshold `，`defaultMinSharePreemptionTimeout `\n\n##### 延迟调度\n\nyarn的资源管理器为任务分配节点的原则是基于任务所需数据先本地后远程，本地如果有资源就优先分配本地节点，如果本地没有资源再寻找远程节点\n\n不过有些时候稍微等待一些时间，待本地节点释放后就可以直接在本地启动任务了，不需要再寻找远程节点，这种行为称为`延迟调度`，容量调度器和公平调度器都支持这种方式\n\n- 对于容量调度器，设置`yarn.scheduler.capacity.node-locality-delay`开启本地延迟，该值为正整数，表示等待本地资源释放期间最多错过多少个远程资源释放的机会，比如设置为3，则表示最多等待3次远程资源释放的信息后，如果本地节点的资源仍然没释放，就直接寻找远程节点的资源，不再等本地了\n- 对于公平调度器，实现稍有不同，是将`yarn.scheduler.fair.locality.threshold.node`设置某个值，比如0.5，表示等待集群中最多半数节点给过资源释放信息后，再考虑远程节点，否则在这之前都将等待本地节点释放\n\n##### 主导资源公平性\n\n对于容量调度或公平调度，都是基于“资源”这一概念进行策略的，资源为内存或者cpu资源的抽象，两种调度模式都是基于某种资源的分配进行调度（内存或者cpu）\n\n不过如果某些任务对于内存或者cpu的依赖各异，这时候分配起来就比较复杂了，往往需要`Dominant Resource Fairness(drf)策略`进行支持\n\n**Dominant Resource Fairness(drf)，主导资源公平策略**：首先观察任务的主导资源（Dominant Resource）是内存还是cpu，选出主导资源，然后根据任务之间主导资源的占比来分配资源\n\n例如：\n\n- job1所需内存资源占集群总内存3%，所需cpu资源占集群总cpu1%，因此job1的主导资源是内存，占比3%\n- job2所需内存资源占2%，所需cpu资源占6%，job2的主导资源是cpu，占比6%\n- 因此job1和job2申请资源比例为`3% : 6%`，也就是1：2，job2分配的container数量为job1的两倍","slug":"Yarn任务调度机制探析","published":1,"updated":"2019-06-04T13:03:35.249Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyh9js60002vg0qregz1x597","content":"<p>yarn作为hadoop任务调度组件，具有良好的可扩展性、高可用性以及其独特的“多租户”特性，它为不同的作业场景提供了几种特性各异的任务调度机制：<code>FIFO调度器</code>，<code>Capacity调度器</code>，<code>Fair调度器</code><br><a id=\"more\"></a></p>\n<h3 id=\"FIFO调度器\"><a href=\"#FIFO调度器\" class=\"headerlink\" title=\"FIFO调度器\"></a>FIFO调度器</h3><p>FIFO调度器采用一个先进先出队列对提交的任务执行顺序进行调度，按照提交的顺序首先为第一个任务的请求分配资源，第一个应用的请求被满足后，再依次为队列中下一个应用分配资源</p>\n<p>FIFO调度器的好处是实现简单，无需任务额外配置，但是缺点也很明显，不适合那种大型任务和小型任务穿插执行的共享集群，因为大型任务可能会独占集群中的全部计算资源，并且任务执行时间会很长，yarn短时间为无法为其他任务分配资源，因此只能阻塞在队列中等待大型任务执行完成释放资源：</p>\n<p><img src=\"\\blog\\images\\1559481447073.png\" alt=\"1559481447073\"></p>\n<p>如图，横轴表示集群资源利用情况，当job1执行时，由于集群资源有限，job2必须等待job1执行完成后才能执行</p>\n<h3 id=\"Capacity调度器\"><a href=\"#Capacity调度器\" class=\"headerlink\" title=\"Capacity调度器\"></a>Capacity调度器</h3><p>容量调度器可以为不同体量的任务提供一个或多个专用的等待队列，保证小型任务一旦提交就可以分配资源启动执行，因此解决了FIFO调度无法兼顾大型任务和小型任务的问题，大型任务的执行不会造成小型任务的长时间等待</p>\n<p>不过容量调度器也有自己的缺点，由于yarn要专门为小型任务预留一部分集群资源，分配给大型任务的资源就会相应减少，执行时间也就变长了：</p>\n<p><img src=\"\\blog\\images\\1559482380158.png\" alt=\"1559482380158\"></p>\n<p>如图，queue B配置为小型任务服务，分配的资源较少，queue A配置为大型任务服务，分配的集群资源更多，保证大型任务和小型任务能够在集群中共存而不会相互阻塞</p>\n<p>配置容量调度器时，可以根据实际需要配置多个队列，每个队列分配不同数额的集群资源，不过如果某个队列的任务在执行过程中分配的集群资源不够用，为了不让该任务等待其他队列释放资源，需要为队列设置<code>maximun-capacity</code>，能够在资源不够用时进行动态扩容，如果集群中有空闲资源，则会为这个队列分配更多的资源，这种方式称为<code>队列弹性</code>，扩容后的资源总量保证不超过<code>maximun-capacity</code>即可</p>\n<p>提交map-reduce任务时，通过指定<code>mapreduce.job.&lt;queue-name&gt;</code>来指定当前任务分配给哪一个队列</p>\n<h3 id=\"Fair调度器\"><a href=\"#Fair调度器\" class=\"headerlink\" title=\"Fair调度器\"></a>Fair调度器</h3><p>即公平调度器，目的是为所有运行的任务公平分配集群资源，在容量调度器的基础上进行了改进，能够在不同任务之间<strong>动态</strong>地调度集群资源：</p>\n<p><img src=\"\\blog\\images\\1559568139040.png\" alt=\"1559568139040\"></p>\n<p>如图，在公平调度器模式下，与容量调度类似，根据实际需要分配多个队列用于执行任务：</p>\n<ol>\n<li><p>job1率先提交，当前集群中没有其他任务共享资源，因为job1独享集群中queue A和queue B的全部资源</p>\n</li>\n<li><p>job1执行过程中，job2提交，此时job1享有queue A为其分配的资源，而job2享有queue B为其分配的资源，job1和job2共享集群资源</p>\n</li>\n<li><p>job2独享queue B的资源时，job3同样提交到queue B中执行，此时job2和job3共享queue B的资源</p>\n</li>\n<li><p>待job2执行完成后，job3独享queue B的资源</p>\n</li>\n</ol>\n<p>可以看出，相比于容量调度器，公平调度模式下几乎不会出现饥饿情况（即有任务长期无法分配到集群资源而长时间处于阻塞状态），在满足一定的分配权重和调度策略的情况下，每个任务都能分享到一定数量的集群资源</p>\n<p>hadoop默认使用容量调度器，如果要在yarn中启用公平调度器，需要在<code>yarn-site.xml</code>作以下配置</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"Fair调度器的队列放置策略\"><a href=\"#Fair调度器的队列放置策略\" class=\"headerlink\" title=\"Fair调度器的队列放置策略\"></a>Fair调度器的队列放置策略</h4><p>与容量调度器相似，公平调度器也可以执行某个任务提交到特定的队列中执行，也可以执行任务放置到以任务提交的用户名为队列名的队列下进行执行</p>\n<p>在yarn-site.xml中配置<code>yarn.scheduler.fair.allocation.file</code>执行队列分配文件，在队列分配文件中制定任务的队列分配策略(来自<a href=\"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html\" target=\"_blank\" rel=\"noopener\">hadoop官网yarn-FairScheduler文档</a>)：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\"?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">allocations</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_queue\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">minResources</span>&gt;</span>10000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">minResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxResources</span>&gt;</span>90000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">maxResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxRunningApps</span>&gt;</span>50<span class=\"tag\">&lt;/<span class=\"name\">maxRunningApps</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxAMShare</span>&gt;</span>0.1<span class=\"tag\">&lt;/<span class=\"name\">maxAMShare</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>2.0<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">schedulingPolicy</span>&gt;</span>fair<span class=\"tag\">&lt;/<span class=\"name\">schedulingPolicy</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_sub_queue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">aclSubmitApps</span>&gt;</span>charlie<span class=\"tag\">&lt;/<span class=\"name\">aclSubmitApps</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">minResources</span>&gt;</span>5000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">minResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_reservable_queue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">reservation</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">reservation</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queueMaxAMShareDefault</span>&gt;</span>0.5<span class=\"tag\">&lt;/<span class=\"name\">queueMaxAMShareDefault</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queueMaxResourcesDefault</span>&gt;</span>40000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">queueMaxResourcesDefault</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondary_group_queue\"</span> <span class=\"attr\">type</span>=<span class=\"string\">\"parent\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>3.0<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxChildResources</span>&gt;</span>4096 mb,4vcores<span class=\"tag\">&lt;/<span class=\"name\">maxChildResources</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">user</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_user\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxRunningApps</span>&gt;</span>30<span class=\"tag\">&lt;/<span class=\"name\">maxRunningApps</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">user</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">userMaxAppsDefault</span>&gt;</span>5<span class=\"tag\">&lt;/<span class=\"name\">userMaxAppsDefault</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"specified\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"primaryGroup\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"nestedUserQueue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondaryGroupExistingQueue\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">rule</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"default\"</span> <span class=\"attr\">queue</span>=<span class=\"string\">\"sample_queue\"</span>/&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>如上队列分配文件，分配了多个队列，每个队列还可在其内部指定多个叶子队列</p>\n<p>最外层隐藏的最顶级队列是root队列，这里面配置的所有队列都是root队列的叶子队列，如果完全没有指定队列分配文件，则所有任务都会默认提交到root队列中执行</p>\n<p>队列中可以指定最小和最大分配资源数以及最大可运行的任务数，权重weight值（为同层级队列指定资源分配比例），指定权限用户（aclSubmitApps，拥有这个权限的用户可以提交和杀死这个队列中的任务，需要注意root队列的acl是<code>*</code>，即每个用户都有权限），调度策略（schedulingPolicy，在一个队列中一共有三种调度策略），最重要的是队列放置策略<code>queuePlacementPolicy</code>：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"specified\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"true\"</span>/&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"user\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"primaryGroup\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"nestedUserQueue\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondaryGroupExistingQueue\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">rule</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"default\"</span> <span class=\"attr\">queue</span>=<span class=\"string\">\"sample_queue\"</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>这是一个规则列表：</p>\n<ul>\n<li>首先如果specified为true，当前提交任务若指定了队列名则放置于指定队列执行，否则进行下一级判断</li>\n<li>如果user为true，则寻找以当前用户名为队列名的队列，若未创建则创建该队列；在这里user为false，就跳过这条判断</li>\n<li>如果primaryGroup为true，则寻找以当前用户的主group名命名的队列</li>\n<li>nestedUserQueue：嵌套用户队列，与primaryGroup不同之处在于，user是应用到root队列的，而nestedUserQueue则会为任务寻找type为parent的队列，在这个队列下再去寻找有没有以任务用户名命名的队列</li>\n<li>secondaryGroupExistingQueue：寻找以用户的Secondary group名字命名的队列；上面的配置中，yarn会在parent队列下寻找符合要求的嵌套队列</li>\n<li>最终如果没有找到匹配以上规则的队列，则执行提交到default队列，这里即为sample_queue队列</li>\n</ul>\n<h5 id=\"Fair调度器的三种调度策略\"><a href=\"#Fair调度器的三种调度策略\" class=\"headerlink\" title=\"Fair调度器的三种调度策略\"></a>Fair调度器的三种调度策略</h5><p>不同于FIFO调度器和容量调度器固定的调度策略，对于所有提交到某个队列中的任务，公平调度器为这个队列提供了三种调度策略，即：默认的<code>fair调度策略</code>，传统模式的<code>FIFO调度策略</code>，还有一种是<code>Dominant Resource Fairness(drf)策略</code></p>\n<h5 id=\"抢占\"><a href=\"#抢占\" class=\"headerlink\" title=\"抢占\"></a>抢占</h5><p>公平调度器中，当一个新的任务提交到一个队列，而该队列并没有空闲资源分配给它，这时该任务需要等待其他任务完成一部分container计算然后释放资源给新任务，以达到公平运行的目的</p>\n<p>为了使作业从提交到执行所需的时间可控，可以设置抢占模式，当等待时间超过一定阈值时即启动抢占，强迫队列中其他任务立刻让出一部分资源给新任务，达到强行公平运行的目的</p>\n<p>yarn-site.xml中将<code>yarn.scheduler.fair.preemption</code>设置为true即可打开抢占模式，并至少配置以下两个参数中的一个：</p>\n<ul>\n<li>最小资源抢占超时时间<code>minSharePreemptionTimeout</code>：若指定等待时间内未获得承诺的最小共享资源则会启动抢占</li>\n<li>公平资源抢占超时时间<code>fairSharePreemptionTimeout</code>：若指定等待时间内未获得承诺的公平共享资源则会启动抢占；承诺的公平共享资源由公平资源抢占阈值<code>fairSharePreemptionThreshold</code>和队列公平资源分配值的乘积决定，例如，当前队列一共提交了2个job，job1独占了队列资源，job2的公平资源理应为当前队列的0.5倍资源，若<code>fairSharePreemptionThreshold</code>为0.8，则承诺给这个任务的队列资源为0.4；该阈值默认是0.5</li>\n</ul>\n<p>以上两个参数均可以设置root队列级别的默认值：<code>defaultFairSharePreemptionThreshold</code>，<code>defaultMinSharePreemptionTimeout</code></p>\n<h5 id=\"延迟调度\"><a href=\"#延迟调度\" class=\"headerlink\" title=\"延迟调度\"></a>延迟调度</h5><p>yarn的资源管理器为任务分配节点的原则是基于任务所需数据先本地后远程，本地如果有资源就优先分配本地节点，如果本地没有资源再寻找远程节点</p>\n<p>不过有些时候稍微等待一些时间，待本地节点释放后就可以直接在本地启动任务了，不需要再寻找远程节点，这种行为称为<code>延迟调度</code>，容量调度器和公平调度器都支持这种方式</p>\n<ul>\n<li>对于容量调度器，设置<code>yarn.scheduler.capacity.node-locality-delay</code>开启本地延迟，该值为正整数，表示等待本地资源释放期间最多错过多少个远程资源释放的机会，比如设置为3，则表示最多等待3次远程资源释放的信息后，如果本地节点的资源仍然没释放，就直接寻找远程节点的资源，不再等本地了</li>\n<li>对于公平调度器，实现稍有不同，是将<code>yarn.scheduler.fair.locality.threshold.node</code>设置某个值，比如0.5，表示等待集群中最多半数节点给过资源释放信息后，再考虑远程节点，否则在这之前都将等待本地节点释放</li>\n</ul>\n<h5 id=\"主导资源公平性\"><a href=\"#主导资源公平性\" class=\"headerlink\" title=\"主导资源公平性\"></a>主导资源公平性</h5><p>对于容量调度或公平调度，都是基于“资源”这一概念进行策略的，资源为内存或者cpu资源的抽象，两种调度模式都是基于某种资源的分配进行调度（内存或者cpu）</p>\n<p>不过如果某些任务对于内存或者cpu的依赖各异，这时候分配起来就比较复杂了，往往需要<code>Dominant Resource Fairness(drf)策略</code>进行支持</p>\n<p><strong>Dominant Resource Fairness(drf)，主导资源公平策略</strong>：首先观察任务的主导资源（Dominant Resource）是内存还是cpu，选出主导资源，然后根据任务之间主导资源的占比来分配资源</p>\n<p>例如：</p>\n<ul>\n<li>job1所需内存资源占集群总内存3%，所需cpu资源占集群总cpu1%，因此job1的主导资源是内存，占比3%</li>\n<li>job2所需内存资源占2%，所需cpu资源占6%，job2的主导资源是cpu，占比6%</li>\n<li>因此job1和job2申请资源比例为<code>3% : 6%</code>，也就是1：2，job2分配的container数量为job1的两倍</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>yarn作为hadoop任务调度组件，具有良好的可扩展性、高可用性以及其独特的“多租户”特性，它为不同的作业场景提供了几种特性各异的任务调度机制：<code>FIFO调度器</code>，<code>Capacity调度器</code>，<code>Fair调度器</code><br>","more":"</p>\n<h3 id=\"FIFO调度器\"><a href=\"#FIFO调度器\" class=\"headerlink\" title=\"FIFO调度器\"></a>FIFO调度器</h3><p>FIFO调度器采用一个先进先出队列对提交的任务执行顺序进行调度，按照提交的顺序首先为第一个任务的请求分配资源，第一个应用的请求被满足后，再依次为队列中下一个应用分配资源</p>\n<p>FIFO调度器的好处是实现简单，无需任务额外配置，但是缺点也很明显，不适合那种大型任务和小型任务穿插执行的共享集群，因为大型任务可能会独占集群中的全部计算资源，并且任务执行时间会很长，yarn短时间为无法为其他任务分配资源，因此只能阻塞在队列中等待大型任务执行完成释放资源：</p>\n<p><img src=\"\\blog\\images\\1559481447073.png\" alt=\"1559481447073\"></p>\n<p>如图，横轴表示集群资源利用情况，当job1执行时，由于集群资源有限，job2必须等待job1执行完成后才能执行</p>\n<h3 id=\"Capacity调度器\"><a href=\"#Capacity调度器\" class=\"headerlink\" title=\"Capacity调度器\"></a>Capacity调度器</h3><p>容量调度器可以为不同体量的任务提供一个或多个专用的等待队列，保证小型任务一旦提交就可以分配资源启动执行，因此解决了FIFO调度无法兼顾大型任务和小型任务的问题，大型任务的执行不会造成小型任务的长时间等待</p>\n<p>不过容量调度器也有自己的缺点，由于yarn要专门为小型任务预留一部分集群资源，分配给大型任务的资源就会相应减少，执行时间也就变长了：</p>\n<p><img src=\"\\blog\\images\\1559482380158.png\" alt=\"1559482380158\"></p>\n<p>如图，queue B配置为小型任务服务，分配的资源较少，queue A配置为大型任务服务，分配的集群资源更多，保证大型任务和小型任务能够在集群中共存而不会相互阻塞</p>\n<p>配置容量调度器时，可以根据实际需要配置多个队列，每个队列分配不同数额的集群资源，不过如果某个队列的任务在执行过程中分配的集群资源不够用，为了不让该任务等待其他队列释放资源，需要为队列设置<code>maximun-capacity</code>，能够在资源不够用时进行动态扩容，如果集群中有空闲资源，则会为这个队列分配更多的资源，这种方式称为<code>队列弹性</code>，扩容后的资源总量保证不超过<code>maximun-capacity</code>即可</p>\n<p>提交map-reduce任务时，通过指定<code>mapreduce.job.&lt;queue-name&gt;</code>来指定当前任务分配给哪一个队列</p>\n<h3 id=\"Fair调度器\"><a href=\"#Fair调度器\" class=\"headerlink\" title=\"Fair调度器\"></a>Fair调度器</h3><p>即公平调度器，目的是为所有运行的任务公平分配集群资源，在容量调度器的基础上进行了改进，能够在不同任务之间<strong>动态</strong>地调度集群资源：</p>\n<p><img src=\"\\blog\\images\\1559568139040.png\" alt=\"1559568139040\"></p>\n<p>如图，在公平调度器模式下，与容量调度类似，根据实际需要分配多个队列用于执行任务：</p>\n<ol>\n<li><p>job1率先提交，当前集群中没有其他任务共享资源，因为job1独享集群中queue A和queue B的全部资源</p>\n</li>\n<li><p>job1执行过程中，job2提交，此时job1享有queue A为其分配的资源，而job2享有queue B为其分配的资源，job1和job2共享集群资源</p>\n</li>\n<li><p>job2独享queue B的资源时，job3同样提交到queue B中执行，此时job2和job3共享queue B的资源</p>\n</li>\n<li><p>待job2执行完成后，job3独享queue B的资源</p>\n</li>\n</ol>\n<p>可以看出，相比于容量调度器，公平调度模式下几乎不会出现饥饿情况（即有任务长期无法分配到集群资源而长时间处于阻塞状态），在满足一定的分配权重和调度策略的情况下，每个任务都能分享到一定数量的集群资源</p>\n<p>hadoop默认使用容量调度器，如果要在yarn中启用公平调度器，需要在<code>yarn-site.xml</code>作以下配置</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"Fair调度器的队列放置策略\"><a href=\"#Fair调度器的队列放置策略\" class=\"headerlink\" title=\"Fair调度器的队列放置策略\"></a>Fair调度器的队列放置策略</h4><p>与容量调度器相似，公平调度器也可以执行某个任务提交到特定的队列中执行，也可以执行任务放置到以任务提交的用户名为队列名的队列下进行执行</p>\n<p>在yarn-site.xml中配置<code>yarn.scheduler.fair.allocation.file</code>执行队列分配文件，在队列分配文件中制定任务的队列分配策略(来自<a href=\"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html\" target=\"_blank\" rel=\"noopener\">hadoop官网yarn-FairScheduler文档</a>)：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\"?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">allocations</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_queue\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">minResources</span>&gt;</span>10000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">minResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxResources</span>&gt;</span>90000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">maxResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxRunningApps</span>&gt;</span>50<span class=\"tag\">&lt;/<span class=\"name\">maxRunningApps</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxAMShare</span>&gt;</span>0.1<span class=\"tag\">&lt;/<span class=\"name\">maxAMShare</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>2.0<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">schedulingPolicy</span>&gt;</span>fair<span class=\"tag\">&lt;/<span class=\"name\">schedulingPolicy</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_sub_queue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">aclSubmitApps</span>&gt;</span>charlie<span class=\"tag\">&lt;/<span class=\"name\">aclSubmitApps</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">minResources</span>&gt;</span>5000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">minResources</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_reservable_queue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">reservation</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">reservation</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queueMaxAMShareDefault</span>&gt;</span>0.5<span class=\"tag\">&lt;/<span class=\"name\">queueMaxAMShareDefault</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queueMaxResourcesDefault</span>&gt;</span>40000 mb,0vcores<span class=\"tag\">&lt;/<span class=\"name\">queueMaxResourcesDefault</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queue</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondary_group_queue\"</span> <span class=\"attr\">type</span>=<span class=\"string\">\"parent\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>3.0<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxChildResources</span>&gt;</span>4096 mb,4vcores<span class=\"tag\">&lt;/<span class=\"name\">maxChildResources</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queue</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">user</span> <span class=\"attr\">name</span>=<span class=\"string\">\"sample_user\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">maxRunningApps</span>&gt;</span>30<span class=\"tag\">&lt;/<span class=\"name\">maxRunningApps</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">user</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">userMaxAppsDefault</span>&gt;</span>5<span class=\"tag\">&lt;/<span class=\"name\">userMaxAppsDefault</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"specified\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"primaryGroup\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"nestedUserQueue\"</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondaryGroupExistingQueue\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">rule</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"default\"</span> <span class=\"attr\">queue</span>=<span class=\"string\">\"sample_queue\"</span>/&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>如上队列分配文件，分配了多个队列，每个队列还可在其内部指定多个叶子队列</p>\n<p>最外层隐藏的最顶级队列是root队列，这里面配置的所有队列都是root队列的叶子队列，如果完全没有指定队列分配文件，则所有任务都会默认提交到root队列中执行</p>\n<p>队列中可以指定最小和最大分配资源数以及最大可运行的任务数，权重weight值（为同层级队列指定资源分配比例），指定权限用户（aclSubmitApps，拥有这个权限的用户可以提交和杀死这个队列中的任务，需要注意root队列的acl是<code>*</code>，即每个用户都有权限），调度策略（schedulingPolicy，在一个队列中一共有三种调度策略），最重要的是队列放置策略<code>queuePlacementPolicy</code>：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"specified\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"true\"</span>/&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"user\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"primaryGroup\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"nestedUserQueue\"</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"secondaryGroupExistingQueue\"</span> <span class=\"attr\">create</span>=<span class=\"string\">\"false\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">rule</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">rule</span> <span class=\"attr\">name</span>=<span class=\"string\">\"default\"</span> <span class=\"attr\">queue</span>=<span class=\"string\">\"sample_queue\"</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">queuePlacementPolicy</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>这是一个规则列表：</p>\n<ul>\n<li>首先如果specified为true，当前提交任务若指定了队列名则放置于指定队列执行，否则进行下一级判断</li>\n<li>如果user为true，则寻找以当前用户名为队列名的队列，若未创建则创建该队列；在这里user为false，就跳过这条判断</li>\n<li>如果primaryGroup为true，则寻找以当前用户的主group名命名的队列</li>\n<li>nestedUserQueue：嵌套用户队列，与primaryGroup不同之处在于，user是应用到root队列的，而nestedUserQueue则会为任务寻找type为parent的队列，在这个队列下再去寻找有没有以任务用户名命名的队列</li>\n<li>secondaryGroupExistingQueue：寻找以用户的Secondary group名字命名的队列；上面的配置中，yarn会在parent队列下寻找符合要求的嵌套队列</li>\n<li>最终如果没有找到匹配以上规则的队列，则执行提交到default队列，这里即为sample_queue队列</li>\n</ul>\n<h5 id=\"Fair调度器的三种调度策略\"><a href=\"#Fair调度器的三种调度策略\" class=\"headerlink\" title=\"Fair调度器的三种调度策略\"></a>Fair调度器的三种调度策略</h5><p>不同于FIFO调度器和容量调度器固定的调度策略，对于所有提交到某个队列中的任务，公平调度器为这个队列提供了三种调度策略，即：默认的<code>fair调度策略</code>，传统模式的<code>FIFO调度策略</code>，还有一种是<code>Dominant Resource Fairness(drf)策略</code></p>\n<h5 id=\"抢占\"><a href=\"#抢占\" class=\"headerlink\" title=\"抢占\"></a>抢占</h5><p>公平调度器中，当一个新的任务提交到一个队列，而该队列并没有空闲资源分配给它，这时该任务需要等待其他任务完成一部分container计算然后释放资源给新任务，以达到公平运行的目的</p>\n<p>为了使作业从提交到执行所需的时间可控，可以设置抢占模式，当等待时间超过一定阈值时即启动抢占，强迫队列中其他任务立刻让出一部分资源给新任务，达到强行公平运行的目的</p>\n<p>yarn-site.xml中将<code>yarn.scheduler.fair.preemption</code>设置为true即可打开抢占模式，并至少配置以下两个参数中的一个：</p>\n<ul>\n<li>最小资源抢占超时时间<code>minSharePreemptionTimeout</code>：若指定等待时间内未获得承诺的最小共享资源则会启动抢占</li>\n<li>公平资源抢占超时时间<code>fairSharePreemptionTimeout</code>：若指定等待时间内未获得承诺的公平共享资源则会启动抢占；承诺的公平共享资源由公平资源抢占阈值<code>fairSharePreemptionThreshold</code>和队列公平资源分配值的乘积决定，例如，当前队列一共提交了2个job，job1独占了队列资源，job2的公平资源理应为当前队列的0.5倍资源，若<code>fairSharePreemptionThreshold</code>为0.8，则承诺给这个任务的队列资源为0.4；该阈值默认是0.5</li>\n</ul>\n<p>以上两个参数均可以设置root队列级别的默认值：<code>defaultFairSharePreemptionThreshold</code>，<code>defaultMinSharePreemptionTimeout</code></p>\n<h5 id=\"延迟调度\"><a href=\"#延迟调度\" class=\"headerlink\" title=\"延迟调度\"></a>延迟调度</h5><p>yarn的资源管理器为任务分配节点的原则是基于任务所需数据先本地后远程，本地如果有资源就优先分配本地节点，如果本地没有资源再寻找远程节点</p>\n<p>不过有些时候稍微等待一些时间，待本地节点释放后就可以直接在本地启动任务了，不需要再寻找远程节点，这种行为称为<code>延迟调度</code>，容量调度器和公平调度器都支持这种方式</p>\n<ul>\n<li>对于容量调度器，设置<code>yarn.scheduler.capacity.node-locality-delay</code>开启本地延迟，该值为正整数，表示等待本地资源释放期间最多错过多少个远程资源释放的机会，比如设置为3，则表示最多等待3次远程资源释放的信息后，如果本地节点的资源仍然没释放，就直接寻找远程节点的资源，不再等本地了</li>\n<li>对于公平调度器，实现稍有不同，是将<code>yarn.scheduler.fair.locality.threshold.node</code>设置某个值，比如0.5，表示等待集群中最多半数节点给过资源释放信息后，再考虑远程节点，否则在这之前都将等待本地节点释放</li>\n</ul>\n<h5 id=\"主导资源公平性\"><a href=\"#主导资源公平性\" class=\"headerlink\" title=\"主导资源公平性\"></a>主导资源公平性</h5><p>对于容量调度或公平调度，都是基于“资源”这一概念进行策略的，资源为内存或者cpu资源的抽象，两种调度模式都是基于某种资源的分配进行调度（内存或者cpu）</p>\n<p>不过如果某些任务对于内存或者cpu的依赖各异，这时候分配起来就比较复杂了，往往需要<code>Dominant Resource Fairness(drf)策略</code>进行支持</p>\n<p><strong>Dominant Resource Fairness(drf)，主导资源公平策略</strong>：首先观察任务的主导资源（Dominant Resource）是内存还是cpu，选出主导资源，然后根据任务之间主导资源的占比来分配资源</p>\n<p>例如：</p>\n<ul>\n<li>job1所需内存资源占集群总内存3%，所需cpu资源占集群总cpu1%，因此job1的主导资源是内存，占比3%</li>\n<li>job2所需内存资源占2%，所需cpu资源占6%，job2的主导资源是cpu，占比6%</li>\n<li>因此job1和job2申请资源比例为<code>3% : 6%</code>，也就是1：2，job2分配的container数量为job1的两倍</li>\n</ul>"},{"title":"对于Map-Reduce并行度的理解","author":"天渊","date":"2019-07-24T13:08:00.000Z","_content":"\nhadoop计算框架map-reduce有一个并行度的概念，每个job，对于输入文件A，需要对A进行切片（即`split`），再针对各个`split`单独启动独立的`mapTask`进行计算（hadoop 2.0后由yarn完成），切片完成后启动多个`mapTask`即为mr任务的并行度\n\n<!-- more -->\n\n### map-reduce的split方式\n\n默认情况下，文件的单个split大小（即`split-size`）通常与HDFS的`block-size`保持一致（即默认的128MB），该工作由`FileInputFormat`调用`getSplits()`方法来完成，通过读取文件metadata进行切分，生成对应的`FileSplit`对象，其中就包含了各个文件切片的offset和length等信息，再序列化到`job.splits`文件中：\n\n```java\n// InputFormat类中的方法，从JobContext中获取输入文件的信息\n// 根据输入文件信息生成切分信息\npublic abstract List<InputSplit> getSplits(JobContext context)\n```\n\n文件的切分信息保存在`FileSplit`对象中，主要保存的了文件在相应`FileSystem`上的path，切分开始位置和切分长度，以及主机信息和当前Split的具体位置：\n\n```java\npublic class FileSplit extends InputSplit implements Writable {\n  private Path file;\n  private long start;\n  private long length;\n  private String[] hosts;\n  private SplitLocationInfo[] hostInfos;\n    \n  ......\n}\n```\n\n### map-reduce任务提交过程\n\nmap-reduce在客户端完成切分工作后上传到服务器，针对每个`Split`单独启动mapTask，下面来看看在客户端提交job后是怎么进行split的：\n\n1. `job.submit()`后，初始化一个`JobSubmitter`对象进行job的提交工作\n\n2. `JobSubmitter`调用`submitJobInternal(job, cluster)`方法进行方法的提交，在进行一系列的初始化过程后，调用`writeSplits(job, submitJobDir)`方法进行切分\n\n3. `writeSplits`最终就会调用上面提到的`InputFormat`的`getSplits`方法\n\n4. 在`getSplits`方法中，首先计算split的最大和最小限制：\n\n   ```java\n   // 默认是1，可以通过mapreduce.input.fileinputformat.split.minsize属性进行设置\n   long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\n   // 默认是Long.MAX_VALUE，可以通过mapreduce.input.fileinputformat.split.maxsize属性来设置\n   long maxSize = getMaxSplitSize(job);\n   ```\n\n5. 在确认最大最小范围后，需要确定真正需要`splitSize`，使用`computeSplitSize`方法进行确认，可以看出通常情况下`splitSize`即为`blockSize`：\n\n   ```java\n   long splitSize = computeSplitSize(blockSize, minSize, maxSize);\n   //computeSplitSize方法：\n   protected long computeSplitSize(long blockSize, long minSize, long maxSize) {\n       // 在maxSize和blockSize中取小值，最后保证比minSize大\n       return Math.max(minSize, Math.min(maxSize, blockSize));\n   }\n   ```\n\n6. 对文件进行split，代码如下：\n\n   ```java\n   // 剩余还未split的数量\n   long bytesRemaining = length;\n   // 如果剩余数量多于1.1倍的splitSize，则持续进行split\n   while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {\n       // 获取当前offset所处的block\n       int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n       // 生成split\n       splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                            blkLocations[blkIndex].getHosts(),\n                            blkLocations[blkIndex].getCachedHosts()));\n       // 更新剩余数量\n       bytesRemaining -= splitSize;\n   }\n   // 剩下的数量小于等于1.1倍的splitSize，直接把他们放到一个split里面去\n   if (bytesRemaining != 0) {\n       int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n       splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,\n                            blkLocations[blkIndex].getHosts(),\n                            blkLocations[blkIndex].getCachedHosts()));\n   }\n   ```\n\n7. 完成split后，通过`JobSplitWriter`将splits信息保存到一个临时文件`job.splits`中：\n\n   ```java\n   private <T extends InputSplit>\n     int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,\n         InterruptedException, ClassNotFoundException {\n       Configuration conf = job.getConfiguration();\n       InputFormat<?, ?> input =\n         ReflectionUtils.newInstance(job.getInputFormatClass(), conf);\n       // 获取splits      \n       List<InputSplit> splits = input.getSplits(job);\n       T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);\n       Arrays.sort(array, new SplitComparator());\n       // 将splits写到本地临时文件      \n       JobSplitWriter.createSplitFiles(jobSubmitDir, conf, \n           jobSubmitDir.getFileSystem(conf), array);\n       // 返回split的数量      \n       return array.length;\n     }\n   ```\n\n   其中`jobSubmitDir`是在提交阶段在本地创建的用于保存提交信息的临时文件夹，最后将split数目返回，即为需要启动的`mapTask`数目，也就是并行度\n\n8. 最后提交本次job，其中`submitClient`即为提交客户端，如果在yarn环境下是由`YARNRunner`这个类来完成：\n\n   ```java\n   status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\n   ```\n\n至此整个提交过程完成，`yarn`会根据提交数据（包括split信息和job配置信息）再结合各计算节点的资源利用率，将job提交给各个计算节点启动多个`mapTask`进行计算\n\n关于任务提交后的流程就得研究`yarn`的运行机制了\n\n\n\n\n\n","source":"_posts/对于Map-Reduce并行度的理解.md","raw":"title: 对于Map-Reduce并行度的理解\nauthor: 天渊\ntags:\n  - map-reduce\ncategories:\n  - 大数据\ndate: 2019-07-24 21:08:00\n---\n\nhadoop计算框架map-reduce有一个并行度的概念，每个job，对于输入文件A，需要对A进行切片（即`split`），再针对各个`split`单独启动独立的`mapTask`进行计算（hadoop 2.0后由yarn完成），切片完成后启动多个`mapTask`即为mr任务的并行度\n\n<!-- more -->\n\n### map-reduce的split方式\n\n默认情况下，文件的单个split大小（即`split-size`）通常与HDFS的`block-size`保持一致（即默认的128MB），该工作由`FileInputFormat`调用`getSplits()`方法来完成，通过读取文件metadata进行切分，生成对应的`FileSplit`对象，其中就包含了各个文件切片的offset和length等信息，再序列化到`job.splits`文件中：\n\n```java\n// InputFormat类中的方法，从JobContext中获取输入文件的信息\n// 根据输入文件信息生成切分信息\npublic abstract List<InputSplit> getSplits(JobContext context)\n```\n\n文件的切分信息保存在`FileSplit`对象中，主要保存的了文件在相应`FileSystem`上的path，切分开始位置和切分长度，以及主机信息和当前Split的具体位置：\n\n```java\npublic class FileSplit extends InputSplit implements Writable {\n  private Path file;\n  private long start;\n  private long length;\n  private String[] hosts;\n  private SplitLocationInfo[] hostInfos;\n    \n  ......\n}\n```\n\n### map-reduce任务提交过程\n\nmap-reduce在客户端完成切分工作后上传到服务器，针对每个`Split`单独启动mapTask，下面来看看在客户端提交job后是怎么进行split的：\n\n1. `job.submit()`后，初始化一个`JobSubmitter`对象进行job的提交工作\n\n2. `JobSubmitter`调用`submitJobInternal(job, cluster)`方法进行方法的提交，在进行一系列的初始化过程后，调用`writeSplits(job, submitJobDir)`方法进行切分\n\n3. `writeSplits`最终就会调用上面提到的`InputFormat`的`getSplits`方法\n\n4. 在`getSplits`方法中，首先计算split的最大和最小限制：\n\n   ```java\n   // 默认是1，可以通过mapreduce.input.fileinputformat.split.minsize属性进行设置\n   long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\n   // 默认是Long.MAX_VALUE，可以通过mapreduce.input.fileinputformat.split.maxsize属性来设置\n   long maxSize = getMaxSplitSize(job);\n   ```\n\n5. 在确认最大最小范围后，需要确定真正需要`splitSize`，使用`computeSplitSize`方法进行确认，可以看出通常情况下`splitSize`即为`blockSize`：\n\n   ```java\n   long splitSize = computeSplitSize(blockSize, minSize, maxSize);\n   //computeSplitSize方法：\n   protected long computeSplitSize(long blockSize, long minSize, long maxSize) {\n       // 在maxSize和blockSize中取小值，最后保证比minSize大\n       return Math.max(minSize, Math.min(maxSize, blockSize));\n   }\n   ```\n\n6. 对文件进行split，代码如下：\n\n   ```java\n   // 剩余还未split的数量\n   long bytesRemaining = length;\n   // 如果剩余数量多于1.1倍的splitSize，则持续进行split\n   while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {\n       // 获取当前offset所处的block\n       int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n       // 生成split\n       splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                            blkLocations[blkIndex].getHosts(),\n                            blkLocations[blkIndex].getCachedHosts()));\n       // 更新剩余数量\n       bytesRemaining -= splitSize;\n   }\n   // 剩下的数量小于等于1.1倍的splitSize，直接把他们放到一个split里面去\n   if (bytesRemaining != 0) {\n       int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n       splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,\n                            blkLocations[blkIndex].getHosts(),\n                            blkLocations[blkIndex].getCachedHosts()));\n   }\n   ```\n\n7. 完成split后，通过`JobSplitWriter`将splits信息保存到一个临时文件`job.splits`中：\n\n   ```java\n   private <T extends InputSplit>\n     int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,\n         InterruptedException, ClassNotFoundException {\n       Configuration conf = job.getConfiguration();\n       InputFormat<?, ?> input =\n         ReflectionUtils.newInstance(job.getInputFormatClass(), conf);\n       // 获取splits      \n       List<InputSplit> splits = input.getSplits(job);\n       T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);\n       Arrays.sort(array, new SplitComparator());\n       // 将splits写到本地临时文件      \n       JobSplitWriter.createSplitFiles(jobSubmitDir, conf, \n           jobSubmitDir.getFileSystem(conf), array);\n       // 返回split的数量      \n       return array.length;\n     }\n   ```\n\n   其中`jobSubmitDir`是在提交阶段在本地创建的用于保存提交信息的临时文件夹，最后将split数目返回，即为需要启动的`mapTask`数目，也就是并行度\n\n8. 最后提交本次job，其中`submitClient`即为提交客户端，如果在yarn环境下是由`YARNRunner`这个类来完成：\n\n   ```java\n   status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\n   ```\n\n至此整个提交过程完成，`yarn`会根据提交数据（包括split信息和job配置信息）再结合各计算节点的资源利用率，将job提交给各个计算节点启动多个`mapTask`进行计算\n\n关于任务提交后的流程就得研究`yarn`的运行机制了\n\n\n\n\n\n","slug":"对于Map-Reduce并行度的理解","published":1,"updated":"2019-07-24T13:09:15.717Z","_id":"cjyh9ktdi0031g0qrixueg7k0","comments":1,"layout":"post","photos":[],"link":"","content":"<p>hadoop计算框架map-reduce有一个并行度的概念，每个job，对于输入文件A，需要对A进行切片（即<code>split</code>），再针对各个<code>split</code>单独启动独立的<code>mapTask</code>进行计算（hadoop 2.0后由yarn完成），切片完成后启动多个<code>mapTask</code>即为mr任务的并行度</p>\n<a id=\"more\"></a>\n<h3 id=\"map-reduce的split方式\"><a href=\"#map-reduce的split方式\" class=\"headerlink\" title=\"map-reduce的split方式\"></a>map-reduce的split方式</h3><p>默认情况下，文件的单个split大小（即<code>split-size</code>）通常与HDFS的<code>block-size</code>保持一致（即默认的128MB），该工作由<code>FileInputFormat</code>调用<code>getSplits()</code>方法来完成，通过读取文件metadata进行切分，生成对应的<code>FileSplit</code>对象，其中就包含了各个文件切片的offset和length等信息，再序列化到<code>job.splits</code>文件中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// InputFormat类中的方法，从JobContext中获取输入文件的信息</span></span><br><span class=\"line\"><span class=\"comment\">// 根据输入文件信息生成切分信息</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">abstract</span> List&lt;InputSplit&gt; <span class=\"title\">getSplits</span><span class=\"params\">(JobContext context)</span></span></span><br></pre></td></tr></table></figure>\n<p>文件的切分信息保存在<code>FileSplit</code>对象中，主要保存的了文件在相应<code>FileSystem</code>上的path，切分开始位置和切分长度，以及主机信息和当前Split的具体位置：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FileSplit</span> <span class=\"keyword\">extends</span> <span class=\"title\">InputSplit</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> Path file;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> start;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> length;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> String[] hosts;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> SplitLocationInfo[] hostInfos;</span><br><span class=\"line\">    </span><br><span class=\"line\">  ......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"map-reduce任务提交过程\"><a href=\"#map-reduce任务提交过程\" class=\"headerlink\" title=\"map-reduce任务提交过程\"></a>map-reduce任务提交过程</h3><p>map-reduce在客户端完成切分工作后上传到服务器，针对每个<code>Split</code>单独启动mapTask，下面来看看在客户端提交job后是怎么进行split的：</p>\n<ol>\n<li><p><code>job.submit()</code>后，初始化一个<code>JobSubmitter</code>对象进行job的提交工作</p>\n</li>\n<li><p><code>JobSubmitter</code>调用<code>submitJobInternal(job, cluster)</code>方法进行方法的提交，在进行一系列的初始化过程后，调用<code>writeSplits(job, submitJobDir)</code>方法进行切分</p>\n</li>\n<li><p><code>writeSplits</code>最终就会调用上面提到的<code>InputFormat</code>的<code>getSplits</code>方法</p>\n</li>\n<li><p>在<code>getSplits</code>方法中，首先计算split的最大和最小限制：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 默认是1，可以通过mapreduce.input.fileinputformat.split.minsize属性进行设置</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class=\"line\"><span class=\"comment\">// 默认是Long.MAX_VALUE，可以通过mapreduce.input.fileinputformat.split.maxsize属性来设置</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> maxSize = getMaxSplitSize(job);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在确认最大最小范围后，需要确定真正需要<code>splitSize</code>，使用<code>computeSplitSize</code>方法进行确认，可以看出通常情况下<code>splitSize</code>即为<code>blockSize</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class=\"line\"><span class=\"comment\">//computeSplitSize方法：</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">long</span> <span class=\"title\">computeSplitSize</span><span class=\"params\">(<span class=\"keyword\">long</span> blockSize, <span class=\"keyword\">long</span> minSize, <span class=\"keyword\">long</span> maxSize)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 在maxSize和blockSize中取小值，最后保证比minSize大</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对文件进行split，代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 剩余还未split的数量</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> bytesRemaining = length;</span><br><span class=\"line\"><span class=\"comment\">// 如果剩余数量多于1.1倍的splitSize，则持续进行split</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (((<span class=\"keyword\">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 获取当前offset所处的block</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class=\"line\">    <span class=\"comment\">// 生成split</span></span><br><span class=\"line\">    splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class=\"line\">                         blkLocations[blkIndex].getHosts(),</span><br><span class=\"line\">                         blkLocations[blkIndex].getCachedHosts()));</span><br><span class=\"line\">    <span class=\"comment\">// 更新剩余数量</span></span><br><span class=\"line\">    bytesRemaining -= splitSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 剩下的数量小于等于1.1倍的splitSize，直接把他们放到一个split里面去</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (bytesRemaining != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class=\"line\">    splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class=\"line\">                         blkLocations[blkIndex].getHosts(),</span><br><span class=\"line\">                         blkLocations[blkIndex].getCachedHosts()));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>完成split后，通过<code>JobSplitWriter</code>将splits信息保存到一个临时文件<code>job.splits</code>中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> &lt;T extends InputSplit&gt;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">writeNewSplits</span><span class=\"params\">(JobContext job, Path jobSubmitDir)</span> <span class=\"keyword\">throws</span> IOException,</span></span><br><span class=\"line\"><span class=\"function\">      InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class=\"line\">    Configuration conf = job.getConfiguration();</span><br><span class=\"line\">    InputFormat&lt;?, ?&gt; input =</span><br><span class=\"line\">      ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class=\"line\">    <span class=\"comment\">// 获取splits      </span></span><br><span class=\"line\">    List&lt;InputSplit&gt; splits = input.getSplits(job);</span><br><span class=\"line\">    T[] array = (T[]) splits.toArray(<span class=\"keyword\">new</span> InputSplit[splits.size()]);</span><br><span class=\"line\">    Arrays.sort(array, <span class=\"keyword\">new</span> SplitComparator());</span><br><span class=\"line\">    <span class=\"comment\">// 将splits写到本地临时文件      </span></span><br><span class=\"line\">    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, </span><br><span class=\"line\">        jobSubmitDir.getFileSystem(conf), array);</span><br><span class=\"line\">    <span class=\"comment\">// 返回split的数量      </span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> array.length;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>其中<code>jobSubmitDir</code>是在提交阶段在本地创建的用于保存提交信息的临时文件夹，最后将split数目返回，即为需要启动的<code>mapTask</code>数目，也就是并行度</p>\n</li>\n<li><p>最后提交本次job，其中<code>submitClient</code>即为提交客户端，如果在yarn环境下是由<code>YARNRunner</code>这个类来完成：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>至此整个提交过程完成，<code>yarn</code>会根据提交数据（包括split信息和job配置信息）再结合各计算节点的资源利用率，将job提交给各个计算节点启动多个<code>mapTask</code>进行计算</p>\n<p>关于任务提交后的流程就得研究<code>yarn</code>的运行机制了</p>\n","site":{"data":{}},"excerpt":"<p>hadoop计算框架map-reduce有一个并行度的概念，每个job，对于输入文件A，需要对A进行切片（即<code>split</code>），再针对各个<code>split</code>单独启动独立的<code>mapTask</code>进行计算（hadoop 2.0后由yarn完成），切片完成后启动多个<code>mapTask</code>即为mr任务的并行度</p>","more":"<h3 id=\"map-reduce的split方式\"><a href=\"#map-reduce的split方式\" class=\"headerlink\" title=\"map-reduce的split方式\"></a>map-reduce的split方式</h3><p>默认情况下，文件的单个split大小（即<code>split-size</code>）通常与HDFS的<code>block-size</code>保持一致（即默认的128MB），该工作由<code>FileInputFormat</code>调用<code>getSplits()</code>方法来完成，通过读取文件metadata进行切分，生成对应的<code>FileSplit</code>对象，其中就包含了各个文件切片的offset和length等信息，再序列化到<code>job.splits</code>文件中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// InputFormat类中的方法，从JobContext中获取输入文件的信息</span></span><br><span class=\"line\"><span class=\"comment\">// 根据输入文件信息生成切分信息</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">abstract</span> List&lt;InputSplit&gt; <span class=\"title\">getSplits</span><span class=\"params\">(JobContext context)</span></span></span><br></pre></td></tr></table></figure>\n<p>文件的切分信息保存在<code>FileSplit</code>对象中，主要保存的了文件在相应<code>FileSystem</code>上的path，切分开始位置和切分长度，以及主机信息和当前Split的具体位置：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FileSplit</span> <span class=\"keyword\">extends</span> <span class=\"title\">InputSplit</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> Path file;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> start;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> length;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> String[] hosts;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> SplitLocationInfo[] hostInfos;</span><br><span class=\"line\">    </span><br><span class=\"line\">  ......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"map-reduce任务提交过程\"><a href=\"#map-reduce任务提交过程\" class=\"headerlink\" title=\"map-reduce任务提交过程\"></a>map-reduce任务提交过程</h3><p>map-reduce在客户端完成切分工作后上传到服务器，针对每个<code>Split</code>单独启动mapTask，下面来看看在客户端提交job后是怎么进行split的：</p>\n<ol>\n<li><p><code>job.submit()</code>后，初始化一个<code>JobSubmitter</code>对象进行job的提交工作</p>\n</li>\n<li><p><code>JobSubmitter</code>调用<code>submitJobInternal(job, cluster)</code>方法进行方法的提交，在进行一系列的初始化过程后，调用<code>writeSplits(job, submitJobDir)</code>方法进行切分</p>\n</li>\n<li><p><code>writeSplits</code>最终就会调用上面提到的<code>InputFormat</code>的<code>getSplits</code>方法</p>\n</li>\n<li><p>在<code>getSplits</code>方法中，首先计算split的最大和最小限制：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 默认是1，可以通过mapreduce.input.fileinputformat.split.minsize属性进行设置</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class=\"line\"><span class=\"comment\">// 默认是Long.MAX_VALUE，可以通过mapreduce.input.fileinputformat.split.maxsize属性来设置</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> maxSize = getMaxSplitSize(job);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在确认最大最小范围后，需要确定真正需要<code>splitSize</code>，使用<code>computeSplitSize</code>方法进行确认，可以看出通常情况下<code>splitSize</code>即为<code>blockSize</code>：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class=\"line\"><span class=\"comment\">//computeSplitSize方法：</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">long</span> <span class=\"title\">computeSplitSize</span><span class=\"params\">(<span class=\"keyword\">long</span> blockSize, <span class=\"keyword\">long</span> minSize, <span class=\"keyword\">long</span> maxSize)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 在maxSize和blockSize中取小值，最后保证比minSize大</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对文件进行split，代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 剩余还未split的数量</span></span><br><span class=\"line\"><span class=\"keyword\">long</span> bytesRemaining = length;</span><br><span class=\"line\"><span class=\"comment\">// 如果剩余数量多于1.1倍的splitSize，则持续进行split</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (((<span class=\"keyword\">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 获取当前offset所处的block</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class=\"line\">    <span class=\"comment\">// 生成split</span></span><br><span class=\"line\">    splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class=\"line\">                         blkLocations[blkIndex].getHosts(),</span><br><span class=\"line\">                         blkLocations[blkIndex].getCachedHosts()));</span><br><span class=\"line\">    <span class=\"comment\">// 更新剩余数量</span></span><br><span class=\"line\">    bytesRemaining -= splitSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 剩下的数量小于等于1.1倍的splitSize，直接把他们放到一个split里面去</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (bytesRemaining != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class=\"line\">    splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class=\"line\">                         blkLocations[blkIndex].getHosts(),</span><br><span class=\"line\">                         blkLocations[blkIndex].getCachedHosts()));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>完成split后，通过<code>JobSplitWriter</code>将splits信息保存到一个临时文件<code>job.splits</code>中：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> &lt;T extends InputSplit&gt;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">writeNewSplits</span><span class=\"params\">(JobContext job, Path jobSubmitDir)</span> <span class=\"keyword\">throws</span> IOException,</span></span><br><span class=\"line\"><span class=\"function\">      InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class=\"line\">    Configuration conf = job.getConfiguration();</span><br><span class=\"line\">    InputFormat&lt;?, ?&gt; input =</span><br><span class=\"line\">      ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class=\"line\">    <span class=\"comment\">// 获取splits      </span></span><br><span class=\"line\">    List&lt;InputSplit&gt; splits = input.getSplits(job);</span><br><span class=\"line\">    T[] array = (T[]) splits.toArray(<span class=\"keyword\">new</span> InputSplit[splits.size()]);</span><br><span class=\"line\">    Arrays.sort(array, <span class=\"keyword\">new</span> SplitComparator());</span><br><span class=\"line\">    <span class=\"comment\">// 将splits写到本地临时文件      </span></span><br><span class=\"line\">    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, </span><br><span class=\"line\">        jobSubmitDir.getFileSystem(conf), array);</span><br><span class=\"line\">    <span class=\"comment\">// 返回split的数量      </span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> array.length;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>其中<code>jobSubmitDir</code>是在提交阶段在本地创建的用于保存提交信息的临时文件夹，最后将split数目返回，即为需要启动的<code>mapTask</code>数目，也就是并行度</p>\n</li>\n<li><p>最后提交本次job，其中<code>submitClient</code>即为提交客户端，如果在yarn环境下是由<code>YARNRunner</code>这个类来完成：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>至此整个提交过程完成，<code>yarn</code>会根据提交数据（包括split信息和job配置信息）再结合各计算节点的资源利用率，将job提交给各个计算节点启动多个<code>mapTask</code>进行计算</p>\n<p>关于任务提交后的流程就得研究<code>yarn</code>的运行机制了</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjyh9js380003g0qr7xe4fiss","category_id":"cjyh9js3f0006g0qrew5okjj4","_id":"cjyh9js3i000cg0qrwmyw65u4"},{"post_id":"cjyh9js3e0005g0qr8k8bjic6","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js3l000gg0qrvbkc4ia8"},{"post_id":"cjyh9js3g0007g0qr0upjhy26","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js3m000kg0qr4fbq7j3z"},{"post_id":"cjyh9js3h0008g0qrpaxeetmk","category_id":"cjyh9js3l000fg0qr0d8rr8yq","_id":"cjyh9js3m000og0qrfk2z8p2r"},{"post_id":"cjyh9js3r000xg0qr2aca0ugn","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js3y0012g0qrf8lyv319"},{"post_id":"cjyh9js3t000yg0qr4wmehvlk","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js3z0014g0qrko1a1m86"},{"post_id":"cjyh9js3u0010g0qrj7s0ni1z","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js410017g0qrizf3mqnr"},{"post_id":"cjyh9js3y0013g0qr5onkb6r6","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js43001bg0qr5mnj71r2"},{"post_id":"cjyh9js410018g0qrf2bwtycy","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js46001hg0qrfzgohsxd"},{"post_id":"cjyh9js42001ag0qrsqmhoq95","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js47001kg0qrambnvny2"},{"post_id":"cjyh9js43001dg0qrap94t63n","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js48001ng0qrhh7x9oi9"},{"post_id":"cjyh9js46001jg0qrrvptipqm","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js49001og0qrdcks6grp"},{"post_id":"cjyh9js47001mg0qrd8g11oil","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js4a001rg0qr2hffwdgi"},{"post_id":"cjyh9js50002ng0qr03a81zcr","category_id":"cjyh9js3i000ag0qrohjo5i4j","_id":"cjyh9js52002pg0qr0c1vn5z0"},{"post_id":"cjyh9js60002vg0qregz1x597","category_id":"cjyh9js3l000fg0qr0d8rr8yq","_id":"cjyh9js63002xg0qr54c50gfb"},{"post_id":"cjyh9ktdi0031g0qrixueg7k0","category_id":"cjyh9js3l000fg0qr0d8rr8yq","_id":"cjyh9ld6d0033g0qr51hcx6y8"}],"PostTag":[{"post_id":"cjyh9js2d0002g0qruk1srr4j","tag_id":"cjyh9js3c0004g0qr1rziv0hm","_id":"cjyh9js3l000hg0qr0k4vzip4"},{"post_id":"cjyh9js2d0002g0qruk1srr4j","tag_id":"cjyh9js3h0009g0qrir6ptyyy","_id":"cjyh9js3l000ig0qr7gtnuw8l"},{"post_id":"cjyh9js2d0002g0qruk1srr4j","tag_id":"cjyh9js3i000bg0qrmyz5zalq","_id":"cjyh9js3m000lg0qrcqhb3b4c"},{"post_id":"cjyh9js380003g0qr7xe4fiss","tag_id":"cjyh9js3k000eg0qr0e7jsffh","_id":"cjyh9js3m000mg0qr42nlkdb5"},{"post_id":"cjyh9js3e0005g0qr8k8bjic6","tag_id":"cjyh9js3l000jg0qrlpeq2qc1","_id":"cjyh9js3n000qg0qropbzdvpu"},{"post_id":"cjyh9js3e0005g0qr8k8bjic6","tag_id":"cjyh9js3m000ng0qr0pmcz0kp","_id":"cjyh9js3n000rg0qrj8gsrmet"},{"post_id":"cjyh9js3g0007g0qr0upjhy26","tag_id":"cjyh9js3m000pg0qrs029emhp","_id":"cjyh9js3n000tg0qr8zw34z0u"},{"post_id":"cjyh9js3h0008g0qrpaxeetmk","tag_id":"cjyh9js3n000sg0qrjb04z4gt","_id":"cjyh9js3o000vg0qrh1ubumvw"},{"post_id":"cjyh9js3h0008g0qrpaxeetmk","tag_id":"cjyh9js3n000ug0qrlqvidt3z","_id":"cjyh9js3o000wg0qrcfd3g2r4"},{"post_id":"cjyh9js3r000xg0qr2aca0ugn","tag_id":"cjyh9js3u000zg0qr4xwt0ptn","_id":"cjyh9js43001cg0qr1qoi9can"},{"post_id":"cjyh9js3r000xg0qr2aca0ugn","tag_id":"cjyh9js3z0015g0qr6q2inpkq","_id":"cjyh9js44001eg0qrn2rde568"},{"post_id":"cjyh9js42001ag0qrsqmhoq95","tag_id":"cjyh9js420019g0qrglerh54t","_id":"cjyh9js46001ig0qr3vsydcqy"},{"post_id":"cjyh9js3t000yg0qr4wmehvlk","tag_id":"cjyh9js420019g0qrglerh54t","_id":"cjyh9js4a001qg0qr7iwj0gvz"},{"post_id":"cjyh9js3t000yg0qr4wmehvlk","tag_id":"cjyh9js44001fg0qryuapv4h0","_id":"cjyh9js4a001sg0qrvlaulgrv"},{"post_id":"cjyh9js3t000yg0qr4wmehvlk","tag_id":"cjyh9js47001lg0qrn1uv22bu","_id":"cjyh9js4b001ug0qrk93adl8v"},{"post_id":"cjyh9js3u0010g0qrj7s0ni1z","tag_id":"cjyh9js44001fg0qryuapv4h0","_id":"cjyh9js4c001xg0qr12qvitq8"},{"post_id":"cjyh9js3u0010g0qrj7s0ni1z","tag_id":"cjyh9js47001lg0qrn1uv22bu","_id":"cjyh9js4c001yg0qrrdszrbsd"},{"post_id":"cjyh9js3u0010g0qrj7s0ni1z","tag_id":"cjyh9js420019g0qrglerh54t","_id":"cjyh9js4d0020g0qrrq8lezej"},{"post_id":"cjyh9js3w0011g0qrdf4483db","tag_id":"cjyh9js3u000zg0qr4xwt0ptn","_id":"cjyh9js4d0021g0qrvnir3ysj"},{"post_id":"cjyh9js3w0011g0qrdf4483db","tag_id":"cjyh9js4c001wg0qr2c5vk0rw","_id":"cjyh9js4e0023g0qroxh910m0"},{"post_id":"cjyh9js3y0013g0qr5onkb6r6","tag_id":"cjyh9js3u000zg0qr4xwt0ptn","_id":"cjyh9js4e0024g0qreh1qywqo"},{"post_id":"cjyh9js3y0013g0qr5onkb6r6","tag_id":"cjyh9js4c001wg0qr2c5vk0rw","_id":"cjyh9js4f0026g0qrc8qcsbg6"},{"post_id":"cjyh9js400016g0qr6goppgy4","tag_id":"cjyh9js4d0022g0qrq2ouqw09","_id":"cjyh9js4f0027g0qrhgpx5a26"},{"post_id":"cjyh9js410018g0qrf2bwtycy","tag_id":"cjyh9js4e0025g0qrj6efruo2","_id":"cjyh9js4g002ag0qr4nhx1sx4"},{"post_id":"cjyh9js410018g0qrf2bwtycy","tag_id":"cjyh9js420019g0qrglerh54t","_id":"cjyh9js4g002bg0qr7emg48xu"},{"post_id":"cjyh9js43001dg0qrap94t63n","tag_id":"cjyh9js4g0029g0qreppvpka0","_id":"cjyh9js4h002eg0qrtmo8gyg0"},{"post_id":"cjyh9js43001dg0qrap94t63n","tag_id":"cjyh9js4g002cg0qrj3qyjz3m","_id":"cjyh9js4h002fg0qrvjn1d1be"},{"post_id":"cjyh9js43001dg0qrap94t63n","tag_id":"cjyh9js420019g0qrglerh54t","_id":"cjyh9js4i002hg0qr9zny98ve"},{"post_id":"cjyh9js46001jg0qrrvptipqm","tag_id":"cjyh9js3u000zg0qr4xwt0ptn","_id":"cjyh9js4i002ig0qr9eteybjr"},{"post_id":"cjyh9js46001jg0qrrvptipqm","tag_id":"cjyh9js4c001wg0qr2c5vk0rw","_id":"cjyh9js4i002jg0qryk49hil0"},{"post_id":"cjyh9js47001mg0qrd8g11oil","tag_id":"cjyh9js3u000zg0qr4xwt0ptn","_id":"cjyh9js4i002kg0qr03ilt6df"},{"post_id":"cjyh9js47001mg0qrd8g11oil","tag_id":"cjyh9js4c001wg0qr2c5vk0rw","_id":"cjyh9js4j002lg0qrgs5o9c24"},{"post_id":"cjyh9js4y002mg0qrbjm25qcp","tag_id":"cjyh9js51002og0qr4kk27bri","_id":"cjyh9js52002rg0qrklig3ncl"},{"post_id":"cjyh9js50002ng0qr03a81zcr","tag_id":"cjyh9js52002qg0qry5rotjcj","_id":"cjyh9js53002tg0qr7dx7mapt"},{"post_id":"cjyh9js50002ng0qr03a81zcr","tag_id":"cjyh9js52002sg0qrhezr1wkx","_id":"cjyh9js53002ug0qr31dohxzg"},{"post_id":"cjyh9js60002vg0qregz1x597","tag_id":"cjyh9js62002wg0qragtkxvet","_id":"cjyh9js64002zg0qrwqquj7rm"},{"post_id":"cjyh9js60002vg0qregz1x597","tag_id":"cjyh9js63002yg0qr5ysqd4dp","_id":"cjyh9js640030g0qrh60jpx3k"},{"post_id":"cjyh9ktdi0031g0qrixueg7k0","tag_id":"cjyh9ld6c0032g0qr2i8z7qjz","_id":"cjyh9ld6d0034g0qr3znifgqa"}],"Tag":[{"name":"k8s","_id":"cjyh9js3c0004g0qr1rziv0hm"},{"name":"devops","_id":"cjyh9js3h0009g0qrir6ptyyy"},{"name":"云原生","_id":"cjyh9js3i000bg0qrmyz5zalq"},{"name":"vim","_id":"cjyh9js3k000eg0qr0e7jsffh"},{"name":"TCP","_id":"cjyh9js3l000jg0qrlpeq2qc1"},{"name":"计算机网络","_id":"cjyh9js3m000ng0qr0pmcz0kp"},{"name":"Nginx","_id":"cjyh9js3m000pg0qrs029emhp"},{"name":"Hadoop","_id":"cjyh9js3n000sg0qrjb04z4gt"},{"name":"Yarn","_id":"cjyh9js3n000ug0qrlqvidt3z"},{"name":"Kafka","_id":"cjyh9js3u000zg0qr4xwt0ptn"},{"name":"reactor","_id":"cjyh9js3z0015g0qr6q2inpkq"},{"name":"Java","_id":"cjyh9js420019g0qrglerh54t"},{"name":"多线程","_id":"cjyh9js44001fg0qryuapv4h0"},{"name":"Java并发包","_id":"cjyh9js47001lg0qrn1uv22bu"},{"name":"大数据","_id":"cjyh9js4c001wg0qr2c5vk0rw"},{"name":"mongodb","_id":"cjyh9js4d0022g0qrq2ouqw09"},{"name":"Netty","_id":"cjyh9js4e0025g0qrj6efruo2"},{"name":"spring","_id":"cjyh9js4g0029g0qreppvpka0"},{"name":"RestTemplate","_id":"cjyh9js4g002cg0qrj3qyjz3m"},{"name":"分布式理论","_id":"cjyh9js51002og0qr4kk27bri"},{"name":"java","_id":"cjyh9js52002qg0qry5rotjcj"},{"name":"类加载器","_id":"cjyh9js52002sg0qrhezr1wkx"},{"name":"yarn","_id":"cjyh9js62002wg0qragtkxvet"},{"name":"hadoop","_id":"cjyh9js63002yg0qr5ysqd4dp"},{"name":"map-reduce","_id":"cjyh9ld6c0032g0qr2i8z7qjz"}]}}